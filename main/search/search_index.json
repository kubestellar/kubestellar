{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"readme/","title":"Readme","text":""},{"location":"readme/#multi-cluster-configuration-management-for-edge-multi-cloud-and-hybrid-cloud","title":"Multi-cluster Configuration Management for Edge, Multi-Cloud, and Hybrid Cloud","text":"<p>KubeStellar is a Cloud Native Computing Foundation (CNCF) Sandbox project that simplifies the deployment and configuration of applications across multiple Kubernetes clusters. It provides a seamless experience akin to using a single cluster, and it integrates with the tools you're already familiar with, eliminating the need to modify existing resources.</p> <p>KubeStellar is particularly beneficial if you're currently deploying in a single cluster and are looking to expand to multiple clusters, or if you're already using multiple clusters and are seeking a more streamlined developer experience.</p> <p> KubeStellar High Level View </p> <p>The use of multiple clusters offers several advantages, including:</p> <ul> <li>Separation of environments (e.g., development, testing, staging)</li> <li>Isolation of groups, teams, or departments</li> <li>Compliance with enterprise security or data governance requirements</li> <li>Enhanced resiliency, including across different clouds</li> <li>Improved resource availability</li> <li>Access to heterogeneous resources</li> <li>Capability to run applications on the edge, including in disconnected environments</li> </ul> <p>In a single-cluster setup, developers typically access the cluster and deploy Kubernetes objects directly. Without KubeStellar, multiple clusters are usually deployed and configured individually, which can be time-consuming and complex.</p> <p>KubeStellar simplifies this process by allowing developers to define a binding policy between clusters and Kubernetes objects. It then uses your regular single-cluster tooling to deploy and configure each cluster based on these binding policies, making multi-cluster operations as straightforward as managing a single cluster. This approach enhances productivity and efficiency, making KubeStellar a valuable tool in a multi-cluster Kubernetes environment.</p>"},{"location":"readme/#contributing","title":"Contributing","text":"<p>We \u2764\ufe0f our contributors! If you're interested in helping us out, please head over to our Contributing guide.</p>"},{"location":"readme/#getting-in-touch","title":"Getting in touch","text":"<p>There are several ways to communicate with us:</p> <p>Instantly get access to our documents and meeting invites http://kubestellar.io/joinus</p> <ul> <li>The <code>#kubestellar-dev</code> channel in the Kubernetes Slack workspace</li> <li>Our mailing lists:<ul> <li>kubestellar-dev for development discussions</li> <li>kubestellar-users for discussions among users and potential users</li> </ul> </li> <li>Subscribe to the community calendar for community meetings and events<ul> <li>The kubestellar-dev mailing list is subscribed to this calendar</li> </ul> </li> <li>See recordings of past KubeStellar community meetings on YouTube</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Browse the shared Google Drive to share design docs, notes, etc.<ul> <li>Members of the kubestellar-dev mailing list can view this drive</li> </ul> </li> <li>Read our documentation</li> <li>Follow us on:</li> <li>LinkedIn - #kubestellar</li> <li>Medium - kubestellar.medium.com</li> </ul>"},{"location":"readme/#contributors","title":"\u2764\ufe0f Contributors","text":"<p>Thanks go to these wonderful people:</p> Jun Duan\ud83d\udc40 Braulio Dumba\ud83d\udc40 Mike Spreitzer\ud83d\udc40 Paolo Dettori\ud83d\udc40 Andy Anderson\ud83d\udc40 Franco Stellari\ud83d\udc40 Ezra Silvera\ud83d\udc40 Bob Filepp\ud83d\udc40 Effi Ofer\ud83d\udc40 Maria Camila Ruiz Cardenas\ud83d\udc40 Andrey Odarenko\ud83d\udc40 Aashni Manroa\ud83d\udc40 Kevin Roche\ud83d\udc40 Nick Masluk\ud83d\udc40 Francois Abel\ud83d\udc40 Nir Rozenbaum\ud83d\udc40 Maroon Ayoub\ud83d\udc40 Graham White\ud83d\udc40"},{"location":"Community/_index/","title":"Join the KubeStellar community","text":""},{"location":"Community/_index/#kubestellar-is-an-open-source-project-that-anyone-in-the-community-can-use-improve-and-enjoy-join-us-heres-a-few-ways-to-find-out-whats-happening-and-get-involved","title":"KubeStellar is an open source project that anyone in the community can use, improve, and enjoy. Join us! Here's a few ways to find out what's happening and get involved","text":""},{"location":"Community/_index/#learn-and-connect","title":"Learn and Connect","text":""},{"location":"Community/_index/#using-or-want-to-use-kubestellar-find-out-more-here","title":"Using or want to use KubeStellar? Find out more here:","text":"<ul> <li>User mailing list: Discussion and help from your fellow users</li> <li>YouTube Channel: Follow us on YouTube to view recordings of past KubeStellar community meetings and demo days</li> <li>LinkedIn: See what others are saying about the community</li> <li>Medium Blog Series: Follow us on Medium to read about community developments</li> </ul>"},{"location":"Community/_index/#develop-and-contribute","title":"Develop and Contribute","text":""},{"location":"Community/_index/#if-you-want-to-get-more-involved-by-contributing-to-kubestellar-join-us-here","title":"If you want to get more involved by contributing to KubeStellar, join us here:","text":"<ul> <li>GitHub: Development takes place here!</li> <li>#kubestellar-dev Slack channel in the Kubernetes slack workspace: Chat with other project developers</li> <li>Developer mailing list: Discuss development issues around the project</li> <li>You can find out how to contribute to KubeStellar in our Contribution Guidelines</li> </ul>"},{"location":"Community/_index/#community-meetings","title":"Community Meetings","text":"<ol> <li>Join our Developer mailing list to get your community meeting invitation.</li> <li>You can also directly subscribe to the community calendar, or view our calendar</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Sign up to discuss a topic in the KubeStellar Community Meeting Agenda</li> </ol>"},{"location":"Community/_index/#other-resources","title":"Other Resources","text":"<ul> <li>Google Drive</li> </ul>"},{"location":"Community/partners/argocd/","title":"ArgoCD","text":"<p>This document explains how to add KubeStellar's 'workspaces' as Argo CD's 'clusters'.</p>"},{"location":"Community/partners/argocd/#add-kubestellars-workspaces-to-argo-cd-as-clusters","title":"Add KubeStellar's workspaces to Argo CD as clusters","text":"<p>As of today, the 'workspaces', aka 'logical clusters' used by KubeStellar are not identical with ordinary Kubernetes clusters. Thus, in order to add them as Argo CD's 'clusters', there are a few more steps to take.</p> <p>For KubeStellar's Inventory Management Workspace (IMW) and Workload Management Workspace (WMW). The steps are similar. Let's take WMW as an example:</p> <ol> <li>Create `kube-system` namespace in the workspace.</li> <li>Make sure necessary apibindings exist in the workspace.  For WMW, we need one for Kubernetes and one for KubeStellar's edge API.</li> <li>Exclude `ClusterWorkspace` from discovery and sync.  <pre><code>kubectl -n argocd edit cm argocd-cm\n</code></pre>  Make sure `resource.exclusions` exists in the `data` field of the `argocd-cm` configmap as follows: <pre><code>data:\nresource.exclusions: |\n- apiGroups:\n- \"tenancy.kcp.io\"\nkinds:\n- \"ClusterWorkspace\"\nclusters:\n- \"*\"\n</code></pre>  Restart the Argo CD server. <pre><code>kubectl -n argocd rollout restart deployment argocd-server\n</code></pre>  Argo CD's documentation mentions this feature as [Resource Exclusion/Inclusion](https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#resource-exclusioninclusion). </li> <li>Make sure the current context uses WMW, then identify the admin.kubeconfig. The command and output should be similar to <pre><code>$ argocd cluster add --name wmw --kubeconfig ./admin.kubeconfig workspace.kcp.io/current\nWARNING: This will create a service account `argocd-manager` on the cluster referenced by context `workspace.kcp.io/current` with full cluster level privileges. Do you want to continue [y/N]? y\nINFO[0001] ServiceAccount \"argocd-manager\" already exists in namespace \"kube-system\"\nINFO[0001] ClusterRole \"argocd-manager-role\" updated\nINFO[0001] ClusterRoleBinding \"argocd-manager-role-binding\" updated\nCluster 'https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo' added\n</code></pre>  ### Create Argo CD Applications Once KubeStellar's workspaces are added, Argo CD Applications can be created as normal. There are a few examples listed [here](https://github.com/edge-experiments/gitops-source/tree/main/kubestellar), and the commands to use the examples are listed as follows.  #### Create Argo CD Applications against KubeStellar's IMW Create two Locations. The command and output should be similar to <pre><code>$ argocd app create locations \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/locations/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:imw-turbo \\\n--sync-policy automated\napplication 'locations' created\n</code></pre>  Create two SyncTargets. The command and output should be similar to <pre><code>$ argocd app create synctargets \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/synctargets/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:imw-turbo \\\n--sync-policy automated\napplication 'synctargets' created\n</code></pre>  #### Create Argo CD Application against KubeStellar's WMW Create a Namespace. The command and output should be similar to <pre><code>$ argocd app create namespace \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/namespaces/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'namespace' created\n</code></pre>  Create a Deployment for 'cpumemload'. The command and output should be similar to <pre><code>$ argocd app create cpumemload \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/workloads/cpumemload/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'cpumemload' created\n</code></pre>  Create an EdgePlacement. The command and output should be similar to <pre><code>$ argocd app create edgeplacement \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/placements/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'edgeplacement' created\n</code></pre> </li> </ol>"},{"location":"Community/partners/argocd/#other-resources","title":"Other Resources","text":"<p>Medium - Sync 10,000 ArgoCD Applications in One Shot Medium - Sync 10,000 ArgoCD Applications in One Shot, by Yourself Medium - GitOpsCon - here we come</p>"},{"location":"Community/partners/argocd/#argocd-scale-experiment-kubestellar-community-demo-day","title":"ArgoCD Scale Experiment - KubeStellar Community Demo Day","text":""},{"location":"Community/partners/argocd/#gitopscon-2023-a-quantitative-study-on-argo-scalability-andrew-anderson-jun-duan-ibm","title":"GitOpsCon 2023 - A Quantitative Study on Argo Scalability - Andrew Anderson &amp; Jun Duan, IBM","text":""},{"location":"Community/partners/argocd/#argocd-and-kubestellar-in-the-news","title":"ArgoCD and KubeStellar in the news","text":""},{"location":"Community/partners/fluxcd/","title":"FluxCD","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/kyverno/","title":"Check out KubeStellar working with Kyverno:","text":"<p>Medium - Syncing Objects from one Kubernetes cluster to another Kubernetes cluster</p>"},{"location":"Community/partners/kyverno/#kyverno-and-kubestellar-demo-day","title":"Kyverno and KubeStellar Demo Day","text":""},{"location":"Community/partners/kyverno/#kyverno-and-kubestellar-in-the-news","title":"Kyverno and KubeStellar in the news","text":""},{"location":"Community/partners/kyverno/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/mvi/","title":"Check out KubeStellar working with IBM's Maximo Visual Inspection (MVI):","text":"<p>Medium - Deployment and configuration of MVI-Edge using KubeStellar</p>"},{"location":"Community/partners/mvi/#mvi-and-kubestellar-demo-day","title":"MVI and KubeStellar Demo Day","text":""},{"location":"Community/partners/mvi/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/mvi/#mvi-and-kubestellar-in-the-news","title":"MVI and KubeStellar in the news","text":""},{"location":"Community/partners/openziti/","title":"OpenZiti","text":""},{"location":"Community/partners/turbonomic/","title":"Check out KubeStellar working with Turbonomic:","text":"<p>Medium - Make Multi-Cluster Scheduling a No-Brainer</p>"},{"location":"Community/partners/turbonomic/#turbonomic-and-kubestellar-demo-day","title":"Turbonomic and KubeStellar Demo Day","text":""},{"location":"Community/partners/turbonomic/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>As we can see from the blog and the demo, Turbonomic talks to KubeStellar via GitOps. The scheduling decisions are passed from Turbonomic to KubeStellar in two steps: 1. Turbo -&gt; GitHub repository. 2. GitHub repository -&gt; KubeStellar.</p> <p>For the 1st step (Turbonomic -&gt; GitHub repository), a controller named \"change reconciler\" creates PRs against the GitHub repository, where the PRs contains changes to scheduling decisions.</p> <p>There's also a piece of code which intercepts Turbonomic actions and creates CRs for the above change reconciler.</p> <p>For the 2nd step (GitHub repository-&gt; KubeStellar), we can use Argo CD. The detailed procedure to integrate Argo CD with KubeStellar is documented here.</p> <p>As we can see from the blog and the demo, Turbonomic collects data from edge clusters. This is made possible by installing kubeturbo into each of the edge clusters.</p>"},{"location":"Community/partners/turbonomic/#turbonomic-and-kubestellar-in-the-news","title":"Turbonomic and KubeStellar in the news","text":""},{"location":"Contribution%20guidelines/CONTRIBUTING/","title":"Contributing to KubeStellar","text":"<p>Greetings! We are grateful for your interest in joining the KubeStellar community and making a positive impact. Whether you're raising issues, enhancing documentation, fixing bugs, or developing new features, your contributions are essential to our success.</p> <p>To get started, kindly read through this document and familiarize yourself with our code of conduct. If you have any inquiries, please feel free to reach out to us on the KubeStellar-dev Slack channel.</p> <p>We can't wait to collaborate with you!</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#see-also","title":"See Also","text":"<p>Part of our documentation of how to contribute is meant to be viewed directly at GitHub. See it there.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#see-here","title":"See Here","text":""},{"location":"Contribution%20guidelines/CONTRIBUTING/#issues","title":"Issues","text":"<p>Prioritization for pull requests is given to those that address and resolve existing GitHub issues. Utilize the available issue labels to identify meaningful and relevant issues to work on.</p> <p>If you believe that there is a need for a fix and no existing issue covers it, feel free to create a new one.</p> <p>As a new contributor, we encourage you to start with issues labeled as good first issues.</p> <p>Your assistance in improving documentation is highly valued, regardless of your level of experience with the project.</p> <p>To claim an issue that you are interested in, kindly leave a comment on the issue and request the maintainers to assign it to you.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#committing","title":"Committing","text":"<p>We encourage all contributors to adopt best practices in git commit management to facilitate efficient reviews and retrospective analysis. Your git commits should provide ample context for reviewers and future codebase readers.</p> <p>A recommended format for final commit messages is as follows:</p> <pre><code>{Short Title}: {Problem this commit is solving and any important contextual information} {issue number if applicable}\n</code></pre>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>When submitting a pull request, clear communication is appreciated. This can be achieved by providing the following information:</p> <ul> <li>Detailed description of the problem you are trying to solve, along with links to related GitHub issues</li> <li>Explanation of your solution, including links to any design documentation and discussions</li> <li>Information on how you tested and validated your solution</li> <li>Updates to relevant documentation and examples, if applicable</li> </ul> <p>The pull request template has been designed to assist you in communicating this information effectively.</p> <p>Smaller pull requests are typically easier to review and merge than larger ones. If your pull request is big, it is always recommended to collaborate with the maintainers to find the best way to divide it.</p> <p>Approvers will review your PR within a business day. A PR requires both an /lgtm and then an /approve in order to get merged. You may /approve your own PR but you may not /lgtm it. Automation will add the PR it to the OpenShift PR merge queue. The OpenShift Tide bot will automatically merge your work when it is available.</p> <p>Congratulations! Your pull request has been successfully merged! \ud83d\udc4f</p> <p>If you have any questions about contributing, don't hesitate to reach out to us on the KubeStellar-dev Slack channel.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#licensing","title":"Licensing","text":"<p>KubeStellar is Apache 2.0 licensed and we accept contributions via GitHub pull requests.</p> <p>Please read the following guide if you're interested in contributing to KubeStellar.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#certificate-of-origin","title":"Certificate of Origin","text":"<p>By contributing to this project you agree to the Developer Certificate of Origin (DCO). This document was created by the Linux Kernel community and is a simple statement that you, as a contributor, have the legal right to make the contribution. See the DCO file for details.</p>"},{"location":"Contribution%20guidelines/LICENSE/","title":"License","text":"<p>Apache License Version 2.0, January 2004 http://www.apache.org/licenses/  TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <p>To apply the Apache License to your work, attach the following    boilerplate notice, with the fields enclosed by brackets \"[]\"    replaced with your own identifying information. (Don't include    the brackets!)  The text should be enclosed in the appropriate    comment syntax for the file format. We also recommend that a    file or class name and description of purpose be included on the    same \"printed page\" as the copyright notice for easier    identification within third-party archives.</p> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"Contribution%20guidelines/coc/","title":"Code of Conduct","text":"<p>This project is following the CNCF Code of Conduct. </p>"},{"location":"Contribution%20guidelines/coc/#kubestellar-community-code-of-conduct","title":"KubeStellar Community Code of Conduct","text":"<p>As contributors, maintainers, and participants in the CNCF community, and in the interest of fostering an open and welcoming community, we pledge to respect all people who participate or contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, attending conferences or events, or engaging in other community or project activities.</p> <p>We are committed to making participation in the CNCF community a harassment-free experience for everyone, regardless of age, body size, caste, disability, ethnicity, level of experience, family status, gender, gender identity and expression, marital status, military or veteran status, nationality, personal appearance, race, religion, sexual orientation, socieconomic status, tribe, or any other dimension of diversity.</p>"},{"location":"Contribution%20guidelines/coc/#scope","title":"Scope","text":"<p>This code of conduct applies: * within project and community spaces, * in other spaces when an individual CNCF community participant's words or actions are directed at or are about a CNCF project, the CNCF community, or another CNCF community participant.</p>"},{"location":"Contribution%20guidelines/coc/#cncf-events","title":"CNCF Events","text":"<p>CNCF events that are produced by the Linux Foundation with professional events staff are governed by the Linux Foundation Events Code of Conduct available on the event page. This is designed to be used in conjunction with the CNCF Code of Conduct.</p>"},{"location":"Contribution%20guidelines/coc/#our-standards","title":"Our Standards","text":"<p>The CNCF Community is open, inclusive and respectful. Every member of our community has the right to have their identity respected.</p> <p>Examples of behavior that contributes to a positive environment include but are not limited to:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> <li>Using welcoming and inclusive language</li> </ul> <p>Examples of unacceptable behavior include but are not limited to:</p> <ul> <li>The use of sexualized language or imagery</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment in any form</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Violence, threatening violence, or encouraging others to engage in violent behavior</li> <li>Stalking or following someone without their consent</li> <li>Unwelcome physical contact</li> <li>Unwelcome sexual or romantic attention or advances</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul> <p>The following behaviors are also prohibited: * Providing knowingly false or misleading information in connection with a Code of Conduct investigation or otherwise intentionally tampering with an investigation. * Retaliating against a person because they reported an incident or provided information about an incident as a witness.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct.  By adopting this Code of Conduct, project maintainers commit themselves to fairly and consistently applying these principles to every aspect of managing a CNCF project.  Project maintainers who do not follow or enforce the Code of Conduct may be temporarily or permanently removed from the project team.</p>"},{"location":"Contribution%20guidelines/coc/#reporting","title":"Reporting","text":"<p>For incidents occurring in the KubeStellar community, contact the KubeStellar Code of Conduct Committee of Conduct Committee. You can expect a response within three business days.</p> <p>For other projects, or for incidents that are project-agnostic or impact multiple CNCF projects, please contact the CNCF Code of Conduct Committee via conduct@cncf.io.  Alternatively, you can contact any of the individual members of the CNCF Code of Conduct Committee to submit your report. For more detailed instructions on how to submit a report, including how to submit a report anonymously, please see our Incident Resolution Procedures. You can expect a response within three business days.</p> <p>For incidents ocurring at CNCF event that is produced by the Linux Foundation, please contact eventconduct@cncf.io.</p>"},{"location":"Contribution%20guidelines/coc/#enforcement","title":"Enforcement","text":"<p>Upon review and investigation of a reported incident, the CoC response team that has jurisdiction will determine what action is appropriate based on this Code of Conduct and its related documentation. </p> <p>For information about which Code of Conduct incidents are handled by project leadership, which incidents are handled by the CNCF Code of Conduct Committee, and which incidents are handled by the Linux Foundation (including its events team), see our Jurisdiction Policy.</p>"},{"location":"Contribution%20guidelines/coc/#amendments","title":"Amendments","text":"<p>Consistent with the CNCF Charter, any substantive changes to this Code of Conduct must be approved by the Technical Oversight Committee.</p>"},{"location":"Contribution%20guidelines/coc/#acknowledgements","title":"Acknowledgements","text":"<p>This Code of Conduct is adapted from the Contributor Covenant (http://contributor-covenant.org), version 2.0 available at http://contributor-covenant.org/version/2/0/code_of_conduct/</p>"},{"location":"Contribution%20guidelines/governance/","title":"KubeStellar Project Governance","text":"<p>The KubeStellar project is dedicated to solving challenges stemming from multi-cluster configuration management for edge, multi-cloud, and hybrid cloud.  This governance explains how the project is run.</p> <ul> <li>Manifesto</li> <li>Values</li> <li>Maintainers</li> <li>Code of Conduct Enforcement</li> <li>Security Response Team</li> <li>Voting</li> <li>Modifying this Charter</li> </ul>"},{"location":"Contribution%20guidelines/governance/#manifesto","title":"Manifesto","text":"<ul> <li>KubeStellar Maintainers strive to be good citizens in the Kubernetes project.</li> <li>KubeStellar Maintainers see KubeStellar always as part of the Kubernetes ecosystem and always     strive to keep that ecosystem united. In particular, this means:</li> <li>KubeStellar strives to not divert from Kubernetes, but strives to extend its       use-cases to non-container control planes while keeping the ecosystems of       libraries and tooling united.</li> <li>KubeStellar \u2013 as a consumer of Kubernetes API Machinery \u2013 will strive to stay 100%       compatible with the semantics of Kubernetes APIs, while removing container       orchestration specific functionality.</li> <li>KubeStellar strives to upstream changes to Kubernetes code as much as possible.</li> </ul>"},{"location":"Contribution%20guidelines/governance/#values","title":"Values","text":"<p>The KubeStellar and its leadership embrace the following values:</p> <ul> <li>Openness: Communication and decision-making happens in the open and is     discoverable for future reference. As much as possible, all discussions and     work take place in public forums and open repositories.</li> <li>Fairness: All stakeholders have the opportunity to provide feedback and     submit contributions, which will be considered on their merits.</li> <li>Community over Product or Company: Sustaining and growing our community     takes priority over shipping code or sponsors' organizational goals. Each     contributor participates in the project as an individual.</li> <li>Inclusivity: We innovate through different perspectives and skill sets,     which can only be accomplished in a welcoming and respectful environment.</li> <li>Participation: Responsibilities within the project are earned through     participation, and there is a clear path up the contributor ladder into     leadership positions.</li> </ul>"},{"location":"Contribution%20guidelines/governance/#maintainers","title":"Maintainers","text":"<p>KubeStellar Maintainers have write access to the project GitHub repository. They can merge their own patches or patches from others. The current maintainers can be found as top-level approvers in OWNERS.  Maintainers collectively  manage the project's resources and contributors.</p> <p>This privilege is granted with some expectation of responsibility: maintainers are people who care about the KubeStellar project and want to help it grow and improve. A maintainer is not just someone who can make changes, but someone who has demonstrated their ability to collaborate with the team, get the most knowledgeable people to review code and docs, contribute high-quality code, and follow through to fix issues (in code or tests).</p> <p>A maintainer is a contributor to the project's success and a citizen helping the project succeed.</p> <p>The collective team of all Maintainers is known as the Maintainer Council, which  is the governing body for the project.</p>"},{"location":"Contribution%20guidelines/governance/#becoming-a-maintainer","title":"Becoming a Maintainer","text":"<p>To become a Maintainer you need to demonstrate the following:</p> <ul> <li>commitment to the project:<ul> <li>participate in discussions, contributions, code and documentation reviews   for 3 months or more,</li> <li>perform reviews for 5 non-trivial pull requests,</li> <li>contribute 5 non-trivial pull requests and have them merged,</li> </ul> </li> <li>ability to write quality code and/or documentation,</li> <li>ability to collaborate with the team,</li> <li>understanding of how the team works (policies, processes for testing and code review, etc),</li> <li>understanding of the project's code base and coding and documentation style.    </li> </ul> <p>A new Maintainer must be proposed by an existing maintainer by sending a message to the developer mailing list. A simple majority  vote of existing Maintainers approves the application.</p> <p>Maintainers who are selected will be granted the necessary GitHub rights, and invited to the private maintainer mailing list.</p>"},{"location":"Contribution%20guidelines/governance/#bootstrapping-maintainers","title":"Bootstrapping Maintainers","text":"<p>To bootstrap the process, 3 maintainers are defined (in the initial PR adding  this to the repository) that do not necessarily follow the above rules. When a  new maintainer is added following the above rules, the existing maintainers  define one not following the rules to step down, until all of them follow the  rules.</p>"},{"location":"Contribution%20guidelines/governance/#removing-a-maintainer","title":"Removing a Maintainer","text":"<p>Maintainers may resign at any time if they feel that they will not be able to  continue fulfilling their project duties.</p> <p>Maintainers may also be removed after being inactive, failure to fulfill their  Maintainer responsibilities, violating the Code of Conduct, or other reasons.  Inactivity is defined as a period of very low or no activity in the project for  a year or more, with no definite schedule to return to full Maintainer activity.</p> <p>A Maintainer may be removed at any time by a 2/3 vote of the remaining maintainers.</p> <p>Depending on the reason for removal, a Maintainer may be converted to Emeritus  status. Emeritus Maintainers will still be consulted on some project matters,  and can be rapidly returned to Maintainer status if their availability changes.</p>"},{"location":"Contribution%20guidelines/governance/#meetings","title":"Meetings","text":"<p>Time zones permitting, Maintainers are expected to participate in the public  community call meeting. Maintainers will also have closed meetings in order to  discuss security reports or Code of Conduct violations. Such meetings should be  scheduled by any Maintainer on receipt of a security issue or CoC report.  All current Maintainers must be invited to such closed meetings, except for any  Maintainer who is accused of a CoC violation.</p>"},{"location":"Contribution%20guidelines/governance/#code-of-conduct","title":"Code of Conduct","text":"<p>Code of Conduct violations by community members will be discussed and resolved on the private Maintainer mailing list.</p>"},{"location":"Contribution%20guidelines/governance/#security-response-team","title":"Security Response Team","text":"<p>The Maintainers will appoint a Security Response Team to handle security reports. This committee may simply consist of the Maintainer Council themselves. If this  responsibility is delegated, the Maintainers will appoint a team of at least two  contributors to handle it. The Maintainers will review who is assigned to this  at least once a year.</p> <p>The Security Response Team is responsible for handling all reports of security  holes and breaches according to the security policy.</p>"},{"location":"Contribution%20guidelines/governance/#voting","title":"Voting","text":"<p>While most business in KubeStellar is conducted by \"lazy consensus\", periodically the Maintainers may need to vote on specific actions or changes. A vote can be taken on the developer mailing list or the private Maintainer mailing list for security or conduct matters.  Votes may also be taken at the community call  meeting. Any Maintainer may demand a vote be taken.</p> <p>Most votes require a simple majority of all Maintainers to succeed. Maintainers can be removed by a 2/3 majority vote of all Maintainers, and changes to this Governance require a 2/3 vote of all Maintainers.</p>"},{"location":"Contribution%20guidelines/governance/#modifying-this-charter","title":"Modifying this Charter","text":"<p>Changes to this Governance and its supporting documents may be approved by a  2/3 vote of the Maintainers.</p>"},{"location":"Contribution%20guidelines/onboarding/","title":"Onboarding","text":"<p>KubeStellar GitHub Organization On-boarding and Off-boarding Policy</p> <p>Effective Date: June 1st, 2023</p> <p>At KubeStellar we love our contributors.  Our contributors can make various valuable contributions to our project. They can actively engage in code development by submitting pull requests, implementing new features, or fixing bugs. Additionally, contributors can assist with testing, CICD, documentation, providing clear and comprehensive guides, tutorials, and examples. Moreover, they can contribute to the project by participating in discussions, offering feedback, and helping to improve overall community engagement and collaboration.</p> <ol> <li> <p>Introduction: The purpose of this policy is to ensure a smooth on-boarding and off-boarding process for members of the KubeStellar GitHub organization. This policy applies to all individuals joining or leaving the organization, including community contributors.</p> </li> <li> <p>On-boarding Process: 2.1. Access Request:</p> </li> <li>New members shall submit an access request, via a blank GitHub issue from the KubeStellar repository, mentioning all members of the OWNERS file.</li> <li>The access request should include the member's GitHub username and a brief description of their role and contributions to the KubeStellar project.</li> </ol> <p>2.2. Review and Approval: - The organization's maintainers or designated personnel will review the access request issue. - The maintainers will evaluate the request based on the member's role, contributions, and adherence to the organization's code of conduct. - Upon approval, the member will receive an invitation to join the KubeStellar GitHub organization.</p> <p>2.3. Getting Help: - The organization's maintainers are here to help contributors be efficient and confident in their collaboration effort. If you need help you can reach out to the maintainers on slack at the KubeStellar-dev channel. - Be sure to join the KubeStellar-dev Google Group to get access to important artifacts like proposals, diagrams, and meeting invitations.</p> <p>2.4. Orientation: - Newly on-boarded members will be provided with contribution guidelines. - The guide will include instructions on how to access relevant repositories, participate in discussions, and contribute to ongoing projects.</p> <ol> <li>Off-boarding Process: 3.1. Departure Notification:</li> <li>Members leaving the organization shall notify the maintainers or their respective team lead in advance of their departure date.</li> <li>The notification should include the member's departure date and any necessary transition information.</li> </ol> <p>3.2. Access Termination: - Upon receiving the departure notification, the maintainers or designated personnel will initiate the off-boarding process. - The member's access to the KubeStellar GitHub organization will be revoked promptly to ensure data security and prevent unauthorized access.</p> <p>3.3. Knowledge Transfer: - Departing members should facilitate the transfer of their ongoing projects, tasks, and knowledge to their respective replacements or relevant team members. - Documentation or guidelines related to ongoing projects should be updated and made available to the team for seamless continuity.</p> <ol> <li>Code of Conduct:</li> <li>All members of the KubeStellar GitHub organization are expected to adhere to the organization's code of conduct, promoting a respectful and inclusive environment.</li> <li> <p>Violations of the code of conduct will be addressed following the organization's established procedures for handling such incidents.</p> </li> <li> <p>Policy Compliance:</p> </li> <li>It is the responsibility of all members to comply with the on-boarding and off-boarding policy.</li> <li> <p>The organization's maintainers or designated personnel will oversee the implementation and enforcement of this policy.</p> </li> <li> <p>Policy Review:</p> </li> <li>This policy will be reviewed periodically to ensure its effectiveness and relevance.</li> <li>Any updates or revisions to the policy will be communicated to the organization's members in a timely manner.</li> </ol> <p>Please note that this policy is subject to change, and any modifications will be communicated to all members of the KubeStellar GitHub organization.</p> <p>By joining the organization, all members agree to abide by the terms and guidelines outlined in this policy.</p> <p>Andy Anderson (clubanderson) KubeStellar Maintainer June 1, 2023</p>"},{"location":"Contribution%20guidelines/operations/all-macros/","title":"All macros","text":"<p>All variables supported by this documentation implementation:</p>"},{"location":"Contribution%20guidelines/operations/all-macros/#macros-plugin-environment","title":"Macros Plugin Environment","text":""},{"location":"Contribution%20guidelines/operations/all-macros/#general-list","title":"General List","text":"<p>All available variables and filters within the macros plugin:</p> Variable Type Content extra dict version [dict], analytics [dict] config MkDocsConfig {'config_file_path': '/home/runner/work/kubestellar/kubestellar/docs/mike-mkdocsqxjpuwkp.yml', 'site_name': 'KubeStellar', 'nav': [{'Welcome': [{'Landing': 'index.md'}]}, {'Overview': [{'Readme': 'readme.md'}, {'Architecture': 'direct/architecture.md'}, {'Release-notes': 'direct/release-notes.md'}]}, {'Getting Started': [{'Pre-reqs': 'direct/pre-reqs.md'}, {'Setting up KubeStellar': [{'Using existing hosting cluster': 'direct/hosting-cluster.md'}, {'KubeStellar on KIND': 'direct/ks-on-kind.md'}, {'KubeStellar on K3D': 'direct/deploy-on-k3d.md'}, {'KubeStellar on OCP': 'direct/ks-on-ocp.md'}]}, {'Examples': 'direct/examples.md'}]}, {'Contributing': [{'Overview of contributing': 'direct/contributor.md'}, {'Packaging': 'direct/packaging.md'}, {'Release Process': 'direct/release.md'}, {'Guidelines': 'Contribution guidelines/CONTRIBUTING.md'}, {'Code of Conduct': 'Contribution guidelines/coc.md'}, {'License': 'Contribution guidelines/LICENSE.md'}, {'Governance': 'Contribution guidelines/governance.md'}, {'Onboarding': 'Contribution guidelines/onboarding.md'}, {'Website': 'Contribution guidelines/operations/document-management.md'}, {'Security': [{'Policy': 'Contribution guidelines/security/security.md'}, {'Contacts': 'Contribution guidelines/security/security_contacts.md'}]}]}, {'Community': [{'Get Involved': 'Community/_index.md'}, {'Contact Us': [{'Mailing List': 'https://kubestellar.io/join_us'}, {'Community Meeting Agenda (join mailing list first)': 'https://kubestellar.io/agenda'}, {'Slack': 'https://kubestellar.io/slack'}, {'Medium Blog': 'https://kubestellar.io/blog'}, {'YouTube Channel': 'https://kubestellar.io/tv'}, {'LinkedIn': 'https://kubestellar.io/linkedin'}, {'Reddit': 'https://www.reddit.com/r/kubestellar/'}, {'Google Drive': 'https://drive.google.com/drive/u/1/folders/1p68MwkX0sYdTvtup0DcnAEsnXElobFLS'}]}, {'Partners': [{'ArgoCD': 'Community/partners/argocd.md'}, {'Turbonomic': 'Community/partners/turbonomic.md'}, {'MVI': 'Community/partners/mvi.md'}, {'FluxCD': 'Community/partners/fluxcd.md'}, {'OpenZiti': 'Community/partners/openziti.md'}, {'Kyverno': 'Community/partners/kyverno.md'}]}]}, {'Blog': 'https://medium.com/@kubestellar/list/predefined:e785a0675051:READING_LIST\" target=\"_blank'}], 'pages': None, 'site_url': 'https://docs.kubestellar.io/main', 'site_description': None, 'site_author': None, 'theme': Theme(name='material', dirs=['/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/mkdocs_static_i18n/custom_i18n_sitemap', '/home/runner/work/kubestellar/kubestellar/docs/overrides', '/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/material', '/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/mkdocs/templates'], static_templates=['404.html', 'sitemap.xml'], name='material', locale=Locale(language='en', territory=''), language='en', direction=None, features=['content.action.edit', 'content.action.view', 'navigation.indexes', 'navigation.tabs', 'navigation.path', 'navigation.footer', 'content.code.copy', 'content.code.annotate'], palette={'primary': None, 'accent': None}, font={'text': 'SapceMono', 'code': 'Roboto Mono'}, icon=None, favicon='favicons/favicon.ico', logo='logo.png'), 'docs_dir': '/home/runner/work/kubestellar/kubestellar/docs/content', 'site_dir': '/home/runner/work/kubestellar/kubestellar/docs/generated', 'copyright': None, 'google_analytics': None, 'dev_addr': _IpAddressValue(host='127.0.0.1', port=8000), 'use_directory_urls': True, 'repo_url': 'https://github.com/kubestellar/kubestellar', 'repo_name': 'GitHub', 'edit_uri_template': None, 'edit_uri': 'edit/main/docs/content/', 'extra_css': ['stylesheets/kubestellar.css'], 'extra_javascript': ['js/open_in_new_tab.js'], 'extra_templates': [], 'markdown_extensions': ['toc', 'tables', 'fenced_code', 'markdown_captions', 'pymdownx.superfences', 'attr_list', 'md_in_html', 'pymdownx.highlight', 'pymdownx.inlinehilite', 'pymdownx.snippets', 'pymdownx.tabbed', 'admonition'], 'mdx_configs': {'pymdownx.superfences': {'custom_fences': [{'name': 'mermaid', 'class': 'mermaid', 'format': }]}, 'toc': {'permalink': '#'}, 'pymdownx.highlight': {'anchor_linenums': True, 'pygments_lang_class': True}, 'pymdownx.tabbed': {'alternate_style': True}}, 'strict': False, 'remote_branch': 'gh-pages', 'remote_name': 'origin', 'extra': {'version': {'default': 'stable', 'provider': 'mike'}, 'analytics': {'provider': 'google', 'property': 'G-SR5TD1CXY7', 'feedback': {'title': 'Was this page helpful?', 'ratings': [{'icon': 'material/emoticon-happy-outline', 'name': 'This page was helpful', 'data': 1, 'note': 'Thanks for your feedback!'}, {'icon': 'material/emoticon-sad-outline', 'name': 'This page could be improved', 'data': 0, 'note': 'Thanks for your feedback! Help us improve this page by using our feedback form.'}]}}}, 'plugins': {'mike': , 'awesome-pages': , 'material/search': , 'mermaid2': , 'open-in-new-tab': , 'include-markdown': , 'macros': , 'i18n': }, 'hooks': {}, 'watch': ['/home/runner/work/kubestellar/kubestellar/docs/mkdocs.yml', '/home/runner/work/kubestellar/kubestellar/docs/content', '/home/runner/work/kubestellar/kubestellar/docs/overrides'], 'repo_short_name': 'kubestellar/kubestellar', 'repo_default_file_path': 'kubestellar', 'helm_repo_short_name': 'kubestellar/helm', 'helm_repo_default_file_path': 'helm', 'brew_repo_short_name': 'kubestellar/homebrew-kubestellar', 'brew_repo_default_file_path': 'homebrew-kubestellar', 'docs_url': 'https://docs.kubestellar.io', 'repo_raw_url': 'https://raw.githubusercontent.com/kubestellar/kubestellar', 'ks_branch': 'main', 'ks_tag': 'latest', 'ks_stable_tag': 'v0.9.0', 'ks_current_branch': 'release-0.15', 'ks_current_tag': 'v0.15.0', 'ks_current_helm_version': '8', 'ks_next_branch': 'release-0.16', 'ks_next_tag': 'v0.16.0', 'ks_next_helm_version': '9', 'ks_kind_port_num': '1119'} environment dict system = 'Linux', system_version = '6.5.0-1016-azure', python_version = '3.10.14', mkdocs_version = '1.4.2', macros_plugin_version = '0.7.0', jinja2_version = '3.1.2' plugin LegacyConfig {'module_name': 'main', 'modules': [], 'include_dir': 'overrides', 'include_yaml': [], 'j2_block_start_string': '', 'j2_block_end_string': '', 'j2_variable_start_string': '', 'j2_variable_end_string': '', 'on_undefined': 'keep', 'on_error_fail': False, 'verbose': False} git dict status = True, date [datetime], short_commit = '68829243d', commit = '68829243d46ea601acc56554fd0652c58ec0b2e3', tag = '', author = 'kcp CI Bot', author_email = '134318005+kcp-ci-bot@users.noreply.github.com', committer = 'GitHub', committer_email = 'noreply@github.com', date_ISO = 'Wed Apr 3 10:13:03 2024 +0200', message = 'Merge pull request #2005 from waltforme/render\\n\\n\ud83d\udcd6 Updates for mkdocs to better render the architecture webpage', raw = 'commit 68829243d46ea601acc56554fd0652c58ec0b2e3\\nAuthor: kcp CI Bot &lt;134318005+kcp-ci-bot@users.noreply.github.com&gt;\\nDate:   Wed Apr 3 10:13:03 2024 +0200\\n\\n    Merge pull request #2005 from waltforme/render\\n    \\n    \ud83d\udcd6 Updates for mkdocs to better render the architecture webpage', root_dir = '/home/runner/work/kubestellar/kubestellar' version dict default = 'stable', provider = 'mike' analytics dict provider = 'google', property = 'G-SR5TD1CXY7', feedback [dict] macros SuperDict context [function], macros_info [function], now [function], fix_url [function], section_items [function] filters dict pretty [function] filters_builtin dict abs [builtin_function_or_method], attr [function], batch [function], capitalize [function], center [function], count [builtin_function_or_method], d [function], default [function], dictsort [function], e [builtin_function_or_method], escape [builtin_function_or_method], filesizeformat [function], first [function], float [function], forceescape [function], format [function], groupby [function], indent [function], int [function], join [function], last [function], length [builtin_function_or_method], list [function], lower [function], items [function], map [function], min [function], max [function], pprint [function], random [function], reject [function], rejectattr [function], replace [function], reverse [function], round [function], safe [function], select [function], selectattr [function], slice [function], sort [function], string [builtin_function_or_method], striptags [function], sum [function], title [function], trim [function], truncate [function], unique [function], upper [function], urlencode [function], urlize [function], wordcount [function], wordwrap [function], xmlattr [function], tojson [function] navigation Navigation Section(title='Welcome')    Page(title='Landing', url='/main/')Section(title='Overview')    Page(title='Readme', url='/main/readme/')    Page(title='Architecture', url='/main/direct/architecture/')    Page(title='Release-notes', url='/main/direct/release-notes/')Section(title='Getting Started')    Page(title='Pre-reqs', url='/main/direct/pre-reqs/')    Section(title='Setting up KubeStellar')        Page(title='Using existing hosting cluster', url='/main/direct/hosting-cluster/')        Page(title='KubeStellar on KIND', url='/main/direct/ks-on-kind/')        Page(title='KubeStellar on K3D', url='/main/direct/deploy-on-k3d/')        Page(title='KubeStellar on OCP', url='/main/direct/ks-on-ocp/')    Page(title='Examples', url='/main/direct/examples/')Section(title='Contributing')    Page(title='Overview of contributing', url='/main/direct/contributor/')    Page(title='Packaging', url='/main/direct/packaging/')    Page(title='Release Process', url='/main/direct/release/')    Page(title='Guidelines', url='/main/Contribution%20guidelines/CONTRIBUTING/')    Page(title='Code of Conduct', url='/main/Contribution%20guidelines/coc/')    Page(title='License', url='/main/Contribution%20guidelines/LICENSE/')    Page(title='Governance', url='/main/Contribution%20guidelines/governance/')    Page(title='Onboarding', url='/main/Contribution%20guidelines/onboarding/')    Page(title='Website', url='/main/Contribution%20guidelines/operations/document-management/')    Section(title='Security')        Page(title='Policy', url='/main/Contribution%20guidelines/security/security/')        Page(title='Contacts', url='/main/Contribution%20guidelines/security/security_contacts/')Section(title='Community')    Page(title='Get Involved', url='/main/Community/_index/')    Section(title='Contact Us')        Link(title='Mailing List', url='https://kubestellar.io/join_us')        Link(title='Community Meeting Agenda (join mailing list first)', url='https://kubestellar.io/agenda')        Link(title='Slack', url='https://kubestellar.io/slack')        Link(title='Medium Blog', url='https://kubestellar.io/blog')        Link(title='YouTube Channel', url='https://kubestellar.io/tv')        Link(title='LinkedIn', url='https://kubestellar.io/linkedin')        Link(title='Reddit', url='https://www.reddit.com/r/kubestellar/')        Link(title='Google Drive', url='https://drive.google.com/drive/u/1/folders/1p68MwkX0sYdTvtup0DcnAEsnXElobFLS')    Section(title='Partners')        Page(title='ArgoCD', url='/main/Community/partners/argocd/')        Page(title='Turbonomic', url='/main/Community/partners/turbonomic/')        Page(title='MVI', url='/main/Community/partners/mvi/')        Page(title='FluxCD', url='/main/Community/partners/fluxcd/')        Page(title='OpenZiti', url='/main/Community/partners/openziti/')        Page(title='Kyverno', url='/main/Community/partners/kyverno/')Link(title='Blog', url='https://medium.com/@kubestellar/list/predefined:e785a0675051:READING_LIST\" target=\"_blank') files I18nFiles page Page Page(title='All macros', url='/main/Contribution%20guidelines/operations/all-macros/')"},{"location":"Contribution%20guidelines/operations/all-macros/#config-information","title":"Config Information","text":"<p>Standard MkDocs configuration information. Do not try to modify.</p> <p>e.g. <code>{{ config.docs_dir }}</code></p> <p>See also the MkDocs documentation on the config object.</p> Variable Type Content config_file_path str '/home/runner/work/kubestellar/kubestellar/docs/mike-mkdocsqxjpuwkp.yml' site_name str 'KubeStellar' nav list [{'Welcome': [{'Landing': 'index.md'}]}, {'Overview': [{'Readme': 'readme.md'}, {'Architecture': 'direct/architecture.md'}, {'Release-notes': 'direct/release-notes.md'}]}, {'Getting Started': [{'Pre-reqs': 'direct/pre-reqs.md'}, {'Setting up KubeStellar': [{'Using existing hosting cluster': 'direct/hosting-cluster.md'}, {'KubeStellar on KIND': 'direct/ks-on-kind.md'}, {'KubeStellar on K3D': 'direct/deploy-on-k3d.md'}, {'KubeStellar on OCP': 'direct/ks-on-ocp.md'}]}, {'Examples': 'direct/examples.md'}]}, {'Contributing': [{'Overview of contributing': 'direct/contributor.md'}, {'Packaging': 'direct/packaging.md'}, {'Release Process': 'direct/release.md'}, {'Guidelines': 'Contribution guidelines/CONTRIBUTING.md'}, {'Code of Conduct': 'Contribution guidelines/coc.md'}, {'License': 'Contribution guidelines/LICENSE.md'}, {'Governance': 'Contribution guidelines/governance.md'}, {'Onboarding': 'Contribution guidelines/onboarding.md'}, {'Website': 'Contribution guidelines/operations/document-management.md'}, {'Security': [{'Policy': 'Contribution guidelines/security/security.md'}, {'Contacts': 'Contribution guidelines/security/security_contacts.md'}]}]}, {'Community': [{'Get Involved': 'Community/_index.md'}, {'Contact Us': [{'Mailing List': 'https://kubestellar.io/join_us'}, {'Community Meeting Agenda (join mailing list first)': 'https://kubestellar.io/agenda'}, {'Slack': 'https://kubestellar.io/slack'}, {'Medium Blog': 'https://kubestellar.io/blog'}, {'YouTube Channel': 'https://kubestellar.io/tv'}, {'LinkedIn': 'https://kubestellar.io/linkedin'}, {'Reddit': 'https://www.reddit.com/r/kubestellar/'}, {'Google Drive': 'https://drive.google.com/drive/u/1/folders/1p68MwkX0sYdTvtup0DcnAEsnXElobFLS'}]}, {'Partners': [{'ArgoCD': 'Community/partners/argocd.md'}, {'Turbonomic': 'Community/partners/turbonomic.md'}, {'MVI': 'Community/partners/mvi.md'}, {'FluxCD': 'Community/partners/fluxcd.md'}, {'OpenZiti': 'Community/partners/openziti.md'}, {'Kyverno': 'Community/partners/kyverno.md'}]}]}, {'Blog': 'https://medium.com/@kubestellar/list/predefined:e785a0675051:READING_LIST\" target=\"_blank'}] pages NoneType None site_url str 'https://docs.kubestellar.io/main' site_description NoneType None site_author NoneType None theme Theme Theme(name='material', dirs=['/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/mkdocs_static_i18n/custom_i18n_sitemap', '/home/runner/work/kubestellar/kubestellar/docs/overrides', '/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/material', '/home/runner/work/kubestellar/kubestellar/docs/venv/lib/python3.10/site-packages/mkdocs/templates'], static_templates=['404.html', 'sitemap.xml'], name='material', locale=Locale(language='en', territory=''), language='en', direction=None, features=['content.action.edit', 'content.action.view', 'navigation.indexes', 'navigation.tabs', 'navigation.path', 'navigation.footer', 'content.code.copy', 'content.code.annotate'], palette={'primary': None, 'accent': None}, font={'text': 'SapceMono', 'code': 'Roboto Mono'}, icon=None, favicon='favicons/favicon.ico', logo='logo.png') docs_dir str '/home/runner/work/kubestellar/kubestellar/docs/content' site_dir str '/home/runner/work/kubestellar/kubestellar/docs/generated' copyright NoneType None google_analytics NoneType None dev_addr _IpAddressValue _IpAddressValue(host='127.0.0.1', port=8000) use_directory_urls bool True repo_url str 'https://github.com/kubestellar/kubestellar' repo_name str 'GitHub' edit_uri_template NoneType None edit_uri str 'edit/main/docs/content/' extra_css list ['stylesheets/kubestellar.css'] extra_javascript list ['js/open_in_new_tab.js'] extra_templates list [] markdown_extensions list ['toc', 'tables', 'fenced_code', 'markdown_captions', 'pymdownx.superfences', 'attr_list', 'md_in_html', 'pymdownx.highlight', 'pymdownx.inlinehilite', 'pymdownx.snippets', 'pymdownx.tabbed', 'admonition'] mdx_configs dict pymdownx.superfences [dict], toc [dict], pymdownx.highlight [dict], pymdownx.tabbed [dict] strict bool False remote_branch str 'gh-pages' remote_name str 'origin' extra LegacyConfig {'version': {'default': 'stable', 'provider': 'mike'}, 'analytics': {'provider': 'google', 'property': 'G-SR5TD1CXY7', 'feedback': {'title': 'Was this page helpful?', 'ratings': [{'icon': 'material/emoticon-happy-outline', 'name': 'This page was helpful', 'data': 1, 'note': 'Thanks for your feedback!'}, {'icon': 'material/emoticon-sad-outline', 'name': 'This page could be improved', 'data': 0, 'note': 'Thanks for your feedback! Help us improve this page by using our feedback form.'}]}}} plugins PluginCollection mike [MikePlugin], awesome-pages [AwesomePagesPlugin], material/search [SearchPlugin], mermaid2 [MarkdownMermaidPlugin], open-in-new-tab [OpenInNewTabPlugin], include-markdown [IncludeMarkdownPlugin], macros [MacrosPlugin], i18n [I18n] hooks dict watch list ['/home/runner/work/kubestellar/kubestellar/docs/mkdocs.yml', '/home/runner/work/kubestellar/kubestellar/docs/content', '/home/runner/work/kubestellar/kubestellar/docs/overrides'] repo_short_name str 'kubestellar/kubestellar' repo_default_file_path str 'kubestellar' helm_repo_short_name str 'kubestellar/helm' helm_repo_default_file_path str 'helm' brew_repo_short_name str 'kubestellar/homebrew-kubestellar' brew_repo_default_file_path str 'homebrew-kubestellar' docs_url str 'https://docs.kubestellar.io' repo_raw_url str 'https://raw.githubusercontent.com/kubestellar/kubestellar' ks_branch str 'main' ks_tag str 'latest' ks_stable_tag str 'v0.9.0' ks_current_branch str 'release-0.15' ks_current_tag str 'v0.15.0' ks_current_helm_version str '8' ks_next_branch str 'release-0.16' ks_next_tag str 'v0.16.0' ks_next_helm_version str '9' ks_kind_port_num str '1119'"},{"location":"Contribution%20guidelines/operations/all-macros/#macros","title":"Macros","text":"<p>These macros have been defined programmatically for this environment (module or pluglets). </p> Variable Type Content context function (obj, e) <p>Default mkdocs_macro List the defined variables</p> macros_info function () <p>Test/debug function:         list useful documentation on the mkdocs_macro environment.</p> now function () <p>Get the current time (returns a datetime object).          Used alone, it provides a timestamp.         To get the year use <code>now().year</code>, for the month number          <code>now().month</code>, etc.</p> fix_url function (url, r) <p>If url is relative, fix it so that it points to the docs diretory.     This is necessary because relative links in markdown must be adapted     in html ('img/foo.png' =&gt; '../img/img.png').</p> section_items function (page, nav, config, children, siblings, child) <p>Returns a list of all pages that are siblings to page.</p>"},{"location":"Contribution%20guidelines/operations/all-macros/#git-information","title":"Git Information","text":"<p>Information available on the last commit and the git repository containing the documentation project:</p> <p>e.g. <code>{{ git.message }}</code></p> Variable Type Content status bool True date datetime datetime.datetime(2024, 4, 3, 10, 13, 3, tzinfo=tzoffset(None, 7200)) short_commit str '68829243d' commit str '68829243d46ea601acc56554fd0652c58ec0b2e3' tag str '' author str 'kcp CI Bot' author_email str '134318005+kcp-ci-bot@users.noreply.github.com' committer str 'GitHub' committer_email str 'noreply@github.com' date_ISO str 'Wed Apr 3 10:13:03 2024 +0200' message str 'Merge pull request #2005 from waltforme/render\\n\\n\ud83d\udcd6 Updates for mkdocs to better render the architecture webpage' raw str 'commit 68829243d46ea601acc56554fd0652c58ec0b2e3\\nAuthor: kcp CI Bot &lt;134318005+kcp-ci-bot@users.noreply.github.com&gt;\\nDate:   Wed Apr 3 10:13:03 2024 +0200\\n\\n    Merge pull request #2005 from waltforme/render\\n    \\n    \ud83d\udcd6 Updates for mkdocs to better render the architecture webpage' root_dir str '/home/runner/work/kubestellar/kubestellar'"},{"location":"Contribution%20guidelines/operations/all-macros/#page-attributes","title":"Page Attributes","text":"<p>Provided by MkDocs. These attributes change for every page (the attributes shown are for this page).</p> <p>e.g. <code>{{ page.title }}</code></p> <p>See also the MkDocs documentation on the page object.</p> Variable Type Content file I18nFile I18nFile(src_path='Contribution guidelines/operations/all-macros.md', abs_src_path='/home/runner/work/kubestellar/kubestellar/docs/content/Contribution guidelines/operations/all-macros.md', dest_path='Contribution guidelines/operations/all-macros/index.html', abs_dest_path='/home/runner/work/kubestellar/kubestellar/docs/generated/Contribution guidelines/operations/all-macros/index.html', name='all-macros', locale_suffix='None', dest_language='', dest_name='all-macros.md', url='Contribution%20guidelines/operations/all-macros/') title str 'All macros' parent NoneType None children NoneType None previous_page NoneType None next_page NoneType None _Page__active bool False update_date str '2024-04-03' canonical_url str 'https://docs.kubestellar.io/main/Contribution%20guidelines/operations/all-macros/' abs_url str '/main/Contribution%20guidelines/operations/all-macros/' edit_url str 'https://github.com/kubestellar/kubestellar/edit/main/docs/content/Contribution guidelines/operations/all-macros.md' markdown str 'All variables supported by this documentation implementation:\\n\\n{{ macros_info() }}' content NoneType None toc list [] meta dict <p>To have all titles of all pages, use:</p> <pre><code>{% for page in navigation.pages %}\n- {{ page.title }}\n{% endfor %}\n</code></pre>"},{"location":"Contribution%20guidelines/operations/all-macros/#plugin-filters","title":"Plugin Filters","text":"<p>These filters are provided as a standard by the macros plugin.</p> Variable Type Content pretty function (var_list, rows, header, e) <p>Default mkdocs_macro Prettify a dictionary or object          (used for environment documentation, or debugging).</p>"},{"location":"Contribution%20guidelines/operations/all-macros/#builtin-jinja2-filters","title":"Builtin Jinja2 Filters","text":"<p>These filters are provided by Jinja2 as a standard.</p> <p>See also the Jinja2 documentation on builtin filters).</p> Variable Type Content abs builtin_function_or_method <p>Return the absolute value of the argument.</p> attr function (environment, obj, name, value) <p>Get an attribute of an object.  <code>foo|attr(\"bar\")</code> works like     <code>foo.bar</code> just that always an attribute is returned and items are not     looked up.</p> batch function (value, linecount, fill_with, tmp, item) <p>A filter that batches items. It works pretty much like <code>slice</code>     just the other way round. It returns a list of lists with the     given number of items. If you provide a second parameter this     is used to fill up missing items. See this example.</p> capitalize function (s) <p>Capitalize a value. The first character will be uppercase, all others     lowercase.</p> center function (value, width) <p>Centers the value in a field of a given width.</p> count builtin_function_or_method <p>Return the number of items in a container.</p> d function (value, default_value, boolean) <p>If the value is undefined it will return the passed default value,     otherwise the value of the variable.</p> default function (value, default_value, boolean) <p>If the value is undefined it will return the passed default value,     otherwise the value of the variable.</p> dictsort function (value, case_sensitive, by, reverse, sort_func) <p>Sort a dict and yield (key, value) pairs. Python dicts may not     be in the order you want to display them in, so sort them first.</p> e builtin_function_or_method <p>Replace the characters <code>&amp;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>'</code>, and <code>\"</code> in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML.</p> escape builtin_function_or_method <p>Replace the characters <code>&amp;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>'</code>, and <code>\"</code> in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML.</p> filesizeformat function (value, binary, bytes, base, prefixes, i, prefix, unit) <p>Format the value like a 'human-readable' file size (i.e. 13 kB,     4.1 MB, 102 Bytes, etc).  Per default decimal prefixes are used (Mega,     Giga, etc.), if the second parameter is set to <code>True</code> the binary     prefixes are used (Mebi, Gibi).</p> first function (args, kwargs, b) <p>Return the first item of a sequence.</p> float function (value, default) <p>Convert the value into a floating point number. If the     conversion doesn't work it will return <code>0.0</code>. You can     override this default using the first parameter.</p> forceescape function (value) <p>Enforce HTML escaping.  This will probably double escape variables.</p> format function (value, args, kwargs) <p>Apply the given values to a <code>printf-style</code>_ format string, like     <code>string % values</code>.</p> groupby function (args, kwargs, b) <p>Group a sequence of objects by an attribute using Python's     :func:<code>itertools.groupby</code>. The attribute can use dot notation for     nested access, like <code>\"address.city\"</code>. Unlike Python's <code>groupby</code>,     the values are sorted first so only one group is returned for each     unique value.</p> indent function (s, width, first, blank, newline, rv, lines) <p>Return a copy of the string with each line indented by 4 spaces. The     first line and blank lines are not indented by default.</p> int function (value, default, base) <p>Convert the value into an integer. If the     conversion doesn't work it will return <code>0</code>. You can     override this default using the first parameter. You     can also override the default base (10) in the second     parameter, which handles input with prefixes such as     0b, 0o and 0x for bases 2, 8 and 16 respectively.     The base is ignored for decimal numbers and non-string values.</p> join function (args, kwargs, b) <p>Return a string which is the concatenation of the strings in the     sequence. The separator between elements is an empty string per     default, you can define it with the optional parameter.</p> last function (environment, seq) <p>Return the last item of a sequence.</p> length builtin_function_or_method <p>Return the number of items in a container.</p> list function (args, kwargs, b) <p>Convert the value into a list.  If it was a string the returned list     will be a list of characters.</p> lower function (s) <p>Convert a value to lowercase.</p> items function (value) <p>Return an iterator over the <code>(key, value)</code> items of a mapping.</p> map function (args, kwargs, b) <p>Applies a filter on a sequence of objects or looks up an attribute.     This is useful when dealing with lists of objects but you are really     only interested in a certain value of it.</p> min function (environment, value, case_sensitive, attribute) <p>Return the smallest item from the sequence.</p> max function (environment, value, case_sensitive, attribute) <p>Return the largest item from the sequence.</p> pprint function (value) <p>Pretty print a variable. Useful for debugging.</p> random function (context, seq) <p>Return a random item from the sequence.</p> reject function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to each object,     and rejecting the objects with the test succeeding.</p> rejectattr function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to the specified     attribute of each object, and rejecting the objects with the test     succeeding.</p> replace function (eval_ctx, s, old, new, count) <p>Return a copy of the value with all occurrences of a substring     replaced with a new one. The first argument is the substring     that should be replaced, the second is the replacement string.     If the optional third argument <code>count</code> is given, only the first     <code>count</code> occurrences are replaced.</p> reverse function (value, rv, e) <p>Reverse the object or return an iterator that iterates over it the other     way round.</p> round function (value, precision, method, func) <p>Round the number to a given precision. The first     parameter specifies the precision (default is <code>0</code>), the     second the rounding method.</p> safe function (value) <p>Mark the value as safe which means that in an environment with automatic     escaping enabled this variable will not be escaped.</p> select function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to each object,     and only selecting the objects with the test succeeding.</p> selectattr function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to the specified     attribute of each object, and only selecting the objects with the     test succeeding.</p> slice function (args, kwargs, b) <p>Slice an iterator and return a list of lists containing     those items. Useful if you want to create a div containing     three ul tags that represent columns.</p> sort function (environment, value, reverse, case_sensitive, attribute, key_func) <p>Sort an iterable using Python's :func:<code>sorted</code>.</p> string builtin_function_or_method <p>Convert an object to a string if it isn't already. This preserves a :class:<code>Markup</code> string rather than converting it back to a basic string, so it will still be marked as safe and won't be escaped again.</p> striptags function (value) <p>Strip SGML/XML tags and replace adjacent whitespace by one space.</p> sum function (args, kwargs, b) <p>Returns the sum of a sequence of numbers plus the value of parameter     'start' (which defaults to 0).  When the sequence is empty it returns     start.</p> title function (s) <p>Return a titlecased version of the value. I.e. words will start with     uppercase letters, all remaining characters are lowercase.</p> trim function (value, chars) <p>Strip leading and trailing characters, by default whitespace.</p> truncate function (env, s, length, killwords, end, leeway, result) <p>Return a truncated copy of the string. The length is specified     with the first parameter which defaults to <code>255</code>. If the second     parameter is <code>true</code> the filter will cut the text at length. Otherwise     it will discard the last word. If the text was in fact     truncated it will append an ellipsis sign (<code>\"...\"</code>). If you want a     different ellipsis sign than <code>\"...\"</code> you can specify it using the     third parameter. Strings that only exceed the length by the tolerance     margin given in the fourth parameter will not be truncated.</p> unique function (environment, value, case_sensitive, attribute, getter, seen, item, key) <p>Returns a list of unique items from the given iterable.</p> upper function (s) <p>Convert a value to uppercase.</p> urlencode function (value, items) <p>Quote data for use in a URL path or query using UTF-8.</p> urlize function (eval_ctx, value, trim_url_limit, nofollow, target, rel, extra_schemes, policies, rel_parts, scheme, rv) <p>Convert URLs in text into clickable links.</p> wordcount function (s) <p>Count the words in that string.</p> wordwrap function (environment, s, width, break_long_words, wrapstring, break_on_hyphens) <p>Wrap a string to the given width. Existing newlines are treated     as paragraphs to be wrapped separately.</p> xmlattr function (eval_ctx, d, autospace, rv) <p>Create an SGML/XML attribute string based on the items in a dict.     All values that are neither <code>none</code> nor <code>undefined</code> are automatically     escaped.</p> tojson function (eval_ctx, value, indent, policies, dumps, kwargs) <p>Serialize an object to a string of JSON, and mark it safe to     render in HTML. This filter is only for use in HTML documents.</p>"},{"location":"Contribution%20guidelines/operations/code-management/","title":"Code management","text":""},{"location":"Contribution%20guidelines/operations/code-management/#code-management","title":"Code Management","text":"<p>Fork kubestellar into your own repo, create a local branch, set upstream to kubestellar, add and commit changes to local branch, and squash your commits</p>"},{"location":"Contribution%20guidelines/operations/code-management/#initial-setup","title":"Initial setup","text":""},{"location":"Contribution%20guidelines/operations/code-management/#fork-the-github-kubestellar-repo-into-your-own-github-repo","title":"Fork the Github kubestellar repo into your own Github repo:","text":"<p>You can do this either 1: from the kubestellar Github website using the \"Fork\" button or 2: by using the git fork command from your local git command line interface, such as git bash.</p> <p>copy the forked repo from Github to your local system by using the \"git clone\" command or by downloading the repository's zip file.</p> <p>In your new local forked repo, set upstream to kubestellar main</p> <p>check what your repo's remote settings are <pre><code>git remote -v\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/code-management/#set-upstream-to-use-kubestellar","title":"Set upstream to use kubestellar:","text":"<pre><code>git remote add upstream git@github.com:kubestellar/kubestellar.git\n</code></pre> <p>For example: <pre><code>owner@BOOK-U0EMIUAFHD MINGW64 ~/src/edge-mc (main)\n git remote -v\norigin  git@github.com:fileppb/edge-mc.git (fetch)\norigin  git@github.com:fileppb/edge-mc.git (push)\n\nowner@BOOK-U0EMIUAFHD MINGW64 ~/src/edge-mc (main)\n git remote add upstream git@github.com:kubestellar/kubestellar.git\n\nowner@BOOK-U0EMIUAFHD MINGW64 ~/src/edge-mc (main)\n git remote -v\norigin  git@github.com:fileppb/edge-mc.git (fetch)\norigin  git@github.com:fileppb/edge-mc.git (push)\nupstream        git@github.com:kubestellar/kubestellar.git (fetch)\nupstream        git@github.com:kubestellar/kubestellar.git (push)\n\nowner@BOOK-U0EMIUAFHD MINGW64 ~/src/edge-mc (main)\n git fetch upstream\nEnter passphrase for key '/c/Users/owner/.ssh/id_rsa':\nremote: Enumerating objects: 60394, done.\nremote: Counting objects: 100% (5568/5568), done.\nremote: Compressing objects: 100% (255/255), done.\nremote: Total 60394 (delta 4768), reused 5457 (delta 4706), pack-reused 54826\nReceiving objects: 100% (60394/60394), 52.38 MiB | 3.25 MiB/s, done.\nResolving deltas: 100% (34496/34496), completed with 415 local objects.\n\nowner@BOOK-U0EMIUAFHD MINGW64 ~/src/edge-mc (main)\n git status\n\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/code-management/#ongoing-contributions","title":"Ongoing contributions","text":""},{"location":"Contribution%20guidelines/operations/code-management/#prior-to-working-on-an-issue","title":"Prior to working on an issue","text":"<p>Ensure that you personal repository if up to date with the kubestellar repository. You can do this by opening your github repository page, check that the selected branch is \"main\", and press the \"sync fork\" button.</p>"},{"location":"Contribution%20guidelines/operations/code-management/#select-an-issue-to-work-on-and-create-a-local-branch","title":"Select an issue to work on and create a local branch,","text":"<p>Create a local branch for your work, preferably including the issue number in the branch name</p> <p>for example if working on issue #11187, then you might name your local branch \"issue-1187\" <pre><code>git checkout -b issue-1187\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/code-management/#as-you-work-and-change-files-you-should-try-to-commit-relatively-small-pieces-of-work-using-the-following-commands","title":"As you work and change files, you should try to commit relatively small pieces of work, using the following commands","text":"<pre><code>git add (there are several options you can specify for the git add command)\n\ngit commit -m \"your message\"\n\ngit push -u origin branch-name (-u sets upstream to origin which is your remote github repository)\n</code></pre>"},{"location":"Contribution%20guidelines/operations/code-management/#when-you-have-completed-your-work-and-tested-it-locally-then-you-should-perform-a-squash-of-the-git-commits-to-make-the-upcoming-push-request-more-manageable","title":"When you have completed your work and tested it locally, then you should perform a squash of the git commits to make the upcoming push request more manageable.","text":"<p>To perform a squash, checkout the branch you want to squash, 1. use the \"git log\" command to see the history of commits to the branch 2. Count the number of commits you want to squash 3. use the \"git rebase -i HEAD~n\" where n is the number of commits you would like to squash together. (There are other ways to do this) 4. The text editor you have configured to use with git should automagically open your source and you will see a list of commits preceded by \"pick\". Leaving the first \"pick\" as it is, replace the remaining \"pick\"s with \"squash\"es.  5. Save the text file and exit the editor. 6. The text editor will open again to let you edit comments for your new squashed commit. 7. Make your edits if any and save and exit the file. The commits will then be squashed into one commit.</p>"},{"location":"Contribution%20guidelines/operations/code-management/#when-you-are-done-with-the-squash-push-your-changes-to-your-remote-branch-you-can-either","title":"When you are done with the squash, push your changes to your remote branch. You can either:","text":"<p><pre><code>git push -u origin &lt;branch-name&gt;\n\nor \n\ngit push --force-with-lease\n</code></pre> Note: if using the git push -u origin  command, the -u only needs to specified the first time you push. It will set tracking for subsequent pushes to the branch. On the other hand, keeping the -u in the command does no particular harm."},{"location":"Contribution%20guidelines/operations/code-management/#run-actions-automated-workflow-tests-manually-in-your-personal-github-repository","title":"Run Actions (automated workflow tests) manually in your personal Github repository","text":"<ol> <li>Select the \"Actions\" tab toward the upper left of your github personal web page. This will cause a list of Actions to show.</li> <li>Select the action you wish to execute from the list of Actions. For example you might chose \"docs-ecutable - example1\". Note: docs-ecutable should be described in a separate section. But in a nutshell it's a Continuous Integration automation technique of embedding scripts and data within the body of documentation, and then parsing and executing those scripts which in turn interpret and execute source code from a branch that you designate. It's somewhat similar to Travis. So the Action \"docs-ecutable - example1\" executes scripts and data embedded within the documentation for the Example 1 scenario, described in the Kubestellar documents. Those scripts will run using the source code pointed to by the next step, step 3.</li> <li>Select the source code branch you wish to exercise by following the next 3 steps:</li> <li>select the black and white \"Run Workflow\" on the right side of your github web page. This will open a dialog box.</li> <li>within the dialog box, select the branch you wish to exercise by opening the dropdown labeled \"use workflow from\"</li> <li>within the dialog box, select the green \"Run Workflow\" button  Your selected Action workflow will execute and the results will be available when it completes.  </li> </ol>"},{"location":"Contribution%20guidelines/operations/code-management/#create-a-pull-request-pr-from-your-github-repo-branch-in-order-to-request-review-and-approval-from-the-kubestellar-team","title":"Create a Pull Request (PR) from your Github repo branch in order to request review and approval from the Kubestellar team","text":"<p>Take a look at https://github.com/kubestellar/kubestellar/blob/main/CONTRIBUTING.md</p> <p>You can create a Pull Request from your Github web repository by selecting the \"Compare &amp; pull request\" button.</p> <p>You will be presented with a Github web page titled Comparing Changes, which allows you to enter metadata regarding your pull request</p> <p>Reference the issue you are addressing ( add #issue-number) Add one of the listed emojis to the first character of the title of your new PR indicating the type of issue (bug fix, feature, etc) Complete the summary description field Complete the Related issue field by inserting the issue number preceded by the # character, for example \"#1187\" Decide whether this is a draft PR or if it's ready for review, and select the option you want by expanding on the Create Pull Reuest button. Assign a label to the PR from the available list of labels (a drop down list on the right side of the web page)</p> <p>Kubestellar CI pipeline:</p> <p>Prow (https://docs.prow.k8s.io/docs/overview/)</p>"},{"location":"Contribution%20guidelines/operations/document-management/","title":"Website","text":""},{"location":"Contribution%20guidelines/operations/document-management/#overview","title":"Overview","text":"<p>Our documentation is powered by mike and MkDocs. MkDocs is powered by Python-Markdown. These are immensely configurable and extensible. You can see our MkDocs configuration in <code>docs/mkdocs.yml</code>. Following are some of the choices we have made.</p> <ul> <li>The MkDocs theme is Material for MkDocs.</li> <li>MkDocs plugin awesome-pages for greater control over how navigation links are shown.</li> <li>MkDocs plugin macros.</li> <li>Our own slightly improved vintage of the <code>include-markdown</code> MkDocs plugin, allowing the source to be factored into re-used files.</li> <li>Python-Markdown extension SuperFences, supporting fenced code blocks that play nice with other markdown features.</li> <li>Python-Markdown extension Highlight, for syntax highlighting of fenced code.</li> <li>Pygments for even fancier code highlighting.</li> <li>MkDocs plugin mkdocs-static-i18n to support multiple languages. We currently only have documentation in English. If you're interested in contributing translations, please let us know!</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#serving-up-documents-locally","title":"Serving up documents locally","text":"<p>You can view and modify our documentation in your local development environment.  Simply checkout one of our branches.</p> <pre><code>git clone git@github.com:kubestellar/kubestellar.git\ncd kubestellar/docs\ngit checkout main\n</code></pre> <p>You can view and modify our documentation in the branch you have checked out by using <code>mkdocs serve</code> from mkdocs.  We have a Python requirements file in <code>requirements.txt</code>, and a Makefile target that builds a Python virtual environment and installs the requirements there.  You can either install those requirements into your global Python environment or use the Makefile target.  To install those requirements into your global Python environment, do the following usual thing.</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Alternatively, use the following commands to use the Makefile target to construct an adequate virtual environment and enter it.</p> <pre><code>( cd ..; make venv )\n. venv/bin/activate\n</code></pre> <p>Then, using your chosen environment with the requirements installed, build and serve the documents with the following command.</p> <p><pre><code>mkdocs serve\n</code></pre> Then open a browser to <code>http://localhost:8000/</code></p> <p>Another way to view (not modify - this method reflects what has been deployed to the <code>gh-pages</code> branch of our repo) all branches/versions of our documentation locally using 'mike' mike for mkdocs:</p> <p><pre><code>git clone git@github.com:kubestellar/kubestellar.git\ncd kubestellar\ngit checkout main\ncd docs\nmike set-default main\ncd ..\nmake serve-docs\n</code></pre> Then open a browser to <code>http://localhost:8000/</code></p>"},{"location":"Contribution%20guidelines/operations/document-management/#jinja-templating","title":"Jinja templating","text":"<p>Our documentation stack includes Jinja. The Jinja constructs --- {\\% ... \\%} for statements, {{ ... }} for expressions, and {# ... #} for comments --- can appear in the markdown sources.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#file-structure","title":"File structure","text":"<p>All documentation-related items live in <code>docs</code> (with the small exception of various <code>make</code> targets and some helper  scripts in <code>hack</code>).</p> <p>The structure of <code>docs</code> is as follows:</p> Path Description config/$language/mkdocs.yml Language-specific <code>mkdocs</code> configuration. content/$language Language-specific website content. generated/branch All generated content for all languages for the current version. generated/branch/$language Generated content for a single language. Never added to git. generated/branch/index.html Minimal index for the current version that redirects to the default language (en) overrides Global (not language-specific) content. Dockerfile Builds the kubestellar-docs image containing mkdocs + associated tooling. mkdocs.yml Minimal <code>mkdocs</code> configuration for <code>mike</code> for multi-version support. requirements.txt List of Python modules used to build the site."},{"location":"Contribution%20guidelines/operations/document-management/#global-variables","title":"Global Variables","text":"<p>There are many global variables defined in the docs/mkdocs.yml.  The following are some very common variables you are encouraged to use in our documentation.  Use of these variables/macros allows our documentation to have github branch context and take advantage of our evolution without breaking</p> <pre><code>- site_name: KubeStellar\n- repo_url: https://github.com/kubestellar/kubestellar\n- site_url: https://docs.kubestellar.io/main\n- repo_default_file_path: kubestellar\n- repo_short_name: kubestellar/kubestellar\n- docs_url: https://docs.kubestellar.io\n- repo_raw_url: https://raw.githubusercontent.com/kubestellar/kubestellar\n- edit_uri: edit/main/docs/content/\n- ks_branch: main\n- ks_tag: latest\n</code></pre> <p>to use a variables/macro in your documentation reference like this:</p> <p>{{ config.&lt;var_name&gt; }}</p> <p>and in context that can look something like this:</p> <p>bash &lt;(curl -s {{ config.repo_raw_url }}/{{ config.ks_branch }}/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version {{ config.ks_tag }}</p> <p>note:  \u00a0\u00a0\u00a0\u00a0- A more extensive and detailed list is located at mkdocs information  \u00a0\u00a0\u00a0\u00a0- We also check for broken links as part of our PR pipeline.  For more information check out our Broken Links Crawler</p>"},{"location":"Contribution%20guidelines/operations/document-management/#page-variables","title":"Page variables","text":"<p>A markdown source file can contribute additional variables by defining them in <code>name: value</code> lines at the start of the file, set off by lines of triple dashes. For example, suppose a markdown file begins with the following.</p> <pre><code>---\nshort_name: example1\nmanifest_name: 'docs/content/Coding Milestones/PoC2023q1/example1.md'\n---\n</code></pre> <p>These variables can be referenced as {{ page.meta.short_name }} and {{ page.meta.manifest_name }}.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#including-external-markdown","title":"Including external markdown","text":"<p>We make extensive use of 'include-markdown' to help us keep our documentation modular and up-to-date.  To use 'include-markdown' you must add a block in your document that refers to a block in your external document content:</p> <p>In your original markdown document, add a block that refers to the external markdown you want to include: Include Markdown </p> <p>In the document you want to include, add the start and end tags you configured in the include-markdown block in your original document: Included Markdown </p> <p>for more information on the 'include-markdown' plugin for mkdocs look here</p>"},{"location":"Contribution%20guidelines/operations/document-management/#supported-aliases-for-our-documentation","title":"Supported aliases for our documentation","text":"<p>We currently support 3 aliases for our documentation:</p> <pre><code>- from the release major.minor branch:\n    - [https://docs.kubestellar.io/stable](../../../https://docs.kubestellar.io/stable)\n- from the main branch:\n    - [https://docs.kubestellar.io/unstable](../../../https://docs.kubestellar.io/unstable)\n    - [https://docs.kubestellar.io/latest](../../../https://docs.kubestellar.io/latest)\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#shortcut-urls","title":"Shortcut URLs","text":"<p>We have a few shortcut urls that come in handy when referring others to our project:</p> <p>note: You need to join our mailing list first to get access to some of the links that follow (https://docs.kubestellar.io/joinus)</p> <ul> <li>https://kubestellar.io/agenda - our community meeting agenda google doc</li> <li>https://kubestellar.io/blog - our medium reading list</li> <li>https://kubestellar.io/code - our current GitHub repo (wherever that is)</li> <li>https://kubestellar.io/community - our stable docs community page</li> <li>https://kubestellar.io/drive - our google drive</li> <li>https://kubestellar.io/joinus - our dev mailing list where you join and get our invites</li> <li>https://kubestellar.io/join_us - also, our dev mailing list</li> <li>https://kubestellar.io/linkedin - our linkedin filter (soon, our page)</li> <li>https://kubestellar.io/tv - our youtube channel</li> <li>https://kubestellar.io/youtube - also, our youtube channel</li> <li>https://kubestellar.io/infomercial - our infomercial that premieres on June 12th at 9am</li> </ul> <p>and.. the very important\u2026 - https://kubestellar.io/quickstart - our 'stable' quickstart</p>"},{"location":"Contribution%20guidelines/operations/document-management/#codeblocks","title":"Codeblocks","text":"<p>mkdocs has some very helpful ways to include blocks of code in a style that makes it clear to our readers that console interaction is necessary in the documentation.  There are options to include a plain codeblock (```), shell (shell), console (console - no used in our documentation), language or format-specific (yaml, etc.), and others.  For more detailed information, checkout the mkdocs information on codeblocks.</p> <p>NOTE: the docs-ecutable technology does not apply Jinja, at any stage; Jinja source inside executed code blocks will not be expanded by Jinja but rather seen directly by <code>bash</code>.</p> <p>Here are some examples of how we use codeblocks.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#seen-and-executed","title":"Seen and executed","text":"<p>For a codeblock that can be 'tested' (and seen by the reader) as part of our CI, use the <code>shell</code> block: codeblock: <pre><code>```shell\nmkdocs serve\n```\n</code></pre> as seen by reader: <pre><code>mkdocs serve\n</code></pre> </p>"},{"location":"Contribution%20guidelines/operations/document-management/#executed-but-not-seen","title":"Executed but not seen","text":"<p>(Think hard before hiding stuff from your reader.)</p> <p>For a codeblock that should be 'tested', BUT not seen by the reader, use the <code>.bash</code> with the plain codeblock, and the '.hide-me' style (great for hiding a sleep command that user does not need to run, but CI does): codeblock: <pre><code>``` {.bash .hide-me}\nsleep 10\n```\n</code></pre> as seen by reader: <pre><code>\n</code></pre> </p>"},{"location":"Contribution%20guidelines/operations/document-management/#seen-but-not-executed","title":"Seen but not executed","text":"<p>(To avoid confusing readers of the HTML, this should be used only for output seen in a shell session.)</p> <p>For a codeblock that should not be 'tested' as part of our CI, use the <code>.bash</code> with the plain codeblock, and without the '.hide-me' style: codeblock: <pre><code>``` {.bash}\nmkdocs server\n```\n</code></pre> as seen by reader: <pre><code>mkdocs server\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#seen-but-not-executed-and-no-copy-button","title":"Seen but not executed and no copy button","text":"<p>For a codeblock that should not be 'tested', be seen by the reader, and not include a 'copy' icon (great for output-only instances), use the <code>.bash</code> codeblock without the '.no-copy' style: codeblock: <pre><code>``` {.bash .no-copy}\nI0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\n```\n</code></pre> as seen by reader: <pre><code>I0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#other-language-specific-highlighting","title":"Other language-specific highlighting","text":"<p>For other language-specific highlighting (yaml, etc.), use the yaml codeblock codeblock: <pre><code>```yaml\nnav:\n  - Home: index.md\n  - QuickStart: Getting-Started/quickstart.md\n  - Contributing: \n      - Guidelines: Contribution guidelines/CONTRIBUTING.md\n```\n</code></pre> as seen by reader: <pre><code>nav:\n- Home: index.md\n- QuickStart: Getting-Started/quickstart.md\n- Contributing: - Guidelines: Contribution guidelines/CONTRIBUTING.md\n</code></pre> </p>"},{"location":"Contribution%20guidelines/operations/document-management/#codeblock-with-a-title","title":"Codeblock with a title","text":"<p>For a codeblock that has a title, and will not be tested, use the 'title' parameter in conjunction with the plain codeblock (greater for showing or prescribing contents of files): codeblock: <pre><code>``` title=\"testing.sh\"\n#!/bin/sh\necho hello KubeStellar\n```\n</code></pre> as seen by reader: testing.sh<pre><code>#!/bin/sh\necho hello KubeStellar\n</code></pre> </p> <p>(other variations are possible, PR an update to the kubestellar.css file and, once approved, use the style on the plain codeblock in your documentation.)</p>"},{"location":"Contribution%20guidelines/operations/document-management/#testingrunning-docs","title":"Testing/Running Docs","text":"<p>How do we ensure that our documented examples work?  Simple, we 'execute' our documentation in our CI.  We built automation called 'docs-ecutable' which can be invoked to test any markdown (.md) file in our repository. You could use it in your project as well - afterall it is opensource.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#the-way-it-works","title":"The way it works:","text":"<ul> <li>create your .md file as you normally would</li> <li>add codeblocks that can be tested, tested but hidden, or not tested at all:<ul> <li>use 'shell' to indicate code you want to be tested</li> <li>use '.bash' with the plain codeblock, and the '.hide-md' style for code you want to be tested, but hidden from the reader (some like this, but its not cool if you want others to run your instructions without hiccups)</li> <li>use plain codeblock (```) if you want to show sample output that is not to be tested</li> </ul> </li> <li>you can use 'include-markdown' blocks, and they will also be executed (or not), depending on the codeblock style you use in the included markdown files.</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#the-github-workflow","title":"The GitHub Workflow:","text":"<ul> <li>One example of the GitHub Workflow is located in our kubestellar/kubestellar at https://github.com/kubestellar/kubestellar/blob/main/.github/workflows/docs-ecutable-where-resolver.yml</li> <li>An example workflow using the newer technology is located in our kubestellar/kubestellar repo at https://github.com/kubestellar/kubestellar/blob/main/.github/workflows/docs-ecutable-example1.yml</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#the-original-secret-sauce","title":"The original secret sauce:","text":"<ul> <li>The original code that made all this possible is at https://github.com/kubestellar/kubestellar/blob/main/docs/scripts/docs-ecutable.sh<ul> <li>This code parses the .md file you give it to pull out all the 'shell' and '.bash .hide-me' blocks</li> <li>The code is smart enough to traverse the include-markdown blocks and include the 'shell' and '.bash .hide-me' blocks in them</li> <li>The Jinja constructs are not expanded by this code.</li> <li>It then creates a file called 'generate_script.sh' which is then run at the end of the docs-ecutable execution.</li> </ul> </li> </ul> <p>All of this is invoke in a target in our Makefile <pre><code>.PHONY: docs-ecutable\ndocs-ecutable: MANIFEST=$(MANIFEST) docs/scripts/docs-ecutable.sh\n</code></pre></p> <p>You give the path from that follows the 'https://github.com/kubestellar/kubestellar/docs' path, and name of the .md file you want to 'execute'/'test' as the value for the MANIFEST variable:</p> How to 'make' our docs-ecutable target<pre><code>make MANIFEST=\"'docs/content/Getting-Started/quickstart.md'\" docs-ecutable\n</code></pre> <p>note: there are single and double-quotes used here to avoid issues with 'spaces' used in files names or directories.  Use the single and double-quotes as specified in the quickstart example here.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#the-new-and-improved-secret-sauce","title":"The new and improved secret sauce:","text":"<ul> <li>The newer code for executing bash snippets in documentation is at https://github.com/kubestellar/kubestellar/blob/main/docs/scripts/execute-html.sh<ul> <li>This code parses the HTML generated by MkDocs to extract all the fenced code blocks tagged for the \"shell\" language.</li> <li>This HTML scraping is relatively easy because it does not have to work on general HTML but only the HTML generated by our stack from our sources. The use of the option setting <code>pygments_lang_class: true</code> for the Python-Markdown extension <code>pymdownx.highlight</code> plays a critical role, getting the source language into the generated HTML.</li> <li>Because it reads the generated HTML, invisible code blocks are not extracted.</li> <li>Because it reads the generated HTML, the Jinja constructs have their usual effects.</li> <li>This script is given the name of the HTML file to read and the current working directory to establish at the start of the extracted bash.</li> <li>It then creates a file called 'generated_script.sh' which is then run.</li> </ul> </li> </ul> <p>All of this is invoked in a target in our Makefile <pre><code>.PHONY: execute-html\nexecute-html: venv\n    . $(VENV)/activate; \\\ncd docs; \\\nmkdocs build; \\\nscripts/execute-html.sh \"$$PWD/..\" \"generated/$(MANIFEST)/index.html\"\n</code></pre></p> <p>The <code>make</code> target requires the variable <code>MANIFEST</code> to be set to the directory that contains the generated <code>index.html</code> file, relative to 'https://github.com/kubestellar/kubestellar/docs/generated'. This is the name of the markdown source file, relative to 'https://github.com/kubestellar/kubestellar/docs/content' and with the <code>.md</code> extension dropped.</p> How to 'make' a docs-ecutable target<pre><code>make MANIFEST=\"Coding Milestones/PoC2023q1/example1\" execute-html\n</code></pre> <p>note: this target has no special needs for quoting --- which is not to deny the quoting that your shell needs.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#important-files-in-our-gh-pages-branch","title":"Important files in our gh-pages branch","text":""},{"location":"Contribution%20guidelines/operations/document-management/#indexhtml-and-homehtml","title":"index.html and home.html","text":"<p>In the 'gh-pages' branch there are two(2) important files that redirect the github docs url to our KubeStellar doc site hosted with GoDaddy.com.</p> <p>https://github.com/kubestellar/kubestellar/blob/gh-pages/home.html https://github.com/kubestellar/kubestellar/blob/gh-pages/index.html</p> <p>both files have content similar to: index.html and home.html<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;KubeStellar&lt;/title&gt;\n&lt;meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" &gt;\n&lt;meta http-equiv=\"refresh\" content=\"0; URL=https://docs.kubestellar.io/stable\" /&gt;\n&lt;/head&gt;\n</code></pre></p> <p>Do not remove these files!</p>"},{"location":"Contribution%20guidelines/operations/document-management/#cname","title":"CNAME","text":"<p>The CNAME file has to be in the gh-pages root to allow github to recognize the url tls cert served by our hosting provider.  Do not remove this file!</p> <p>the CNAME file must have the following content in it: CNAME<pre><code>docs.kubestellar.io\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/document-management/#versionsjson","title":"versions.json","text":"<p>The versions.json file contains the version and alias information required by 'mike' to properly serve our doc site.  This file is maintained by the 'mike' environment and should not be edited by hand.</p> <pre><code>[{\"version\": \"release-0.2\", \"title\": \"release-0.2\", \"aliases\": [\"stable\"]}, {\"version\": \"main\", \"title\": \"main\", \"aliases\": [\"latest\", \"unstable\"]}]\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#in-case-of-emergency","title":"In case of emergency","text":"<p>If you find yourself in a jam and the pages are not showing up at kubestellar.io or docs.kubestellar.io, check the following 1) Is the index.html, home.html, CNAME, and versions.json file in the gh-pages branch alongside the folders for the compiled documents?  If not, then recreate those files as indicated above (except for versions.json which is programmatically created by 'mike'). 2) Is GitHub settings for 'Pages' for the domain pointing at the https://docs.kubestellar.io url?  If not, paste it in and check off 'enforce https'.  This can happen if the CNAME file goes missing from the gh-pages branch.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#how-to-recreate-the-gh-pages-branch","title":"How to recreate the gh-pages branch","text":"<p>To recreate the gh-pages branch, do the following: - checkout the gh-pages branch to your local system <pre><code>git clone -b gh-pages https://github.com/kubestellar/kubestellar KubeStellar\ncd KubeStellar\n</code></pre> - delete all files in the branch and push it to GitHub <pre><code>rm -rf *\ngit add; git commit -m \"removing all gh-pages files\"; git push -u origin gh-pages\n</code></pre> -- switch to the 'main' branch <pre><code>git checkout main\ngit pull\n</code></pre> - switch to /docs and run 'mike deploy' for the main branch for alias 'unstable' and 'latest' <pre><code>cd docs\nmike deploy --push --rebase --update-aliases main unstable\nmike deploy --push --rebase --update-aliases main latest\n</code></pre> - switch to the 'release' branch and 'mike deploy' for the release branch for alias 'stable' (your release name will vary) <pre><code>git checkout release-0.2\ngit pull\nmike deploy --push --rebase --update-aliases release-0.2 stable\n</code></pre> - switch back to the gh-pages branch and recreate the home.html, index.html, and CNAME files as needed (make sure you back out of the docs path first before switching to gh-pages because that path does not exist in that branch) <pre><code>cd ..\ngit checkout gh-pages\ngit pull\nvi index.html\nvi home.html\nvi CNAME\n</code></pre> - push the new files into gh-pages <pre><code>git add .;git commit -m \"add index, home, and CNAME files\";git push -u origin gh-pages\n</code></pre> - go into the GitHub UI and go to the settings for the project and click on 'Pages' to add https://docs.kubestellar.io as the domain and check the box to enforce https.</p> <ul> <li>if the above did not work, then you might have an issue with the GoDaddy domain (expired, files missing, etc.)</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#publishing-workflow","title":"Publishing Workflow","text":"<p>All documentation building and publishing is done using GitHub Actions in docs-gen-and-push.yaml. The overall sequence is:</p>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/","title":"Packaging and Versioning","text":"<p>This document surveys the various sorts of piles of things that exist in KubeStellar and how they are versioned.</p> <p> outline of things </p> <p>The picture above outlines the situation. The thing at the tail of an arrow refers to or in some way consumes from the thing at the head of the arrow. Some details omitted for brevity: the full release archives, the Kubernetes API group, and the space framework component.</p>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#kubernetes-api-group","title":"Kubernetes API group","text":"<p>KubeStellar defines a Kubernetes API group, named <code>control.kubestellar.io</code>. API groups are versioned; see the Kubernetes documentation on API group versioning. The Kubernetes API of KubeStellar is defined in <code>api/control</code>.</p> <p>The API objects form the interface between users and the controllers that implement the semantics of those objects.</p>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#syncer-container-image","title":"Syncer container image","text":"<p>The syncer is KubeStellar's agent in WECs, and is based on a Kubernetes Pod that uses a container image at <code>quay.io/kubestellar/syncer</code>. This container image is built from contents of the KubeStellar GitHub repo focused by ko on the syncer binary. This image is normally built using the <code>make</code> target named \"build-kubestellar-syncer-image. The syncer image is referenced in the output generated by the <code>kubectl kubestellar syncer-gen</code> command, which is invoked by <code>kubectl kubestellar prep-for-syncer</code>, which is invoked by <code>kubectl kubestellar prep-for-cluster</code>. The <code>kubectl kubestellar prep-for-syncer</code> script holds the default value for the tag to use in the reference to the syncer image.</p> <p>Following are some recent tags and what they correspond to.</p> <ul> <li> <p><code>v0.9.0</code>, for the git commit with that tag (i.e., that release).</p> </li> <li> <p><code>v0.8.0</code>, for the git commit with that tag (i.e., that release).</p> </li> <li> <p><code>pr-1103</code> = <code>git-ae64d5722-clean</code>, incorporating PR   1103, the   latest development in the release-0.7 line.</p> </li> </ul>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#core-container-image","title":"Core container image","text":"<p>The core container image is used in containers that run the KubeStellar core controllers , initialize the KubeStellar core, and run one of the supported space providers. It is built from some contents of the KubeStellar GitHub repo, specifically excluding some that are only needed by users outside this container. This image is normally built using the <code>make</code> target \"kubestellar-image\". This image is referenced from the core Helm chart, which can be used directly from the Helm repo and also is used by the <code>kubectl kubestellar deploy</code> command (which has a fixed image tag in its source, which is edited when it is time to advance that tag).</p> <p>Following are some recent tags and what they correspond to.</p> <ul> <li> <p><code>release-0.9</code>, for the release 0.9 line of development; currently   this points to the same image as <code>v0.9.0</code>   but may move in the future.</p> </li> <li> <p><code>v0.9.0</code> = <code>v0.9.0-alpha.1.git1f0e2cc-clean</code>, for the v0.9.0 release.</p> </li> </ul>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#kubestellar-core-helm-chart","title":"KubeStellar core Helm chart","text":"<p>This Helm chart describes a deployment of the KubeStellar core components (a space provider, KubeStellar controllers, initialization) as workload for a Kubernetes cluster.</p> <p>The source of this Helm chart is in the <code>core-helm-chart</code> subdirectory of the kubestellar GitHub repo.</p> <p>The <code>kubectl kubestellar deploy</code> command uses the adjacent copy of this Helm chart. That is: (1) in the case of working with a local copy of the kubestellar/kubestellar GitHub repo, the copy in the <code>core-helm-chart</code> directory, and (2) in the case of using an unpacked release archive --- which contains <code>core-helm-chart</code> as well as <code>bin</code> --- that unpacked copy of <code>core-helm-chart</code>.</p> <p>Alternatively, users can use the Helm repository at <code>https://helm.kubestellar.io/</code>, which is served via GitHub pages from the <code>main</code> branch of the kubestellar/helm GitHub repo. This repository holds tar archives of released versions of the chart, created manually from the sources in the <code>core-helm-chart</code> subdirectory of the kubestellar GitHub repo.</p> <p>A Helm chart has both a version number for the chart itself and a version number for the \"application\" it installs.</p> <p>Currently there is only one version of the chart that is interesting.</p> Chart Version App Version What it is 1/2 (some confusion here) v0.9.0 KubeStellar release-0.9 branch"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#github-repos","title":"GitHub repos","text":""},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#kubestellarkubestellar","title":"kubestellar/kubestellar","text":"<p>The kubestellar repo is the main repo for KubeStellar. It holds the code and documentation and the source of the Helm chart. The following branches and tags are most interesting.</p> <ul> <li>branch <code>main</code> is the main line of development.</li> <li>branch <code>release-0.9</code> is for development along the line of the 0.9 release.</li> <li>tag <code>v0.9.0</code> is release 0.9.0.</li> </ul>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#kubestellarhelm","title":"kubestellar/helm","text":"<p>The helm GitHub repo hosts distributions of archived versions of the chart, as discussed above.</p>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#kubestellarhomebrew-kubestellar","title":"kubestellar/homebrew-kubestellar","text":"<p>The homebrew-kubestellar GitHub repo serves directly as the homebrew repository for KubeStellar. According to homebrew convention, users refer to it in homebrew as <code>kubestellar/kubestellar</code> (e.g., <code>brew tap kubestellar/kubestellar</code> connects directly to this GitHub repository).</p> <p>This HomeBrew repository has two formulae.  One to control the space provider and another formula called <code>kubestellar_cli</code> containing tools needed by a user who deploys the KubeStellar core as workload in a Kubernetes cluster or another user of that deployment. This formula fetches and installs a <code>kubestellaruser</code> release archive from GitHub. Users invoke the <code>brew</code> command directly to install, upgrade, and remove these formulae.</p> <p>Following are the versions available.</p> <ul> <li><code>kubestellar_cli</code> <code>v0.9.0</code>, for that release of KubeStellar.</li> </ul>"},{"location":"Contribution%20guidelines/operations/packaging-and-versioning/#kubestellarkubeflex","title":"kubestellar/kubeflex","text":"<p>The kubeflex repo holds kubeflex, an independent component that we anticipate using as a provider for the Space framework in the future.</p>"},{"location":"Contribution%20guidelines/operations/release-management/","title":"Release management","text":""},{"location":"Contribution%20guidelines/operations/release-management/#publishing-a-new-kubestellar-release","title":"Publishing a new KubeStellar release","text":""},{"location":"Contribution%20guidelines/operations/release-management/#prerequisite-make-sure-you-have-a-gpg-signing-key","title":"Prerequisite - make sure you have a GPG signing key","text":"<ol> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/generating-a-new-gpg-key</li> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/adding-a-gpg-key-to-your-github-account</li> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/telling-git-about-your-signing-key</li> </ol>"},{"location":"Contribution%20guidelines/operations/release-management/#create-the-tags","title":"Create the tags","text":""},{"location":"Contribution%20guidelines/operations/release-management/#note","title":"Note:","text":"<p>You currently need write access to the https://github.com/kubestellar/kubestellar repository to perform these tasks.</p>"},{"location":"Contribution%20guidelines/operations/release-management/#checkout-the-main-branch","title":"Checkout the main branch","text":"<pre><code>git clone git@github.com:kubestellar/kubestellar.git\ncd kubestellar\ngit checkout main\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-kubectl-kubestellar-prep_for_syncer-file-with-a-reference-to-the-new-version-of-the-kubestellar-syncer-version-if-you-made-a-new-version-and-see-the-syncer-doc-for-how-to-do-that-being-careful-to-not-exclude-the-git-commit-and-cleanliness-information-from-all-the-tags-on-the-image","title":"Update the 'kubectl-kubestellar-prep_for_syncer' file with a reference to the new version of the kubestellar syncer version IF you made a new version (and see the syncer doc for how to do that, being careful to not exclude the git commit and cleanliness information from all the tags on the image).","text":"<pre><code>vi scripts/outer/kubectl-kubestellar-prep_for_syncer\n</code></pre> <p>change the version in the following line: <pre><code>syncer_image=\"quay.io/kubestellar/syncer:v0.16.0\"\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-core-helm-chart-chartyaml-and-valuesyaml-files-with-a-reference-to-the-new-version-of-the-kubestellar-helm-chart-version","title":"Update the core-helm-chart 'Chart.yaml' and 'values.yaml' files with a reference to the new version of the KubeStellar Helm chart version","text":"<pre><code>vi core-helm-chart/Chart.yaml\n</code></pre> <p>change the versions in the 'Chart.yaml' file in the following lines: <pre><code>...\nversion: 9\n...\nappVersion: v0.16.0\n...\n</code></pre></p> <p>then in 'values.yaml' <pre><code>vi core-helm-chart/values.yaml\n</code></pre></p> <p>change the version in the 'values.yaml' file in the following line: <pre><code># KubeStellar image parameters\nimage:\n  repository: quay.io/kubestellar/kubestellar\n  pullPolicy: IfNotPresent\n  tag: release-0.16\n...\n# Space abstraction layer image parameters\nspaceimage:\n  repository: quay.io/kubestellar/space-framework\n  pullPolicy: IfNotPresent\n  tag: release-0.16\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-version-file","title":"Update the VERSION file","text":"<p>The VERSION file points to the 'latest' and 'stable' release tags associated with the most recent release (latest) and the most stable release (stable).  Update the 'stable' and 'latest tags accordingly</p> <pre><code>vi VERSION\n</code></pre> <p>before: VERSION<pre><code>...\nstable=v0.9.0\nlatest=v0.15.0\n...\n</code></pre></p> <p>after: VERSION<pre><code>...\nstable=v0.9.0\nlatest=v0.16.0\n...\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-mkdocsyml-file-pre-branch","title":"Update the mkdocs.yml file (pre branch)","text":"<p>The mkdocs.yml file points to the branch and tag associated with the branch you have checked out.  Update the ks_branch and ks_tag key/value pairs at the top of the file</p> <pre><code>vi docs/mkdocs.yml\n</code></pre> <p>before: mkdocs.yml<pre><code>...\nks_current_branch: 'release-0.15'\nks_current_tag: 'v0.15.0'\nks_current_helm_version: 8\nks_next_branch: 'release-0.16'\nks_next_tag: 'v0.16.0'\nks_next_helm_version: 9\n...\n</code></pre></p> <p>after: mkdocs.yml<pre><code>...\nks_current_branch: 'release-0.16'\nks_current_tag: 'v0.16.0'\nks_current_helm_version: 9\nks_next_branch:    # put the branch name of the next numerical branch that will come in the future\nks_next_tag:       # put the tag name of the next numerical tag that will come in the future\nks_next_helm_version: # put the number of the next logical helm version\n...\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#push-the-main-branch","title":"Push the main branch","text":"<pre><code>git add .\ngit commit -m \"updates to main to support new release v0.16.0\"\ngit push -u origin main\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-release-majorminor-branch","title":"Create a release-major.minor branch","text":"<p>To create a release branch, identify the current 'release' branches' name (e.g. release-0.16).  Increment the  or  segment as part of the 'release' branches' name.  For instance, the 'release' branch is 'main', you might name the new release branch 'release-0.16'. <pre><code>git checkout -b release-0.16\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-mkdocsyml-file-post-branch","title":"Update the mkdocs.yml file (post branch)","text":"<p>The mkdocs.yml file points to the branch and tag associated with the branch you have checked out.  Update the ks_branch and ks_tag key/value pairs at the top of the file</p> <pre><code>vi docs/mkdocs.yml\n</code></pre> <p>before: mkdocs.yml<pre><code>...\nedit_uri: edit/main/docs/content\nks_branch: 'main'\nks_tag: 'latest'\n...\n</code></pre></p> <p>after: mkdocs.yml<pre><code>...\nedit_uri: edit/release-0.16/docs/content\nks_branch: 'release-0.16'\nks_tag: 'v0.16.0'\n...\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-branch-name-in-readmemd","title":"Update the branch name in /README.MD","text":"<p>There are quite a few references to the main branch /README.MD.  They connect the GitHub Actions for the specific branch to the README.MD page.  Since we are on the new release branch, its time to update these to point to the release itself.</p> <pre><code>vi README.MD\n</code></pre> <p>before: <pre><code>https://github.com/kubestellar/kubestellar/actions/workflows/docs-gen-and-push.yml/badge.svg?branch=main\n</code></pre></p> <p>after: <pre><code>https://github.com/kubestellar/kubestellar/actions/workflows/docs-gen-and-push.yml/badge.svg?branch=release-0.16\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#push-the-new-release-branch","title":"Push the new release branch","text":"<pre><code>git add .\ngit commit -m \"new release version release-0.16\"\ngit push -u origin release-0.16 # replace &lt;major&gt;.&lt;minor&gt; with your incremented &lt;major&gt;.&lt;minor&gt; pair\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#remove-the-current-stable-alias-using-mike-danger","title":"Remove the current 'stable' alias using 'mike' (DANGER!)","text":"<p>Be careful, this will cause links to the 'stable' docs, which is the default for our community, to become unavailable.  For now, point 'stable' at 'main' <pre><code>cd docs\nmike delete stable # remove the 'stable' alias from the current 'main' branches' doc set\nmike deploy --push --rebase --update-aliases main stable # this generates the 'main' branches' docs set and points 'stable' at it temporarily\ncd ..\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-stable-alias-using-mike","title":"Update the 'stable' alias using 'mike'","text":"<pre><code>cd docs\nmike delete stable # remove the 'stable' alias from the 'main' branches' doc set\ngit pull\nmike deploy --push --rebase --update-aliases release-0.16 stable  # this generates the new 'release-0.16' branches' doc set and points 'stable' at it\ncd ..\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#test-your-doc-site","title":"Test your doc site","text":"<p>Open a Chrome Incognito browser to https://docs.kubestellar.io and look for the version drop down to be updated to the new release you just pushed with 'git' and deployed with 'mike'</p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-tagged-release","title":"Create a tagged release","text":"<p>View the existing tags you have for the repo</p> <pre><code>git fetch --tags\ngit tag\n</code></pre> <p>create a tag that follows ...  For this example we will increment tag 'v0.15.0' to 'v0.16.0' <pre><code>TAG=v0.16.0\nREF=release-0.16\ngit tag --sign --message \"$TAG\" \"$TAG\" \"$REF\"\ngit push origin --tags\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#clean-out-previous-release-tar-images-and-the-checksums256txt-file-from-your-local-build-environment","title":"Clean out previous release tar images and the checksums256.txt file from your local build environment","text":"<p>When you create a build, output goes to your local /build/release.  Make sure this path is empty before you start so there is no mixup with your current build.</p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-kubestellar-full-build","title":"Create a KubeStellar full build","text":"<pre><code>./hack/make-release-full.sh v0.16.0\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-release-in-gh-ui","title":"Create a release in GH UI","text":"<ul> <li>Navigate to the KubeStellar GitHub Source Repository Releases section at https://github.com/kubestellar/kubestellar/releases</li> <li> <p>Click 'Draft a new release' and select the tag ('v0.16.0')</p> <ul> <li>Select the release branch you created above (release-0.16)</li> <li>Add a release title (v0.16.0)</li> <li>Add some release notes ('generate release notes' if you like)</li> <li>select 'pre-release' as a the first step.  Once validated the release is working properly, come back and mark as 'release'</li> <li>Attach the binaries that were created in the 'make-release-full' process above<ul> <li>You add the KubeStellar-specific '*.tar.gz' and the 'checksums256.txt' files</li> <li>GitHub will automatically add the 'Source Code (zip)' and 'Source Code (tar.gz)'</li> </ul> </li> </ul> <p> Release Example </p> </li> </ul>"},{"location":"Contribution%20guidelines/operations/release-management/#create-the-kubestellar-core-container-image","title":"Create the KubeStellar Core container image","text":"<p>First, login to quay.io with a user that has credentials to 'write' to the kubestellar quay repo <pre><code>docker login quay.io\n</code></pre></p> <p>then, remove any running container from moby/buildkit <pre><code>CONTAINER ID   IMAGE                           COMMAND              \nc943925fd137   moby/buildkit:buildx-stable-1   \"buildkitd\" \n\ndocker rm c943925fd137 -f\n</code></pre></p> <p>and remove the 'buildx' container image from your local docker images <pre><code>REPOSITORY      TAG               IMAGE ID       CREATED        SIZE\nmoby/buildkit   buildx-stable-1   16fc6c95ddff   10 days ago    168MB\n\ndocker rmi 16fc6c95ddff\n</code></pre></p> <p>finally, make the KubeStellar image from within the local copy of the release branch 'release-0.16' <pre><code>make kubestellar-image\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-space-core-build","title":"Create a Space Core build","text":"<pre><code>pushd space-framework\n./make spacecore-image v0.16.0\npopd\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-kubestellar-and-space-core-container-images-just-build-and-uploaded-to-quayio","title":"Update the KubeStellar and Space Core container images just build and uploaded to quay.io","text":"<p>Head up to quay.io and look for the image of KubeStellar Core container just uploaded. Tag the image with: 'latest', 'release-0.16', and 'v0.16.0' so that helm and other install methods pickup this image.</p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-kubestellar-core-helm-repository","title":"Update KubeStellar Core Helm repository","text":"<p>First, make sure you have a version of 'tar' that supports the '--transform' command line option <pre><code>brew install gnu-tar\n</code></pre></p> <p>then, from root of local copy of https://github.com/kubestellar/kubestellar repo: <pre><code>gtar -zcf kubestellar-core-9.tar.gz core-helm-chart/ --transform s/core-helm-chart/kubestellar-core/\nmv kubestellar-core-9.tar.gz ~\nshasum -a 256 ~/kubestellar-core-9.tar.gz\n</code></pre> Clone the homebrew-kubestellar repo <pre><code>git clone git@github.com:kubestellar/helm.git\ncd helm\ngit checkout main\n</code></pre></p> <p>then, from root of local copy of https://github.com/kubestellar/helm repo <pre><code>mv ~/kubestellar-core-9.tar.gz charts\n</code></pre></p> <p>next, update 'index.yaml' in root of local copy of helm repo (only update the data, not time, on lines 6 and 15): index.yaml<pre><code>apiVersion: v1\nentries:\n  kubestellar-core:\n  - apiVersion: v2\n    appVersion: v0.16.0\n    created: \"2023-10-30T12:00:00.727185806-04:00\"\ndescription: A Helm chart for KubeStellar Core deployment as a service\n    digest: 6f42d9e850308f8852842cd23d1b03ae5be068440c60b488597e4122809dec1e\n    icon: https://raw.githubusercontent.com/kubestellar/kubestellar/main/docs/favicons/favicon.ico\n    name: kubestellar\n    type: application\n    urls:\n    - https://helm.kubestellar.io/charts/kubestellar-core-{{ no such element: mkdocs.config.defaults.MkDocsConfig object['ks_new_helm_version'] }}.tar.gz\n    version: \"9\"\ngenerated: \"2023-10-30T12:00:00.727185806-04:00\"\n</code></pre></p> <p>finally, finally, push to the main branch <pre><code>git add .\ngit commit -m \"updates to main to support release v0.16.0 of KubeStellar Helm component\"\ngit push -u origin main\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#update-kubestellar-cli-brew-repository","title":"Update KubeStellar CLI Brew repository","text":"<p>Clone the homebrew-kubestellar repo <pre><code>git clone git@github.com:kubestellar/homebrew-kubestellar.git\ncd homebrew-kubestellar\ngit checkout main\n</code></pre></p> <p>edit the kubestellar_cli.rb file <pre><code>vi Formula/kubestellar_cli.rb\n</code></pre></p> <p>update all instances of 'url' from v0.15.0 to v0.16.0 (should be 6 of these) <pre><code>...\n    when :arm64\n      url \"https://github.com/kubestellar/kubestellar/releases/download/v0.16.0/kubestellaruser_v0.16.0_darwin_arm64.tar.gz\"\nsha256 \"5be4c0b676e8a4f5985d09f2cfe6c473bd2f56ebd3ef4803ca345e6f04d83d6b\" ...\n</code></pre></p> <p>then, update all instances of 'sha256' with the corresponding sha256 hash values in the build/release/checksums256.txt you create during the make-full-release.sh section above. (should be 6 of these)</p> <pre><code>...\n    when :arm64\n      url \"https://github.com/kubestellar/kubestellar/releases/download/v0.16.0/kubestellaruser_v0.16.0_darwin_arm64.tar.gz\"\nsha256 \"&lt;corresponding sha256 hash from checksums256.txt&gt;\" ...\n</code></pre> <p>finally, push to the main branch <pre><code>git add .\ngit commit -m \"updates to main to support release v0.16.0 of KubeStellar Brew component\"\ngit push -u origin main\n</code></pre></p> <p>and, to test <pre><code>brew update\nbrew install kubestellar-cli\n</code></pre></p> <p>you should see output that indicates an update for the kubestellar brew tap and then an update to version v0.16.0 of the kubestellar_cli brew formula.</p>"},{"location":"Contribution%20guidelines/operations/release-management/#check-that-gh-workflows-for-docs-are-working","title":"Check that GH Workflows for docs are working","text":"<p>Check to make sure the GitHub workflows for doc generation, doc push, and broken links is working and passing https://github.com/kubestellar/kubestellar/actions/workflows/docs-gen-and-push.yml https://github.com/kubestellar/kubestellar/actions/workflows/broken-links-crawler.yml</p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-an-email-addressed-to-kubestellar-devgooglegroupscom-and-kubestellar-usersgooglegroupscom","title":"Create an email addressed to kubestellar-dev@googlegroups.com and kubestellar-users@googlegroups.com","text":"<pre><code>Subject: KubeStellar release v0.16.0\n\nBody:\n\nDear KubeStellar Community,\n    Release v0.16.0 is now available at https://github.com/kubestellar/kubestellar/releases/tag/v0.16.0\n\nWhat's Changed\n\n\ud83d\udc1b Fix display of initial spaces after deploy in kube by @MikeSpreitzer in #1143\n\u2728 Generalize bootstrap wrt namespace in hosting cluster by @MikeSpreitzer in #1144\n\u2728 Generalize bootstrap wrt namespace in hosting cluster by @MikeSpreitzer in #1145\n\u2728 Switch to use k8s code generators by @ezrasilvera in #1139\n\u2728 Bump actions/checkout from 4.1.0 to 4.1.1 by @dependabot in #1151\n\ud83c\udf31 Align default core image ref in chart with coming release by @MikeSpreitzer in #1146\n\ud83d\udcd6Update dev-env.md by @francostellari in #1157\n\ud83d\udcd6Update Chart.yaml appVersion by @francostellari in #1158\n\ud83d\udc1b Use realpath to see through symlinks by @MikeSpreitzer in #1156\n\u2728 Increase kind version to v0.20 for ubuntu by @fab7 in #1155\n\ud83d\udcd6 Document syncer removal by @MikeSpreitzer in #1164\n\ud83c\udf31 Rename urmeta to ksmeta by @MikeSpreitzer in #1166\n\u2728 Make get-internal-kubeconfig fetch mid-level kubeconfig by @MikeSpreitzer in #1161\n\u2728 Make ensure/remove wmw insensitive to current workspace by @MikeSpreitzer in #1160\nNew Contributors\n\n@fab7 made their first contribution in #1155\nFull Changelog: v0.8.0\u2026v0.9.0\n\nThank you for your continued support,\n\nAndy\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#post-the-same-message-in-the-kubestellar-slack-channel","title":"Post the same message in the #kubestellar Slack channel","text":""},{"location":"Contribution%20guidelines/operations/testing-doc-prs/","title":"Testing doc prs","text":""},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#testing-a-kubestellar-documentation-pr","title":"Testing a KubeStellar documentation PR","text":"<p>Here are the steps to checkout a git pull request for local testing.</p> <p>STEP-1: Checkout the Pull Request</p> <p>Helpers: GitHub, DevOpsCube</p>"},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#11-use-the-pull-request-number-to-fetch-origin-note-be-sure-to-check-out-the-right-branch","title":"1.1 Use the pull request number to fetch origin (note: be sure to check out the right branch!)","text":"<p>\u00a0\u00a0\u00a0\u00a0Fetch the reference to the pull request based on its ID number, creating a new branch locally. Replace ID with your PR # and BRANCH_NAME with the desired branch name.</p> <p> <code>git fetch origin pull/ID/head:BRANCH_NAME</code> </p>"},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#12-switch-to-the-new-branch","title":"1.2 Switch to the new branch","text":"<p>\u00a0\u00a0\u00a0Checkout the BRANCH_NAME where you have all the changes from the pull request.</p> <p> <code>git switch BRANCH_NAME</code> </p> <p>\u00a0\u00a0\u00a0\u00a0At this point, you can do anything you want with this branch. You can run some local tests, or merge other branches into the branch.</p> <p>STEP-2: Test and Build the Documentation (optional)</p> <p>\u00a0\u00a0\u00a0\u00a0Use this procedure if you want to view and modify the documentation in the branch you have checked out.</p> <p>Helpers: KubeStellar/docs, MkDocs</p>"},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#21-install-mkdocs-and-its-requirements","title":"2.1 Install MkDocs and its requirements","text":"<pre><code>  cd docs\n  pip install mkdocs\n  pip install -r requirements.txt  \n</code></pre>"},{"location":"Contribution%20guidelines/operations/testing-doc-prs/#22-build-and-view-the-documentation","title":"2.2 Build and view the documentation","text":"<p> <code>mkdocs serve</code></p> <p>\u00a0\u00a0\u00a0\u00a0Next, open a browser to http://127.0.0.1:8000 and review the changes.</p>"},{"location":"Contribution%20guidelines/security/security/","title":"Policy","text":""},{"location":"Contribution%20guidelines/security/security/#security-announcements","title":"Security Announcements","text":"<p>Join the kubestellar-security-announce group for emails about security and major API announcements.</p>"},{"location":"Contribution%20guidelines/security/security/#report-a-vulnerability","title":"Report a Vulnerability","text":"<p>We're extremely grateful for security researchers and users that report vulnerabilities to the KubeStellar Open Source Community. All reports are thoroughly investigated by a set of community volunteers.</p> <p>You can also email the private kubestellar-security-announce@googlegroups.com list with the security details and the details expected for all KubeStellar bug reports.</p>"},{"location":"Contribution%20guidelines/security/security/#when-should-i-report-a-vulnerability","title":"When Should I Report a Vulnerability?","text":"<ul> <li>You think you discovered a potential security vulnerability in KubeStellar</li> <li>You are unsure how a vulnerability affects KubeStellar</li> <li>You think you discovered a vulnerability in another project that KubeStellar depends on</li> <li>For projects with their own vulnerability reporting and disclosure process, please report it directly there</li> </ul>"},{"location":"Contribution%20guidelines/security/security/#when-should-i-not-report-a-vulnerability","title":"When Should I NOT Report a Vulnerability?","text":"<ul> <li>You need help tuning KubeStellar components for security</li> <li>You need help applying security related updates</li> <li>Your issue is not security related</li> </ul>"},{"location":"Contribution%20guidelines/security/security/#security-vulnerability-response","title":"Security Vulnerability Response","text":"<p>Each report is acknowledged and analyzed by the maintainers of KubeStellar within 3 working days.</p> <p>Any vulnerability information shared with Security Response Committee stays within KubeStellar project and will not be disseminated to other projects unless it is necessary to get the issue fixed.</p> <p>As the security issue moves from triage, to identified fix, to release planning we will keep the reporter updated.</p>"},{"location":"Contribution%20guidelines/security/security/#public-disclosure-timing","title":"Public Disclosure Timing","text":"<p>A public disclosure date is negotiated by the KubeStellar Security Response Committee and the bug submitter. We prefer to fully disclose the bug as soon as possible once a user mitigation is available. It is reasonable to delay disclosure when the bug or the fix is not yet fully understood, the solution is not well-tested, or for vendor coordination. The timeframe for disclosure is from immediate (especially if it's already publicly known) to a few weeks. For a vulnerability with a straightforward mitigation, we expect report date to disclosure date to be on the order of 7 days. The KubeStellar maintainers hold the final say when setting a disclosure date.</p>"},{"location":"Contribution%20guidelines/security/security_contacts/","title":"Contacts","text":"<p>Defined below are the security contacts for this repo.</p> <p>They are the contact point for the Product Security Committee to reach out to for triaging and handling of incoming issues.</p> <p>The below names agree to address security concerns if and when they arise.</p> <p>DO NOT REPORT SECURITY VULNERABILITIES DIRECTLY TO THESE NAMES, SEND INFORMATION TO kubestellar-security-announce@googlegroups.com</p> <p>clubanderson MikeSpreitzer ezrasilvera pdettori</p>"},{"location":"common-subs/coming-soon/","title":"Coming soon","text":""},{"location":"direct/","title":"KubeStellar","text":"<p>KubeStellar supports multi-cluster deployment of Kubernetes objects, controlled by a  simple binding policy and deploying Kubernetes objects in their native format.  It uses OCM as  transport, with standard OCM agents (Klusterlet). We show examples of deploying workloads to  multi-cluster with kubectl, helm and ArgoCD using a simple label-selectors-based binding policy.</p>"},{"location":"direct/#supported-features","title":"Supported Features:","text":"<ol> <li>Multi-cluster Deployment: Kubernetes objects are deployed across multiple clusters, controlled by a  straightforward binding policy.</li> <li>Pure-Kube User Experience: Deployment of non-wrapped objects is handled in a pure Kubernetes manner.</li> <li>Object Management via WDS: Creation, update, and deletion of objects in managed clusters are performed from WDS.</li> <li>OCM as Transport: The Open Cluster Management (OCM) is used as transport, with standard OCM agents (Klusterlet).</li> <li>Multi-WDS and single OCM Shard: Multiple WDSs and a single OCM shard are supported.</li> <li>Resiliency: All components are running in Kubernetes, ensuring continued operation even after restarts of any component.</li> <li>Re-evaluation of Objects: Existing objects are re-evaluated when a new binding policy is added or updated.</li> <li>Object Removal: Objects are removed from clusters when the binding policy that led to their deployment on  those clusters is deleted or updated and the what or where no longer match.</li> <li>Dynamic Handling of APIs: Dynamically start/stop informers when adding/removing CRDs.</li> <li>Simplified setup: Just 3 commands to get a fully functional setup (<code>kflex init</code>, <code>kflex create imbs</code>, <code>kflex create wds</code>)</li> <li>OpenShift Support: Same commands to set it up. All components have been tested in OpenShift,  including OCM Klusterlet for the WECs.</li> <li>Singleton Status Addressed by the status controller in KubeStellar and the Status Add-On for OCM.</li> </ol>"},{"location":"direct/#to-be-supported","title":"To be supported","text":"<ol> <li>Status summarization</li> <li>Customization</li> <li>OCM sharding</li> <li>Upsync</li> </ol>"},{"location":"direct/#latest-stable-release","title":"Latest stable release","text":"<p>We do not have one that is proven very good yet. The first release using the new architecture is 0.20.0; it is feture-incomplete. Release 0.21.0 works but is not well documented on the website; view the missing information directly at GitHub. The latest release is 0.21.2. See also the release notes.</p>"},{"location":"direct/#architecture","title":"Architecture","text":"<p>See Architecture.</p>"},{"location":"direct/#packaging-and-delivery","title":"Packaging and Delivery","text":"<p>See Packaging and Delivery</p>"},{"location":"direct/#usage-examples-and-testing","title":"Usage examples, and testing","text":"<p>Examples shows a few examples of how to deploy and use a release of KubeStellar.</p> <p>The <code>test/e2e</code> directory holds end-to-end tests.</p> <p>The contributor guide has a section on integration testing.</p> <p>The contributor guide has a section on unit testing.</p>"},{"location":"direct/#using-an-existing-hosting-cluster","title":"Using an existing hosting cluster","text":"<p>See the instructions for using an existing hosting cluster for notes on that and coping with multi-machine scenarios.</p>"},{"location":"direct/#contributor-guide","title":"Contributor Guide","text":"<p>See the contributor guide, which is also just beginning to be written.</p>"},{"location":"direct/architecture/","title":"KubeStellar Architecture","text":"<p>KubeStellar provides multi-cluster deployment of Kubernetes objects, controlled by simple <code>BindingPolicy</code> objects, where Kubernetes objects are expressed in their native format with no wrapping or bundling. The high-level architecture for KubeStellar is illustrated in Figure 1.</p> <p> Figure 1 - High Level Architecture </p> <p>KubeStellar relies on the concept of spaces. A Space is an abstraction to represent an API service that  behaves like a Kubernetes kube-apiserver (including the persistent storage behind it)  and the subset of controllers in the kube-controller-manager that are concerned with  API machinery generalities (not management of containerized workloads).  A KubeFlex <code>ControlPlane</code> is an example. A regular Kubernetes cluster is another example. Users can use spaces to perform these tasks:</p> <ol> <li>Create Workload Definition Spaces (WDSs) to store the definitions of their workloads. A Kubernetes workload is an application that runs on Kubernetes. A workload can be made by a  single Kubernetes object or several objects that work together.</li> <li>Create Inventory and Transport Spaces (ITSs) to manage the inventory of clusters and  the transport of workloads.</li> <li>Register and label Workload Execution Clusters (WECs) with the Inventory and  Transport Space, to keep track of the available clusters and their characteristics.</li> <li>Define <code>BindingPolicy</code> to specify what objects and where should be  deployed on the WECs.</li> <li>Submit objects in the native Kubernetes format to the WDSs,  and let the <code>BindingPolicy</code> govern which WECs should receive them.</li> <li>Check the status of submitted objects from the WDS.</li> </ol> <p>In KubeStellar, users can assume a variety of roles and responsibilities.  These roles could range from system administrators and application owners  to CISOs and DevOps Engineers. However, for the purpose of this document,  we will not differentiate between these roles. Instead we will use the term  'user' broadly, without attempting to make distinctions among roles.</p> <p>Examples of users interaction with KubeStellar are illustrated in the KubeStellar Usage Examples section.</p> <p>The KubeStellar architecture has these main modules:</p> <ul> <li> <p>KubeStellar Controller Manager: this module is responsible for watching <code>BindingPolicy</code> objects and create from it a matching <code>Binding</code> object that contains list of references to the concrete objects and list of references to the concrete clusters, and for updating the status of objects in the WDS.</p> </li> <li> <p>Pluggable Transport Controller: this module is responsible for delivering workload objects from the WDS to the ITS according to <code>Binding</code> objects.</p> </li> <li> <p>Space Manager: This module manages the lifecycle of spaces.</p> </li> <li> <p>OCM Cluster Manager: This module syncs objects from the ITS to the Workload Execution  Clusters (WECs). In the ITS, each mailbox namespace is associated with one WEC. Objects  that are put in a mailbox namespace are delivered to the matching WEC.</p> </li> <li> <p>Status Add-On Controller: This module installs the OCM status add-on agent  on all WECs and sets the RBAC permissions for it using the OCM add-on framework.</p> </li> <li> <p>OCM Agent: This module registers the WEC to the OCM Hub, watches for  ManifestWorks and unwraps and syncs the objects into the WEC.</p> </li> <li> <p>OCM Status Add-On Agent: This module watches AppliedManifestWorks  to find objects that are synced by the OCM agent, gets their status  and updates WorkStatus objects in the ITS namespace associated with the WEC.</p> </li> </ul> <p> Figure 2 - Main Modules </p>"},{"location":"direct/architecture/#kubestellar-controller-manager","title":"KubeStellar Controller Manager","text":"<p>This module manages binding controller, and status controller.  * The binding controller watches <code>BindingPolicy</code> and workload objects on the Workload Definition Space (WDS), and maintains a <code>Binding</code> object for each <code>BindingPolicy</code> in the WDS. The <code>Binding</code> object contains references to the concrete list of workload objects and references to the concrete list of clusters that were selected by the <code>BindingPolicy</code> selectors.</p> <ul> <li>The status controller watches for WorkStatus objects on the ITS and updates the status of objects in the WDS when singleton status is requested in the <code>BindingPolicy</code> for those objects. </li> </ul> <p>There is one instance of a KubeStellar Controller Manager for each WDS.  Currently this controller-manager runs in the KubeFlex hosting cluster and is responsible for installing the required  CRDs in the associated WDS. More details on the internals of this module are provided in KubeStellar Controllers Architecture.</p>"},{"location":"direct/architecture/#pluggable-transport-controller","title":"Pluggable Transport Controller","text":"<ul> <li>The pluggable transport controller watches <code>Binding</code> objects on the WDS, and maintains in the Inventory and Transport Space (ITS)  a wrapped object per <code>Binding</code> to be delivered.</li> <li>This controller is pluggable and can potentially be implemented using different options. Currently the only option we support is based on the Open Cluster Management Project </li> </ul> <p>There is one instance of the pluggable transport controller for each WDS.  Currently this controller runs in an executable process. This is a work in progress and we're working on running this controller in a dedicated pod. More details on the internals of this module are provided in KubeStellar Controllers Architecture.</p>"},{"location":"direct/architecture/#space-manager","title":"Space Manager","text":"<p>The Space Manager handles the lifecycle of spaces.  KubeStellar uses the KubeFlex project for space management. In KubeFlex, a space is named a <code>ControlPlane</code>, and we will use  both terms in this document. KubeSteller currently prereqs KubeFlex to  provide one or more spaces. We plan to make this optional in the near future.</p> <p>KubeFlex is a flexible framework that supports various kinds of control planes, such as k8s, a basic Kubernetes API Server with a subset of kube controllers, and  vcluster: a virtual cluster that runs on the hosting cluster based on the vCluster Project. More detailed information on the different types of control planes and architecture are described in the KubeFlex Architecture.</p> <p>There are currently two roles for spaces managed by KubeFlex: Inventory and Transport Space  (ITS) and Workload Description Space (WDS). The former runs the OCM Cluster Manager on a vcluster-type control plane, and the latter runs on a k8s-type control plane.</p> <p>An ITS holds the OCM inventory (<code>ManagedCluster</code>) objects and mailbox namespaces. The mailbox namespaces and their contents are implementation details that users do not deal with. Each mailbox namespace corresponds 1:1 with a WEC and holds <code>ManifestWork</code> objects managed by the central KubeStellar controllers.</p> <p>A WDS holds user workload objects and the user's objects that form the interface to KubeStellar control.  Currently, the user control objects are <code>BindingPolicy</code> and <code>Binding</code> objects. We plan to later add objects to specify customization and summarization.</p> <p>KubeFlex provides the ability to start controllers connected to a Control Plane API Server or to deploy Helm Charts into a Control Plane API server with post-create hooks. This feature is currently adopted for KubeStellar modules startup, as it allows to create a Workload Description Space (WDS) and start the KubeStellar Controller Manager, and create an Inventory and Transport Space (ITS) in a <code>vcluster</code> and install the Open Cluster Management Hub there.</p>"},{"location":"direct/architecture/#ocm-cluster-manager","title":"OCM Cluster Manager","text":"<p>This module is based on the Open Cluster Management Project, a community-driven project that focuses on multicluster and multicloud scenarios for Kubernetes apps.  It provides APIs for cluster registration, work distribution and much more.  The project is based on a hub-spoke architecture, where a single hub cluster  handles the distribution of workloads through manifests, and one or more spoke clusters  receive and apply the workload objects from the manifests. In Open Cluster Management, spoke clusters  are called managed clusters, and the component running on the hub cluster is the cluster manager. Manifests provide a summary for the status of each object, however in some use  cases this might not be sufficient as the full status for objects may be required.  OCM provides an add-on framework that allows to automatically install additional  agents on the managed clusters to provide specific features. This framework is used to install the status add-on on all managed clusters. KubeStellar currently exposes users directly to OCM inventory management and WEC registration. We plan to make the transport features provided by the OCM project pluggable in the near future.</p>"},{"location":"direct/architecture/#status-add-on-controller","title":"Status Add-On Controller","text":"<p>This module automates the installation of the status add-on agent  on all managed clusters. It is based on the  OCM Add-on Framework,  which is a framework that helps developers to develop extensions  for working with multiple clusters in custom cases. A module based on  the add-on framework has two components: a controller and the  add-on agent. The controller interacts with the add-on manager to register  the add-on, manage the distribution of the add-on to all clusters, and set  up the RBAC permissions required by the add-on agent to interact with the mailbox  namespace associated with the managed cluster. More specifically, the status  add-on controller sets up RBAC permissions to allow the add-on agent to  list and get ManifestWork objects and create and update WorkStatus objects.</p>"},{"location":"direct/architecture/#ocm-agent","title":"OCM Agent","text":"<p>The OCM Agent Module (a.k.a klusterlet) has two main controllers: the registration agent and the work agent. </p> <p>The registration agent is responsible for registering  a new cluster into OCM. The agent creates an unaccepted ManagedCluster into  the hub cluster along with a temporary CertificateSigningRequest (CSR) resource.  The cluster will be accepted by the hub control plane if the CSR is approved and  signed by any certificate provider setting filling <code>.status.certificate</code> with legit  X.509 certificates, and the ManagedCluster resource is approved by setting  <code>.spec.hubAcceptsClient</code> to true in the spec. Upon approval, the registration  agent observes the signed certificate and persists them as a local secret  named <code>hub-kubeconfig-secret</code> (by default in the <code>open-cluster-management-agent</code> namespace)  which will be mounted to the other fundamental components of klusterlet such as  the work agent. The registration process in OCM is called double opt-in mechanism,  which means that a successful cluster registration requires both sides of approval  and commitment from the hub cluster and the managed cluster.</p> <p>The work agent monitors the <code>ManifestWork</code> resource in the cluster namespace  on the hub cluster. The work agent tracks all the resources defined in ManifestWork  and updates its status. There are two types of status in ManifestWork: the resourceStatus  tracks the status of each manifest in the ManifestWork, and conditions reflects the overall  status of the ManifestWork. The work agent checks whether a resource is Available,  meaning the resource exists on the managed cluster, and Applied, meaning the resource  defined in ManifestWork has been applied to the managed cluster. To ensure the resources  applied by ManifestWork are reliably recorded, the work agent creates an AppliedManifestWork  on the managed cluster for each ManifestWork as an anchor for resources relating to ManifestWork.  When ManifestWork is deleted, the work agent runs a Foreground deletion, and that ManifestWork  will stay in deleting state until all its related resources have been fully cleaned in the managed  cluster.</p>"},{"location":"direct/architecture/#ocm-status-add-on-agent","title":"OCM Status Add-On Agent","text":"<p>The OCM Status Add-On Agent is a controller that runs alongside the OCM Agent  in the managed cluster. Its primary function is to track objects delivered  by the work agent and report the full status of those objects back to the ITS.  The KubeStellar controller can then use different user-defined summarization  policies to report status, such as the <code>wantSingletonReportedState</code> policy that reports  full status for each deployed object when the workload is delivered only to one  cluster. The controller watches AppliedManifestWork objects to determine which  objects have been delivered through the work agent. It then starts dynamic informers  to watch those objects, collect their individual statuses, and report back the status  updating WorkStatus objects in the namespace associated with the WEC in the ITS. Installing the status add-on cause status to be returned to <code>WorkStatus</code>  objects for all downsynced objects.</p>"},{"location":"direct/architecture/#kubestellar-controllers-architecture","title":"KubeStellar Controllers Architecture","text":"<p>The KubeStellar controllers architecture is based on common patterns and best  practices for Kubernetes controllers, such as the  Kubernetes Sample Controller.  A Kubernetes controller uses informers to watch for changes in Kubernetes objects, caches to store the objects, event handlers to react to events, work queues for parallel processing of tasks, and a reconciler to ensure the actual state matches the desired state. However, that pattern has been extended to provide the following features:</p> <ul> <li>Using dynamic informers</li> <li>Starting informers on all API Resources (except some that do not need   watching)</li> <li>Informers and Listers references are maintained in a hash map and   indexed by GVR (Group, Version, Resource) of the watched objects.</li> <li>Using a common work queue and set of workers, where the key is defined as follows:</li> <li>Key is a struct instead than a string, and contains the following:<ul> <li>GVR of the informer and lister for the object that generated the   event</li> <li>Structured Name of the object </li> <li>For delete event: Shallow copy of the object being deleted. This   is required for objects that need to be deleted   from the managed clusters (WECs)</li> </ul> </li> <li>Starting &amp; stopping informers dynamically based on creation or   deletion of CRDs (which add/remove APIs on the WDS).</li> <li>One client connected to the WDS space and one (or more in the future)   to connect to one or more OCM shards.</li> <li>The WDS-connected client is used to start the dynamic     informers/listers for most API resources in the WDS</li> <li>The OCM-connected client is used to start informers/listers for OCM     ManagedClusters and to copy/update/remove the wrapped objects     into/from the OCM mailbox namespaces.</li> </ul> <p>There are three controllers in the KubeStellar controller manager:</p> <ul> <li>Binding Controller - one client connected to the WDS space and one   (or more in the future) to connect to one or more ITS shards.<ul> <li>The WDS-connected client is used to start the dynamic   informers/listers for most API resources in the WDS.</li> <li>The OCM-connected client is used to start informers/listers for OCM   ManagedClusters. This is a temporary state until cluster inventory abstraction is implemented and decoupled from OCM (and then this client should be removed and we would need to use client to inventory space).</li> </ul> </li> <li>Transport controller - one client connected to the WDS space    and one client (or more in the future) to connect to one or more ITS shards.<ul> <li>The OCM-connected client is used to copy/update/remove the wrapped objects   into/from the OCM mailbox namespaces.</li> </ul> </li> <li>Status controller - TODO </li> </ul>"},{"location":"direct/architecture/#binding-controller","title":"Binding Controller","text":"<p>The Binding controller is responsible for watching workload objects and  <code>BindingPolicy</code> objects, and maintains for each of the latter a matching <code>Binding</code> object in the WDS.  A <code>Binding</code> object is mapped 1:1 to a <code>BindingPolicy</code> object and contains references to the concrete list of workload  objects and references to the concrete list of destinations that were selected by the policy.</p> <p>The architecture and the event flow of the code for create/update object events is illustrated in Figure 3 (some details might be omitted to make the flow easier to understand). </p> <p> Figure 3 - Binding Controller </p> <p>At startup, the controller code sets up the dynamic informers, the event handler and the work queue as follows:</p> <ul> <li>lists all API preferred resources (using discovery client's ServerPreferredResources()   to return only one preferred storage version for API group)</li> <li>Filters out some resources</li> <li>For each resource:<ul> <li>Creates GVR key</li> <li>Registers Event Handler</li> <li>Starts Informer</li> <li>Indexes informer and lister in a map by GVR key</li> </ul> </li> <li>Waits for all caches to sync</li> <li>Starts N workers to process work queue</li> </ul> <p>The reflector is started as part of the informer and watches specific resources on the WDS API Server; on create/update/delete object events it puts a copy of the object into the local cache. The informer invokes the event handler. The handler implements the event handling functions (AddFunc, UpdateFunc, DeleteFunc)</p> <p>A typical event flow for a create/update object event will run as follows:</p> <ol> <li> <p>Informer invokes the event handler AddFunc or UpdateFunc</p> </li> <li> <p>The event handler does some filtering (for example, to ignore update     events where the object content is not modified) and then creates a     key to store a reference to the object in the work queue. The key     contains the GVR key used to retrieve a reference to the informer     and lister for that kind of object, and a namespace + name key to     retrieve the actual object. Storing the key in the work queue is a     common pattern in client-go as the object may have changed in the     cache (which is kept updated by the reflector) by the time a worker     gets a copy from the work queue. Workers should always receive the     key and use it to retrieve the object from the cache.</p> </li> <li> <p>A worker pulls a key from the work queue, and then does the     following processing:</p> <ul> <li>Uses the GVR key to get the reference to the lister for the     object</li> <li>Gets the object from the lister cache using the NamespacedName of     the object.</li> <li>If the object was not found (because it was deleted) worker returns.     A delete event for that object consumed by the event handler enqueues     a key for Object Deleted.</li> <li>Gets the lister for BindingPolicy objects and list all binding-policies</li> <li>Iterates on all binding-policies, and for each of them:<ul> <li>Evaluates whether the object matches the downsync selection    criteria in the <code>BindingPolicy</code>.</li> <li>Whether the object is a match or not, the worker notes it through the in-memory representation     of the relevant <code>Binding</code>. If the noting of an object results in a change in the in-memory representation of the  <code>Binding</code>, the worker enqueues the latter for syncing.</li> <li>If a matched <code>BindingPolicy</code> has <code>WantSingletonReportedState</code> set to true (**see note below), the object is     labeled with a special label in order to be able to track it for status reporting.</li> <li>If no matching <code>BindingPolicy</code> has <code>WantSingletonReportedState</code> set to true, the worker sets the label value    to false if the label exists.</li> </ul> </li> <li>Worker returns and is ready to pick other keys from the queue.</li> </ul> </li> </ol> <p>WantSingletonReportedState: currently it is the user's responsibility to make sure that a binding-policy that sets <code>WantSingletonReportedState</code> to true is not in conflict with other binding-policies that do the same, and that the binding-policy selects only a single cluster.</p> <p>There are other event flows, based on the object GVK and type of event.  Error conditions may cause the re-enqueing of keys, resulting in retries. The following sections broadly describe these flows.</p>"},{"location":"direct/architecture/#object-deleted","title":"Object Deleted","text":"<p>When an object is deleted from the WDS, the handler\u2019s DeleteFunc is invoked. A shallow copy of the object is added to a field in the key  before pushing it to the work queue. Then:</p> <ul> <li> <p>Worker pulls the key from the work queue</p> </li> <li> <p>Flow continues the same way as in the create/update scenario, however   the deletedObject field in the key indicates that the object has been   deleted and that it needs to be removed from all clusters.</p> </li> <li> <p>In-memory representations of affected bindings are updated to remove the object. If any are indeed affected, they are enqueued for syncing.</p> </li> <li> <p>Worker returns and is ready to pick other keys from the queue.</p> </li> </ul>"},{"location":"direct/architecture/#bindingpolicy-created-or-updated","title":"BindingPolicy Created or Updated","text":"<p>Worker pulls key from queue; if it is a binding-policy and it has not been deleted (deletion timestamp not set) it follows the following flow:</p> <ul> <li>Re-enqueues all objects to force re-evaluation: this is done by    iterating all GVR-indexed listers, listing objects for each lister   and re-enqueuing the key for each object.</li> <li>Notes the <code>BindingPolicy</code> to create an empty in-memory representation for its <code>Binding</code>.</li> <li>Lists ManagedClusters and finds the matching clusters using the label selector expression for clusters.</li> <li>If there are matching clusters, the in-memory <code>Binding</code> representation is updated with the list of clusters.</li> <li>Enqueues the representation of the relevant <code>Binding</code> for syncing.</li> </ul> <p>Re-enqueuing all object keys forces the re-evaluation of all objects vs. all binding-policies. This is a shortcut as it would be more efficient to re-evaluate all objects vs. the changed binding-policy only, but it saves some additional complexity in the code.</p>"},{"location":"direct/architecture/#bindingpolicy-deleted","title":"BindingPolicy Deleted","text":"<p>When the binding controller first processes a new <code>BindingPolicy</code>, the binding  controller sets a finalizer on it. The Worker pulls a key from queue; if it is  a <code>BindingPolicy</code> and it has been deleted (deletion timestamp is set) it follows the flow below:</p> <ul> <li>Deletes the in-memory representation of the <code>Binding</code>. Note that the actual <code>Binding</code> object   is garbage collected due to the deletion of the <code>BindingPolicy</code> and the latter being an owner of the former (using <code>OwnerReference</code>).</li> <li>If the <code>BindingPolicy</code> had <code>WantSingletonReportedState</code> set to true, the worker enqueues all objects selected by the <code>BindingPolicy</code> for re-evaluation of the label.</li> <li>Deletes <code>BindingPolicy</code> finalizer.</li> </ul>"},{"location":"direct/architecture/#binding-syncing","title":"Binding Syncing","text":"<p>When a binding is enqueued for syncing, the worker pulls the key from the queue and follows the flow below:</p> <ul> <li>If an in memory representation of the binding is not found, the worker returns.</li> <li>The key is used to retrieve the object from the WDS.<ul> <li>If the object is not found, then a <code>Binding</code> object should be created. For the convenience of the flow,   the worker sets the \"retrived object\" as an empty <code>Binding</code> object with the appropriate Meta fields,   including the <code>BindingPolicy</code> object as the single owner reference.</li> </ul> </li> <li>The worker compares the in-memory representation of the binding with the retrieved object.<ul> <li>If the two are the same, the worker returns.</li> <li>If the two are different, the worker updates the binding object resource to reflect the state of the   in-memory representation.</li> </ul> </li> </ul>"},{"location":"direct/architecture/#new-crd-added","title":"New CRD Added","text":"<p>When a new CRD is added, the binding controller needs to start a new informer to watch instances of the new CRD on the WDS.</p> <p>The worker pulls a key from queue and creates a GVR Key; if it is a CRD and not deleted:</p> <ul> <li>Checks if an informer for that GVR was already started, return if that   is the case.</li> <li>If not, creates a new informer</li> <li>Registers the event handler for the informer (same one used for all   other api resources)</li> <li>Starts the new informer with a stopper channel, so that it can be   stopped later on by closing the channel.</li> <li>adds informer, lister and stopper channel references to the hashmap   indexed by the GVR key.</li> </ul>"},{"location":"direct/architecture/#crd-deleted","title":"CRD Deleted","text":"<p>When a CRD is deleted, the controller needs to stop the informer that was used to watch instances of that CRD on the WDS. This is because informers on CRs will keep on throwing exceptions for missing CRDs.</p> <p>The worker pulls a key from queue and creates a GVR Key; if it is a CRD and it has been deleted:</p> <ul> <li>Uses the GVR key to retrieve the stopper channel for the informer.</li> <li>Closes the stopper channel</li> <li>Removes informer, lister and stopper channel references from the   hashmap indexed by the GVR key.</li> </ul>"},{"location":"direct/architecture/#status-controller","title":"Status Controller","text":"<p>The status controller watches for WorkStatus objects on the ITS, and for WDS objects propagated by a <code>BindingPolicy</code> with  the flag <code>wantSingletonReportedState</code> set to true, updates the status of those objects with the corresponding status found in the workstatus object.</p> <p>The WorkStatus objects are created and updated on the ITS by the status add-on. The high-level flow for the singleton status update is described in Figure 4.</p> <p> Figure 4 - Status Controller </p> <p>The status add-on tracks objects applied by the work agent by watching  AppliedManifestWork objects. These objects list the GVR, name and namespace (the latter for namespaced objects) of each object applied by the related ManifestWork. The status add-on then uses this information  to ensure that a singleton informer is started for each GVR,  and to track status updates of each tracked object. The status add-on then creates/updates WorkStatus objects in the ITS with the status of tracked objects in the namespace associated with the WEC cluster. A <code>WorkStatus</code> object contains status for exactly one object, so that  status updates for one object do not require updates of a whole bundle. </p>"},{"location":"direct/architecture/#transport-controller","title":"Transport Controller","text":"<p>The transport controller is pluggable and allows the option to plug different implementations of the transport interface. The interface between the plugin and the generic code is a Go language interface (in <code>pkg/transport/transport.go</code>) that the plugin has to implement. This interface requires the following from the plugin.</p> <ul> <li>Upon registration of a new WEC, plugin should create a namespace for the WEC in the ITS and delete the namespace once the WEC registration goes away (mailbox namespace per WEC);</li> <li>Plugin must be able to wrap any number of objects into a single wrapped object;</li> <li>Have an agent that can be used to pull the wrapped objects from the mailbox namespace and apply them to the WEC. A single example for such an agent is an agent that runs on the WEC and watches the wrapped object in the corresponding namespace in the central hub and is able to unwrap it and apply the objects to the WEC. </li> <li>Have inventory representation for the clusters.</li> </ul> <p>The above list is required in order to comply with SIG Multi-Cluster Work API.</p> <p>Each plugin has an executable with a <code>main</code> func that calls the generic code (in <code>pkg/transport/cmd/generic-main.go</code>), passing the plugin object that implements the plugin interface.</p> <p>KubeStellar currently has one transport plugin implementation which is based on CNCF Sandbox project Open Cluster Management. OCM transport plugin implements the above interface and supplies a function to start the transport controller using the specific OCM implementation. Code is available here. We expect to have more transport plugin options in the future.</p> <p>The following section describes how transport controller works, while the described behavior remains the same no matter which transport plugin is selected. The high level flow for the transport controller is describted in Figure 5.</p> <p> Figure 5 - Transport Controller </p> <p>The transport controller watches for <code>Binding</code> objects on the WDS. <code>Binding</code> objects are mapped 1:1 to <code>BindingPolicy</code> and contain references to the list of concrete objects that were selected for distribution by the policy and references to the concrete list of destination that were selected by the policy. Upon a new <code>Binding</code> event (add/update/delete), the transport controller gets from the WDS api server(s) the objects listed in the <code>Binding</code> workload section. It then wraps all objects into a single wrapped object and puts a copy of the wrapped object in every matching cluster mailbox namespace in the ITS. Once the wrapped object is in the mailbox namespace of a cluster on the ITS, it's the agent responsibility to pull the wrapped object from there and apply/update/delete the workload objects on the WEC. Transport controller is based on the controller design patten and aims to bring the current state to the desired state. That is, if a WEC was removed from the <code>Binding</code>, the transport controller will also make sure to remove the matching wrapped object(s) from the WEC's mailbox namespace. Additionally, if a <code>Binding</code> is removed, transport controller will remove the matching wrapped object(s) from all mailbox namespaces.</p>"},{"location":"direct/contributor/","title":"KubeStellar Contributors","text":"<p>Make sure all pre-requisites are installed as described in pre-reqs</p>"},{"location":"direct/contributor/#unit-testing","title":"Unit testing","text":"<p>The Makefile has a target for running all the unit tests.</p> <pre><code>make test\n</code></pre>"},{"location":"direct/contributor/#integration-testing","title":"Integration testing","text":"<p>There are currently two integration tests. Contributors can run them. There is also a GitHub Actions workflow (in <code>.github/workflows/pr-test-integration.yml</code>) that runs these tests.</p> <p>These tests require you to already have <code>etcd</code> on your <code>$PATH</code>. See https://github.com/kubernetes/kubernetes/blob/v1.28.2/hack/install-etcd.sh for an example of how to do that.</p> <p>To run the tests sequentially, issue a command like the following.</p> <pre><code>CONTROLLER_TEST_NUM_OBJECTS=24 go test -v ./test/integration/controller-manager &amp;&gt; /tmp/test.log\n</code></pre> <p>If <code>CONTROLLER_TEST_NUM_OBJECTS</code> is not set then the number of objects will be 18. This parameterization by an environment variable is only a point-in-time hack, it is expected to go away once we have a test that runs reliably on a large number of objects.</p> <p>To run one of the individual tests, issue a command like the following example.</p> <pre><code>go test -v -timeout 60s -run ^TestCRDHandling$ ./test/integration/controller-manager\n</code></pre>"},{"location":"direct/contributor/#making-releases","title":"Making releases","text":"<p>See the release process document.</p>"},{"location":"direct/deploy-on-k3d/","title":"Deploy KubeStellar on K3D","text":"<p>This document shows how to deploy kubestellar on K3D hub and wec clusters</p>"},{"location":"direct/deploy-on-k3d/#prereqs","title":"Prereqs","text":"<p>In addition to pre-reqs, install k3d v5.6.0 (only k3d version tested so far)</p>"},{"location":"direct/deploy-on-k3d/#common-setup-for-standard-examples","title":"Common Setup for standard examples","text":"<ol> <li> <p>You may want to <code>set -e</code> in your shell so that any failures in the setup or usage scenarios are not lost.</p> </li> <li> <p>If you previously installed KS on K3D:     <pre><code>k3d cluster delete kubeflex\nk3d cluster delete wec1\nkubectl config delete-context kubeflex || true\nkubectl config delete-context wec1 || true\n</code></pre>    If previously running KS on Kind, clean that up with the Kind cleanup script (in <code>test/e2e/common/cleanup.sh</code>).</p> </li> <li> <p>Set environment variables to hold KubeStellar and OCM-status-addon desired versions:     <pre><code>export KUBESTELLAR_VERSION=0.21.0\nexport OCM_STATUS_ADDON_VERSION=0.2.0-rc6\n</code></pre></p> </li> <li> <p>Create a K3D hosting cluster with nginx ingress controller:     <pre><code>k3d cluster create -p \"9443:443@loadbalancer\" --k3s-arg \"--disable=traefik@server:*\" kubeflex\nhelm install ingress-nginx ingress-nginx --repo https://kubernetes.github.io/ingress-nginx --version 4.6.1 --namespace ingress-nginx --create-namespace\nkubectl config rename-context k3d-kubeflex kubeflex\n</code></pre></p> </li> <li> <p>When we use kind, the name of the container is kubeflex-control-plane and that is what we use     in the internal URL for <code>--force-internal-endpoint-lookup</code>.    Here the name of the container created by K3D is <code>k3d-kubeflex-server-0</code> so we rename it:     <pre><code>docker stop k3d-kubeflex-server-0\ndocker rename k3d-kubeflex-server-0 kubeflex-control-plane\ndocker start kubeflex-control-plane\n</code></pre>     Wait 1-2 minutes for all pods to be restarted.     Use the following command to confirm all are fully running:     <pre><code>kubectl --context kubeflex get po -A\n</code></pre></p> </li> <li> <p>Install kubestellar controller and OCM space:    We are using nginx ingress with tls passthru.    The current install for kubeflex installs also nginx ingress but specifically for kind.    To specify passthru for K3D, edit the ingress placement controller with the following command and add <code>--enable-ssl-passthrough</code> to the list of args for the container     <pre><code>kubectl edit deployment ingress-nginx-controller -n ingress-nginx  </code></pre>    Then initialize kubeflex and create the imbs1 space with OCM running in it:     <pre><code>kflex init\nkubectl apply -f https://raw.githubusercontent.com/kubestellar/kubestellar/v${KUBESTELLAR_VERSION}/config/postcreate-hooks/kubestellar.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubestellar/kubestellar/v${KUBESTELLAR_VERSION}/config/postcreate-hooks/ocm.yaml\nkflex create imbs1 --type vcluster -p ocm\n</code></pre></p> </li> <li> <p>Install OCM status addon    First wait until managedclusteraddons resource shows up on imbs1 using:     <code>shell    kubectl --context imbs1 api-resources | grep managedclusteraddons</code>    then install status addon:     <pre><code>helm --kube-context imbs1 upgrade --install status-addon -n open-cluster-management oci://ghcr.io/kubestellar/ocm-status-addon-chart --version v${OCM_STATUS_ADDON_VERSION}\n</code></pre></p> </li> <li> <p>Create a Workload Description Space <code>wds1</code> in KubeFlex.     <pre><code>kflex create wds1 -p kubestellar\n</code></pre></p> </li> <li> <p>Run the OCM based transport controller in a pod. NOTE: This is work in progress, in the future the controller will be deployed through a Helm chart.</p> <p>Run transport deployment script (in <code>scripts/deploy-transport-controller.sh</code>), as follows. This script requires that the user's current kubeconfig context be for the kubeflex hosting cluster. This script expects to get two or three arguments - (1) wds name; (2) imbs name; and (3) transport controller image. While the first and second arguments are mandatory, the third one is optional. The transport controller image argument can be specified to a specific image, or, if omitted, it defaults to the OCM transport plugin release that preceded the KubeStellar release being used. For example, one can deploy transport controller using the following commands: <pre><code>kflex ctx\nbash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/v${KUBESTELLAR_VERSION}/scripts/deploy-transport-controller.sh) wds1 imbs1\n</code></pre></p> </li> <li> <p>Create the Workload Execution Cluster <code>wec1</code> and register it    Make sure <code>wec1</code> shares the same docker network as the <code>kubeflex</code> hosting cluster.     <pre><code>k3d cluster create -p \"31080:80@loadbalancer\"  --network k3d-kubeflex wec1\nkubectl config rename-context k3d-wec1 wec1\n</code></pre>    Register <code>wec1</code>:     <pre><code>flags=\"--force-internal-endpoint-lookup\"\nclusteradm --context imbs1 get token | grep '^clusteradm join' | sed \"s/&lt;cluster_name&gt;/wec1/\" | awk '{print $0 \" --context 'wec1' '${flags}'\"}' | sh\n</code></pre>    Wait for csr to be created:     <pre><code>kubectl --context imbs1 get csr --watch\n</code></pre>     and then accept pending wec1 cluster     <pre><code>clusteradm --context imbs1 accept --clusters wec1\n</code></pre>     Confirm wec1 is accepted and label it for the BindingPolicy:     <pre><code>kubectl --context imbs1 get managedclusters\nkubectl --context imbs1 label managedcluster wec1 location-group=edge name=wec1\n</code></pre></p> </li> <li> <p>(optional) Check relevant deployments and statefulsets running in the hosting cluster. Expect to see the <code>kubestellar-controller-manager</code> in the <code>wds1-system</code> namespace and the  statefulset <code>vcluster</code> in the <code>imbs1-system</code> namespace, both fully ready.</p> <p><pre><code>kubectl --context kubeflex get deployments,statefulsets --all-namespaces\n</code></pre>    The output should look something like the following: <pre><code>NAMESPACE         NAME                                             READY   UP-TO-DATE   AVAILABLE   AGE\nkube-system       deployment.apps/coredns                          1/1     1            1           10m\nkube-system       deployment.apps/local-path-provisioner           1/1     1            1           10m\nkube-system       deployment.apps/metrics-server                   1/1     1            1           10m\ningress-nginx     deployment.apps/ingress-nginx-controller         1/1     1            1           9m50s\nkubeflex-system   deployment.apps/kubeflex-controller-manager      1/1     1            1           5m45s\nwds1-system       deployment.apps/kube-apiserver                   1/1     1            1           3m54s\nwds1-system       deployment.apps/kube-controller-manager          1/1     1            1           3m54s\nwds1-system       deployment.apps/kubestellar-controller-manager   1/1     1            1           3m29s\nwds1-system       deployment.apps/transport-controller             1/1     1            1           2m52s\n\nNAMESPACE         NAME                                   READY   AGE\nkubeflex-system   statefulset.apps/postgres-postgresql   1/1     6m12s\nimbs1-system      statefulset.apps/vcluster              1/1     5m17s\n</code></pre></p> </li> </ol>"},{"location":"direct/example-wecs/","title":"Create and Register WECs for examples","text":"<p>The following steps show how to create new clusters and register them with the hub as descibed in the official open cluster management docs.</p> <ol> <li> <p>Execute the following commands to create two kind clusters, named <code>cluster1</code> and <code>cluster2</code>, and register them with the OCM hub. These clusters will serve as workload clusters. If you have previously executed these commands, you might already have contexts named <code>cluster1</code> and <code>cluster2</code>. If so, you can remove these contexts using the commands <code>kubectl config delete-context cluster1</code> and <code>kubectl config delete-context cluster2</code>.</p> <pre><code>: set flags to \"\" if you have installed KubeStellar on an OpenShift cluster\nflags=\"--force-internal-endpoint-lookup\"\nclusters=(cluster1 cluster2);\nfor cluster in \"${clusters[@]}\"; do\nkind create cluster --name ${cluster}\nkubectl config rename-context kind-${cluster} ${cluster}\nclusteradm --context imbs1 get token | grep '^clusteradm join' | sed \"s/&lt;cluster_name&gt;/${cluster}/\" | awk '{print $0 \" --context '${cluster}' --singleton '${flags}'\"}' | sh\ndone\n</code></pre> <p>The <code>clusteradm</code> command grabs a token from the hub (<code>imbs1</code> context), and constructs the command to apply the new cluster to be registered as a managed cluster on the OCM hub.</p> </li> <li> <p>Repeatedly issue the command:</p> <pre><code>kubectl --context imbs1 get csr\n</code></pre> <p>until you see that the certificate signing requests (CSR) for both cluster1 and cluster2 exist. Note that the CSRs condition is supposed to be <code>Pending</code> until you approve them in step 4.</p> </li> <li> <p>Once the CSRs are created approve the csrs to complete the cluster registration with the command:</p> <pre><code>clusteradm --context imbs1 accept --clusters cluster1\nclusteradm --context imbs1 accept --clusters cluster2\n</code></pre> </li> <li> <p>Check the new clusters are in the OCM inventory and label them:</p> <pre><code>kubectl --context imbs1 get managedclusters\nkubectl --context imbs1 label managedcluster cluster1 location-group=edge name=cluster1\nkubectl --context imbs1 label managedcluster cluster2 location-group=edge name=cluster2\n</code></pre> </li> </ol>"},{"location":"direct/examples/","title":"KubeStellar Usage Examples","text":"<p>This document shows some simple examples of using the release that contains this version of this document. See also the doc about using an existing hosting cluster and multi-machine scenarios for considerations for different scenarios. For historical examples, see our blog posts.</p> <p>There are also end-to-end (E2E) tests that are based on scenario 4 and an extended variant of scenario 1. These tests normally exercise the copy of the repo containing them (rather than a release). They can alternatively test a release. See the e2e tests (in <code>test/e2e</code>). Contributors can run these tests, and CI includes checking that these E2E tests pass. These tests are written in <code>bash</code>, so that contributors can easily follow them.</p>"},{"location":"direct/examples/#prereqs","title":"Prereqs","text":"<p>See pre-reqs.</p>"},{"location":"direct/examples/#common-setup","title":"Common Setup","text":"<p>The following steps establish an initial state used in the examples below.</p> <ol> <li> <p>You may want to <code>set -e</code> in your shell so that any failures in the setup or usage scenarios are not lost.</p> </li> <li> <p>If you ran through these scenarios previously then you will need to do a bit of cleanup first. See how it is done in the cleanup script for our E2E tests (in <code>test/e2e/common/cleanup.sh</code>).</p> </li> <li> <p>Set environment variables to hold KubeStellar and OCM-status-addon desired versions:</p> <pre><code>export KUBESTELLAR_VERSION=0.21.2\nexport OCM_STATUS_ADDON_VERSION=0.2.0-rc6\nexport OCM_TRANSPORT_PLUGIN=0.1.2\n</code></pre> </li> <li> <p>Create a Kind hosting cluster with nginx ingress controller and KubeFlex controller-manager installed:</p> <p><pre><code>kflex init --create-kind\n</code></pre>    If you are installing KubeStellar on an existing Kubernetes or OpenShift cluster, just use the command <code>kflex init</code>.</p> </li> <li> <p>Update the post-create-hooks in KubeFlex to install kubestellar with the desired images:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubestellar/kubestellar/v${KUBESTELLAR_VERSION}/config/postcreate-hooks/kubestellar.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubestellar/kubestellar/v${KUBESTELLAR_VERSION}/config/postcreate-hooks/ocm.yaml\n</code></pre> </li> <li> <p>Create an inventory &amp; mailbox space of type <code>vcluster</code> running OCM (Open Cluster Management) in KubeFlex. Note that <code>-p ocm</code> runs a post-create hook on the vcluster control plane which installs OCM on it.</p> <pre><code>kflex create imbs1 --type vcluster -p ocm\n</code></pre> </li> <li> <p>Install status add-on on imbs1:</p> <p>Wait until the <code>managedclusteraddons</code> resource shows up on <code>imbs1</code>. You can check on that with the command:</p> <pre><code>kubectl --context imbs1 api-resources | grep managedclusteraddons\n</code></pre> <p>and then install the status add-on:</p> <pre><code>helm --kube-context imbs1 upgrade --install status-addon -n open-cluster-management oci://ghcr.io/kubestellar/ocm-status-addon-chart --version v${OCM_STATUS_ADDON_VERSION}\n</code></pre> <p>see here for more details on the add-on.</p> </li> <li> <p>Create a Workload Description Space <code>wds1</code> in KubeFlex. Similarly to before, <code>-p kubestellar</code> runs a post-create hook on the k8s control plane that starts an instance of a KubeStellar controller manager which connects to the <code>wds1</code> front-end and the <code>imbs1</code> OCM control plane back-end.</p> <pre><code>kflex create wds1 -p kubestellar\n</code></pre> </li> <li> <p>Deploy the OCM based transport controller</p> <pre><code>helm --kube-context kind-kubeflex upgrade --install ocm-transport-plugin oci://ghcr.io/kubestellar/ocm-transport-plugin/chart/ocm-transport-plugin --version ${OCM_TRANSPORT_PLUGIN} \\\n--set transport_cp_name=imbs1 \\\n--set wds_cp_name=wds1\n</code></pre> </li> <li> <p>Follow the steps to create and register two clusters with OCM.</p> </li> <li> <p>(optional) Check relevant deployments and statefulsets running in the hosting cluster. Expect to see the <code>kubestellar-controller-manager</code> in the <code>wds1-system</code> namespace and the statefulset <code>vcluster</code> in the <code>imbs1-system</code> namespace, both fully ready.</p> <p><pre><code>kubectl --context kind-kubeflex get deployments,statefulsets --all-namespaces\n</code></pre>    The output should look something like the following:</p> <pre><code>NAMESPACE            NAME                                             READY   UP-TO-DATE   AVAILABLE   AGE\ningress-nginx        deployment.apps/ingress-nginx-controller         1/1     1            1           22h\nkube-system          deployment.apps/coredns                          2/2     2            2           22h\nkubeflex-system      deployment.apps/kubeflex-controller-manager      1/1     1            1           22h\nlocal-path-storage   deployment.apps/local-path-provisioner           1/1     1            1           22h\nwds1-system          deployment.apps/kube-apiserver                   1/1     1            1           22m\nwds1-system          deployment.apps/kube-controller-manager          1/1     1            1           22m\nwds1-system          deployment.apps/kubestellar-controller-manager   1/1     1            1           21m\nwds1-system          deployment.apps/transport-controller             1/1     1            1           21m\n\nNAMESPACE         NAME                                   READY   AGE\nimbs1-system      statefulset.apps/vcluster              1/1     11h\nkubeflex-system   statefulset.apps/postgres-postgresql   1/1     22h\n</code></pre> </li> </ol>"},{"location":"direct/examples/#scenario-1-multi-cluster-workload-deployment-with-kubectl","title":"Scenario 1 - multi-cluster workload deployment with kubectl","text":"<p>This scenario proceeds from the state established by the common setup.</p> <p>Check for available clusters with label <code>location-group=edge</code></p> <pre><code>kubectl --context imbs1 get managedclusters -l location-group=edge\n</code></pre> <p>Create a BindingPolicy to deliver an app to all clusters in wds1:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: nginx-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/name\":\"nginx\"}\nEOF\n</code></pre> <p>This BindingPolicy configuration determines where to deploy the workload by using the label selector expressions found in clusterSelectors. It also specifies what to deploy through the downsync.labelSelectors expressions. Each matchLabels expression is a criterion for selecting a set of objects based on their labels. Other criteria can be added to filter objects based on their namespace, api group, resource, and name. If these criteria are not specified, all objects with the matching labels are selected. If an object has multiple labels, it is selected only if it matches all the labels in the matchLabels expression. If there are multiple objectSelectors, an object is selected if it matches any of them.</p> <p>Now deploy the app:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    app.kubernetes.io/name: nginx\n  name: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: nginx\n  labels:\n    app.kubernetes.io/name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: public.ecr.aws/nginx/nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Verify that manifestworks wrapping the objects have been created in the mailbox namespaces:</p> <pre><code>kubectl --context imbs1 get manifestworks -n cluster1\nkubectl --context imbs1 get manifestworks -n cluster2\n</code></pre> <p>Verify that the deployment has been created in both clusters</p> <pre><code>kubectl --context cluster1 get deployments -n nginx\nkubectl --context cluster2 get deployments -n nginx\n</code></pre> <p>Please note, in line with Kubernetes\u2019 best practices, the order in which you apply a BindingPolicy and the objects doesn\u2019t affect the outcome. You can apply the BindingPolicy first followed by the objects, or vice versa. The result remains consistent because the binding controller identifies any changes in either the BindingPolicy or the objects, triggering the start of the reconciliation loop.</p>"},{"location":"direct/examples/#scenario-2-using-the-hosting-cluster-as-wds-to-deploy-a-custom-resource","title":"Scenario 2 - using the hosting cluster as WDS to deploy a custom resource","text":"<p>This scenario follows on from the state established by scenario 1.</p> <p>The hosting cluster can act as a Workload Description Space (WDS) to distribute your workloads to multiple clusters. This feature works well for Custom Resources, but not for standard Kubernetes resources (deployments, pods, replicasets, etc.). The reason is that the hosting cluster\u2019s controller manager creates pods for those resources on the hosting cluster itself, while the Kubestellar controller copies them to the Workload Execution Clusters (WECs). You can use any Custom Resource to wrap any Kubernetes object you want to send to the WECs. But if you have operators or controllers on the hosting cluster that work on the Custom Resource you want to send, make sure they don\u2019t create workloads on the hosting cluster that you did not intend to create there.</p> <p>In order to run this scenario using the post-create-hook method you need the raise the permissions for the kubeflex controller manager:</p> <pre><code>kubectl --context kind-kubeflex apply -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: kubeflex-manager-cluster-admin-rolebinding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: kubeflex-controller-manager\n  namespace: kubeflex-system\nEOF\n</code></pre> <p>To create a second WDS based on the hosting cluster, run the commands:</p> <pre><code>kflex create wds2 -t host\n\nhelm --kube-context kind-kubeflex upgrade --install ocm-transport-plugin oci://ghcr.io/kubestellar/ocm-transport-plugin/chart/ocm-transport-plugin --version ${OCM_TRANSPORT_PLUGIN} \\\n--set transport_cp_name=imbs1 \\\n--set wds_cp_name=wds2\n</code></pre> <p>where the <code>-t host</code> option specifies a control plane of type <code>host</code>. You can only create on control plane of type <code>host</code>.</p> <p>In this example, we use the helm chart method to install the kubestellat controller manager for the hosting cluster so that we can pass additional startup options.</p> <p>Label the <code>wds2</code> control plane as type <code>wds</code>:</p> <pre><code>kubectl label cp wds2 kflex.kubestellar.io/cptype=wds\n</code></pre> <p>For this example, we use the <code>AppWrapper</code> custom resource defined in the multi cluster app dispatcher project.</p> <p>Install the AppWrapper CRD in the WDS and the WECs.</p> <pre><code>clusters=(wds2 cluster1 cluster2);\nfor cluster in \"${clusters[@]}\"; do\nkubectl --context ${cluster} apply -f https://raw.githubusercontent.com/project-codeflare/multi-cluster-app-dispatcher/v1.39.0/config/crd/bases/workload.codeflare.dev_appwrappers.yaml\ndone\n</code></pre> <p>Apply the kubestellar controller-manager helm chart with the option to allow only delivery of objects with api group <code>workload.codeflare.dev</code></p> <pre><code>helm --kube-context kind-kubeflex upgrade --install -n wds2-system kubestellar oci://ghcr.io/kubestellar/kubestellar/controller-manager-chart --version ${KUBESTELLAR_VERSION} --set ControlPlaneName=wds2 --set APIGroups=workload.codeflare.dev\n</code></pre> <p>Check that the kubestellar controller for wds2 is started:</p> <pre><code>kubectl get deployments.apps -n wds2-system kubestellar-controller-manager\n</code></pre> <p>If desired, you may remove the <code>kubeflex-manager-cluster-admin-rolebinding</code> after the kubestellar-controller-manager is started, with the command <code>kubectl --context kind-kubeflex delete clusterrolebinding kubeflex-manager-cluster-admin-rolebinding</code></p> <p>Run the following comamand to give permission for the Klusterlet to operate on the appwrapper cluster resource.</p> <pre><code>clusters=(cluster1 cluster2);\nfor cluster in \"${clusters[@]}\"; do\nkubectl --context ${cluster} apply -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: appwrappers-access\nrules:\n- apiGroups: [\"workload.codeflare.dev\"]\n  resources: [\"appwrappers\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: klusterlet-appwrappers-access\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: appwrappers-access\nsubjects:\n- kind: ServiceAccount\n  name: klusterlet-work-sa\n  namespace: open-cluster-management-agent\nEOF\ndone\n</code></pre> <p>This step will be eventually automated, see this issue for more details.</p> <p>Next, apply an appwrapper object to wds2:</p> <pre><code>kubectl --context wds2 apply -f  https://raw.githubusercontent.com/project-codeflare/multi-cluster-app-dispatcher/v1.39.0/test/yaml/0008-aw-default.yaml\n</code></pre> <p>Label the appwrapper to match the binding policy:</p> <pre><code>kubectl --context wds2 label appwrappers.workload.codeflare.dev defaultaw-schd-spec-with-timeout-1 app.kubernetes.io/part-of=my-appwrapper-app\n</code></pre> <p>Finally, apply the BindingPolicy:</p> <pre><code>kubectl --context wds2 apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: aw-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/part-of\":\"my-appwrapper-app\"}\nEOF\n</code></pre> <p>Check that the app wrapper has been delivered to both clusters:</p> <pre><code>kubectl --context cluster1 get appwrappers\nkubectl --context cluster2 get appwrappers\n</code></pre>"},{"location":"direct/examples/#scenario-3-multi-cluster-workload-deployment-with-helm","title":"Scenario 3 - multi-cluster workload deployment with helm","text":"<p>This scenario proceeds from the state established by the common setup.</p> <p>Create a BindingPolicy for the helm chart app:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: postgres-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\n      \"app.kubernetes.io/managed-by\": Helm,\n      \"app.kubernetes.io/instance\": postgres}\nEOF\n</code></pre> <p>Note that helm sets <code>app.kubernetes.io/instance</code> to the name of the installed release.</p> <p>Create and label the namespace and install the chart:</p> <pre><code>kubectl --context wds1 create ns postgres-system\nkubectl --context wds1 label ns postgres-system app.kubernetes.io/managed-by=Helm app.kubernetes.io/instance=postgres\nhelm --kube-context wds1 install -n postgres-system postgres oci://registry-1.docker.io/bitnamicharts/postgresql\n</code></pre> <p>Verify that statefulset has been created in both clusters</p> <pre><code>kubectl --context cluster1 get statefulsets -n postgres-system\nkubectl --context cluster2 get statefulsets -n postgres-system\n</code></pre>"},{"location":"direct/examples/#optional-propagate-helm-metadata-secret-to-managed-clusters","title":"[Optional] Propagate helm metadata Secret to managed clusters","text":"<p>Run \"helm list\" on the wds1:</p> <pre><code>$ helm --kube-context wds1 list -n postgres-system\nNAME            NAMESPACE       REVISION        UPDATED                                 STATUS       CHART                    APP VERSION\npostgres        postgres-system 1               2023-10-31 13:39:52.550071 -0400 EDT    deployed     postgresql-13.2.0        16.0.0\n</code></pre> <p>And try that on the managed clusters</p> <pre><code>$ helm list --kube-context cluster1 -n postgres-system\n: returns empty\n$ helm list --kube-context cluster2 -n postgres-system\n: returns empty\n</code></pre> <p>This is because Helm creates a <code>Secret</code> object to hold its metadata about a \"release\" (chart instance) but Helm does not apply the usual labels to that object, so it is not selected by the <code>BindingPolicy</code> above and thus does not get delivered. The workload is functioning in the WECs, but <code>helm list</code> does not recognize its handiwork there. That labeling could be done for example with:</p> <pre><code>kubectl --context wds1 label secret -n postgres-system $(kubectl --context wds1 get secrets -n postgres-system -l name=postgres -l owner=helm  -o jsonpath='{.items[0].metadata.name}') app.kubernetes.io/managed-by=Helm app.kubernetes.io/instance=postgres\n</code></pre> <p>Verify that the chart shows up on the managed clusters:</p> <pre><code>helm list --kube-context cluster1 -n postgres-system\nhelm list --kube-context cluster2 -n postgres-system\n</code></pre> <p>Implementing this in a controller for automated propagation of helm metadata is tracked in this issue.</p>"},{"location":"direct/examples/#scenario-4-singleton-status","title":"Scenario 4 - Singleton status","text":"<p>This scenario proceeds from the state established by the common setup.</p> <p>This scenario shows how to get the full status updated when setting <code>wantSingletonReportedState</code> in the BindingPolicy. This still an experimental feature.</p> <p>Apply a BindingPolicy with the <code>wantSingletonReportedState</code> flag set:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: nginx-singleton-bpolicy\nspec:\n  wantSingletonReportedState: true\n  clusterSelectors:\n  - matchLabels: {\"name\":\"cluster1\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/name\":\"nginx-singleton\"}\nEOF\n</code></pre> <p>Apply a new deployment for the singleton BindingPolicy:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-singleton-deployment\n  labels:\n    app.kubernetes.io/name: nginx-singleton\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: public.ecr.aws/nginx/nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Verify that the status is available in wds1 for the deployment by running the command:</p> <pre><code>kubectl --context wds1 get deployments nginx-singleton-deployment -o yaml\n</code></pre> <p>Finally, scale the deployment from 1 to 2 replicas in wds1:</p> <pre><code>kubectl --context wds1 scale deployment nginx-singleton-deployment --replicas=2\n</code></pre> <p>and verify that replicas has been updated in cluster1 and wds1:</p> <pre><code>kubectl --context cluster1 get deployment nginx-singleton-deployment\nkubectl --context wds1 get deployment nginx-singleton-deployment\n</code></pre>"},{"location":"direct/examples/#scenario-5-resiliency-testing","title":"Scenario 5 - Resiliency testing","text":"<p>This is a test that you can do after finishing Scenario 1.</p> <p>TODO: rewrite this so that it makes sense after Scenario 4.</p> <p>Bring down the control plane: stop and restart wds1 and imbs1 API servers, KubeFlex and KubeStellar controllers:</p> <p>First stop all:</p> <pre><code>kubectl --context kind-kubeflex scale deployment -n wds1-system kube-apiserver --replicas=0\nkubectl --context kind-kubeflex scale statefulset -n imbs1-system vcluster --replicas=0\nkubectl --context kind-kubeflex scale deployment -n kubeflex-system kubeflex-controller-manager --replicas=0\nkubectl --context kind-kubeflex scale deployment -n wds1-system kubestellar-controller-manager --replicas=0\nkubectl --context kind-kubeflex scale deployment -n wds1-system transport-controller --replicas=0\n</code></pre> <p>Then restart all:</p> <pre><code>kubectl --context kind-kubeflex scale deployment -n wds1-system kube-apiserver --replicas=1\nkubectl --context kind-kubeflex scale statefulset -n imbs1-system vcluster --replicas=1\nkubectl --context kind-kubeflex scale deployment -n kubeflex-system kubeflex-controller-manager --replicas=1\nkubectl --context kind-kubeflex scale deployment -n wds1-system kubestellar-controller-manager --replicas=1\nkubectl --context kind-kubeflex scale deployment -n wds1-system transport-controller --replicas=1\n</code></pre> <p>Wait for about a minute for all pods to restart, then apply a new BindingPolicy:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: nginx-res-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/name\":\"nginx-res\"}\nEOF\n</code></pre> <p>and a new workload:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    app.kubernetes.io/name: nginx-res\n  name: nginx-res\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-res-deployment\n  namespace: nginx-res\n  labels:\n    app.kubernetes.io/name: nginx-res\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-res\n  template:\n    metadata:\n      labels:\n        app: nginx-res\n    spec:\n      containers:\n      - name: nginx-res\n        image: public.ecr.aws/nginx/nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Verify that deployment has been created in both clusters</p> <pre><code>kubectl --context cluster1 get deployments -n nginx-res\nkubectl --context cluster2 get deployments -n nginx-res\n</code></pre>"},{"location":"direct/examples/#scenario-6-multi-cluster-workload-deployment-of-app-with-serviceaccount-with-argocd","title":"Scenario 6 - multi-cluster workload deployment of app with ServiceAccount with ArgoCD","text":"<p>This scenario is something you can do after the common setup.</p> <p>Before running this scenario, install ArgoCD on the hosting cluster and configure it work with the WDS as outlined here.</p> <p>Including a ServiceAccount tests whether there will be a controller fight over a token Secret for that ServiceAccount, which was observed in some situations with older code.</p> <p>Apply the following BindingPolicy to wds1:</p> <pre><code>kubectl --context wds1 apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: argocd-sa-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"argocd.argoproj.io/instance\":\"nginx-sa\"}\nEOF\n</code></pre> <p>Switch context to hosting cluster and argocd namespace (this is required by argo to create an app with the CLI)</p> <pre><code>kubectl config use-context kind-kubeflex\nkubectl config set-context --current --namespace=argocd\n</code></pre> <p>Create a new application in ArgoCD:</p> <pre><code>argocd app create nginx-sa --repo https://github.com/pdettori/sample-apps.git --path nginx --dest-server https://wds1.wds1-system --dest-namespace nginx-sa\n</code></pre> <p>Open browser to Argo UI:</p> <pre><code>open https://argocd.localtest.me:9443\n</code></pre> <p>Open the app <code>nginx-sa</code> and sync it by clicking the \"sync\" button and then \"synchronize\".</p> <p>Alternatively, use the CLI to sync the app:</p> <pre><code>argocd app sync nginx-sa\n</code></pre> <p>Finally, check if the app has been deployed to the two clusters.</p> <pre><code>kubectl --context cluster1 -n nginx-sa get deployments,sa,secrets\nkubectl --context cluster2 -n nginx-sa get deployments,sa,secrets\n</code></pre> <p>Repeat multiple syncing on Argo and verify that extra secrets for the service acccount are not created both wds1 and clusters:</p> <pre><code>kubectl --context wds1 -n nginx-sa get secrets\nkubectl --context cluster1 -n nginx-sa get secrets\nkubectl --context cluster2 -n nginx-sa get secrets\n</code></pre>"},{"location":"direct/hosting-cluster/","title":"Using existing Kubernetes cluster to host KubeStellar","text":"<p>Status of this document: it is the barest of a start. Much more needs to be written.</p>"},{"location":"direct/hosting-cluster/#using-an-existing-kind-cluster-as-the-hosting-cluster","title":"Using an existing Kind cluster as the hosting cluster","text":"<p>This requires a pre-existing Kind cluster that has an Ingress controller that is listening on host port 9443 and configured with TLS passthrough.</p> <p>The examples say to create a Kind cluster for hosting using the following command.</p> <pre><code>kflex init --create-kind\n</code></pre> <p>To use a pre-existing Kind cluster instead, make sure that your current kubeconfig context is for accessing that cluster and issue the following command.</p> <pre><code>kflex init\n</code></pre> <p>All the subsequent kubectl and helm commands that say to use the kubeconfig context named <code>kind-kubeflex</code> need to be modified to use the appropriate kubeconfig context for accessing the hosting cluster.</p>"},{"location":"direct/hosting-cluster/#using-an-existing-openshift-cluster-as-the-hosting-cluster","title":"Using an existing OpenShift cluster as the hosting cluster","text":"<p>This is similar to using an existing Kind cluster but requires an additional modification. Modify the <code>kflex</code> init command and subsequent kubeconfig context references as in the existing-kind-cluster scenario.</p> <p>Additionally, the recipe for registering a WEC with the ITS needs to be modified. In the <code>clusteradm</code> command, omit the <code>--force-internal-endpoint-lookup</code> flag. If following the example commands literally, this means to define <code>flags=\"\"</code> rather than <code>flags=\"--force-internal-endpoint-lookup\"</code>.</p>"},{"location":"direct/hosting-cluster/#when-everything-is-not-on-the-same-machine","title":"When everything is not on the same machine","text":"<p>Thus far we can only say how to handle this when the hosting cluster is OpenShift. The problem is getting URLs that work from everywhere. OpenShift is a hosted product, your clusters have domain names that are resolvable from everywhere. In other words, if you use an OpenShift cluster as your hosting cluster then this problem is already solved.</p>"},{"location":"direct/ks-on-kind/","title":"KubeStellar on KIND","text":""},{"location":"direct/ks-on-ocp/","title":"KubeStellar on OCP","text":""},{"location":"direct/packaging/","title":"Packaging and Delivery","text":""},{"location":"direct/packaging/#outline-of-github-repositories","title":"Outline of GitHub repositories","text":"<p>The following is a graph of the GitHub repositories in the <code>kubestellar</code> GitHub organization and the dependencies among them. The repo at the tail of an arrow depends on the repo at the head of the arrow. These are not just build-time dependencies but any reference from one repo to another.</p> flowchart LR     kubestellar --&gt; kubeflex     kubestellar --&gt; ocm-status-addon     ocm-status-addon --&gt; kubestellar     ocm-transport-plugin --&gt; kubestellar     kubestellar --&gt; ocm-transport-plugin  <p>The references from ocm-status-addon to kubestellar are only in documentation and are in the process of being removed (no big difficulty is anticipated).</p>"},{"location":"direct/packaging/#kubeflex","title":"KubeFlex","text":"<p>See the GitHub repo.</p>"},{"location":"direct/packaging/#ocm-status-addon","title":"OCM Status Addon","text":"<p>The OCM Status Addon repo is the source of a RedHat-style operator.</p>"},{"location":"direct/packaging/#outline-of-ocm-status-addon-publishing","title":"Outline of OCM status addon publishing","text":"flowchart LR     subgraph \"ocm-status-addon@GitHub\"     osa_code[OSA source code]     osa_hc_src[OSA Helm chart source]     end     osa_ctr_image[OSA container image] --&gt; osa_code     osa_hc_repo[published OSA Helm Chart] --&gt; osa_hc_src     osa_hc_src -.-&gt; osa_ctr_image     osa_hc_repo -.-&gt; osa_ctr_image  <p>The dashed dependencies are at run time, not build time.</p> <p>\"OSA\" is OCM Status Addon.</p>"},{"location":"direct/packaging/#ocm-status-addon-container-image","title":"OCM status addon container image","text":"<p>There is a container image at ghcr.io/kubestellar/ocm-status-addon.</p> <p>The container image is built and published by that repo's release process, which is documented at its <code>docs/release.md</code> file.</p> <p>Following are notable images in that repo.</p> tag git commit SHA256 digest 0.2.0-alpha.1 1c36248df2c4379ab4ce2a5945d5cce3145dc211 b8ef1802a1d9f30dd65f16f94c86dc0218f2cf30bb34549a6248aeeb2b13eb4a 0.2.0-rc5 6753b9e169d47ff1592f9638d1b53c826f7ee1b9 35d97e4b523388ee28f537b176b3f8dbbc888e31a46bbd308d67cd5a0735c249 0.2.0-rc6 25d27c0e5c7c70fdd578619b4e8381897cccd441 5b5eab49397984ee35cd431b4918f4584e2f56cf0596080e4ec560e46583d8b2 0.2.0-rc7 ce5c42d51b389b0389aa3c2252371832eb0e7137 070ac0600856deebd227a4ca0fe39ba1b685b0563274a182942af84e4218e502 latest ce5c42d51b389b0389aa3c2252371832eb0e7137 070ac0600856deebd227a4ca0fe39ba1b685b0563274a182942af84e4218e502 <p>To support testing, <code>make ko-local-build</code> will build a single-platform image and not push it, only leave it among your Docker images. The single platform's OS is Linux. The single platform's ISA is defined by the <code>make</code> variable <code>ARCH</code>, which defaults to what <code>go env GOARCH</code> prints.</p>"},{"location":"direct/packaging/#ocm-status-addon-helm-chart","title":"OCM status addon Helm chart","text":"<p>The operator is delivered by a Helm chart at ghcr.io/kubestellar/ocm-status-addon-chart. The chart references the container image.</p> <p>The following notable versions exist.</p> OCI tag = version in chart appVersion in chart referenced container tag v0.2.0-alpha.1 v0.2.0-alpha.1 0.2.0-alpha.1 v0.2.0-rc5 v0.2.0-rc5 0.2.0-rc5 v0.2.0-rc6 v0.2.0-rc6 0.2.0-rc6 v0.2.0-rc7 v0.2.0-rc7 0.2.0-rc7"},{"location":"direct/packaging/#ocm-transport-plugin","title":"OCM Transport Plugin","text":"<p>The source is the GitHub repo github.com/kubestellar/ocm-transport-plugin</p>"},{"location":"direct/packaging/#ocm-transport-container-image","title":"OCM Transport container image","text":"<p>This appears at ghcr.io/kubestellar/ocm-transport-plugin/transport-controller.</p> <p>TODO: document how the image is built and published, including explain versioning.</p>"},{"location":"direct/packaging/#kubestellar","title":"KubeStellar","text":""},{"location":"direct/packaging/#warning","title":"WARNING","text":"<p>Literal KubeStellar release numbers appear here, and are historical. The version of this document in a given release does not mention that release. See the release process for more details on what self-references are and are not handled.</p>"},{"location":"direct/packaging/#outline-of-publishing","title":"Outline of publishing","text":"flowchart LR     subgraph cladm_repo[\"ocm/clusteradm@GitHub\"]     cladm_src[\"clusteradm source\"]     end     subgraph ks_repo[\"kubestellar@GitHub\"]     kcm_code[KCM source code]     kcm_hc_src[KCM Helm chart source]     ks_pch[kubestellar PostCreateHook]     ocm_pch[\"ocm PostCreateHook\"]     ks_scripts -.-&gt; ocm_pch     ks_scripts -.-&gt; ks_pch     end     cladm_image[\"ks/clusteradm image\"] --&gt; cladm_src     ocm_pch -.-&gt; cladm_image     kcm_ctr_image[KCM container image] --&gt; kcm_code     kcm_hc_repo[published KCM Helm Chart] --&gt; kcm_hc_src     kcm_hc_src -.-&gt; kcm_ctr_image     kcm_hc_repo -.-&gt; kcm_ctr_image     ks_pch  -.-&gt; kcm_hc_repo     ks_scripts -.-&gt; osa_hc_repo[published OSA Helm Chart]     ks_scripts -.-&gt; otp_something[\"published OTP ??\"]     ks_scripts -.-&gt; KubeFlex  <p>The dashed dependencies are at run time, not build time.</p> <p>\"KCM\" is the KubeStellar controller-manager.</p> <p>\"ks_scripts\" are the user-facing instructions and the end-to-end tests that use published artifacts (container images, Helm charts). (There are also e2e tests that do not use previously published artifacts, their temporary local artifacts are beyond the scope of this document.)</p> <p>NOTE: among the references to published artifacts, some have a   version that is maintained in Git while others have a placeholder in   Git that is replaced in the publishing process. See the release   document for more details. This is an on-going matter   of development.</p>"},{"location":"direct/packaging/#local-copy-of-kubestellar-git-repo","title":"Local copy of KubeStellar git repo","text":"<p>NOTE: Because of a restriction in one of the code generators that we use, a contributor needs to have their local copy of the git repo in a directory whose pathname ends with the Go package name --- that is, ends with <code>/github.com/kubestellar/kubestellar</code>.</p>"},{"location":"direct/packaging/#derived-files","title":"Derived files","text":"<p>Some files in the kubestellar repo are derived from other files there. Contributors are responsible for invoking the commands to (re)derive the derived files as necessary.</p> <p>Some of these derived files are derived by standard generators from the Kubernetes milieu. A contributor can use the following command to make all of those, or use the individual <code>make</code> commands described in the following subsubsections to update particular subsets.</p> <pre><code>make all-generated\n</code></pre> <p>The following command, which we aspire to check in CI, checks whether all those derived files have been correctly derived. It must be invoked in a state where the <code>git status</code> is clean, or at least the dirty files are irrelevant; the current commit is what is checked. This command has side-effects on the filesystem like <code>make all-generated</code>.</p> <pre><code>hack/verify-codegen.sh\n</code></pre>"},{"location":"direct/packaging/#files-generated-by-controller-gen","title":"Files generated by controller-gen","text":"<ul> <li> <p><code>make manifests</code> generates the CustomeResourceDefinition files,   which exist in two places:   <code>config/crd/bases</code> and   <code>pkg/crd/files</code>.</p> </li> <li> <p><code>make generate</code> generates the deep copy code, which exists in   <code>zz_generated.deepcopy.go</code> next to the API source.</p> </li> </ul>"},{"location":"direct/packaging/#files-generated-by-code-generator","title":"Files generated by code-generator","text":"<p>The files in <code>pkg/generated</code> are generated by k/code-generator. This generation is done at development time by the command <code>make codegenclients</code>.</p>"},{"location":"direct/packaging/#kubestellar-controller-manager-container-image","title":"KubeStellar controller-manager container image","text":"<p>KubeStellar has one container image, for what is called the KubeStellar controller-manager. For each WDS, KubeStellar has a pod running that image. It installs the needed custom resource definition objects if they are not already present, and is a controller-manager hosting the per-WDS controllers (binding controller and status controller) from the kubestellar repo.</p> <p>The image repository is <code>ghcr.io/kubestellar/kubestellar/controller-manager</code>. The following notable versions exist.</p> image tag git commit SHA256 digest 0.20.0 a2bcaf75dc895dbef5f6cddcf203b0203423a08a fac5b208a1f691eb5548a7699494bed2ee6b7502e739a2db840f39bbc05b2fd4 0.21.0 dcb232dbcfeb2e90d02315c38e26ba1a380d2cc6 3f04a74698d0e59bfe941d6ab016ee438f6a6ea684c0f671c03db0f4d7ee5eee 0.21.1 13fcd6a1c10eb377b20e7b1a6c5162209f70a281 0e0873088a2a78e578cf97e486ba5348b8728cdf0a270c6741710a241f12af8e <p>The release process builds and publishes that container image.</p> <p><code>make ko-build-local</code> will make a local image for just the local platform. This is used in local testing.</p>"},{"location":"direct/packaging/#kubestellar-controller-manager-helm-chart","title":"KubeStellar controller-manager Helm Chart","text":"<p>There is a Helm chart that is designed to be instantiated in a KubeFlex hosting cluster, once per WDS. The focus of the chart is getting the KubeStellar controller-manager installed.</p> <p>The source for the Helm chart is in the <code>chart/</code> directory. <code>make chart</code> (re)derives it from local sources. This is not included in <code>make all-generated</code>.</p> <p>This chart creates (among other things) a <code>Deployment</code> object that runs a container from the KubeStellar controller-manager container image.</p> <p>The chart is published at the OCI repository <code>ghcr.io/kubestellar/kubestellar/kubestellar-operator-chart</code>. A GitHub Actions workflow (in <code>.github/workflows/goreleaser.yml</code>) specializes and publishes this chart as part of the release process.</p> <p>The following versions exist.</p> OCI tag = version in chart appVersion in chart referenced container tag 0.20.0 0.20.0 0.20.0"},{"location":"direct/packaging/#clusteradm-container-image","title":"clusteradm container image","text":"<p>The kubestellar GitHub repository has a script, <code>hack/build-clusteradm-image.sh</code>, that creates and publishes a container image holding the <code>clusteradm</code> command from OCM. The source of the container image is read from the latest release of github.com/open-cluster-management-io/clusteradm, unless a command line flag says to use a specific version. This script also pushes the built container image to quay.io/kubestellar/clusteradm using a tag that equals the ocm/clusteradm version that the image was built from.</p>"},{"location":"direct/packaging/#kubeflex-postcreatehooks","title":"KubeFlex PostCreateHooks","text":"<p>There are two <code>PostCreateHook</code> objects defined in the <code>config/postcreate-hooks/</code> directory.</p>"},{"location":"direct/packaging/#ocm-postcreatehook","title":"ocm PostCreateHook","text":"<p>The PostCreateHook defined in <code>ocm.yaml</code> gets used on an ITS and adds the hub side of OCM there, using the image <code>quay.io/kubestellar/clusteradm:0.7.2</code>. See above about the source of that. Currently this PostCreateHook is used in the E2E tests but this is a problem because of its fixed reference to container image previously built from sources in this same Git repository.</p>"},{"location":"direct/packaging/#kubestellar-postcreatehook","title":"kubestellar PostCreateHook","text":"<p>The PostCreateHook defined in <code>kubestellar.yaml</code> is intended to be used in the hosting cluster, once per WDS, and runs container image <code>quay.io/kubestellar/helm:v3.14.0</code> (which is built from the Helm source by a process that we need to document) to instantiate the chart from <code>oci://ghcr.io/kubestellar/kubestellar/controller-manager-chart</code>; the chart version appears as a literal in the PostCreateHook, currently \"0.20.0\". Currently the only reference to any copy of this PostCreateHook is from the examples doc, which references the copy in the Git commit tagged <code>v0.20.0</code>.</p>"},{"location":"direct/packaging/#amalgamated-graph","title":"Amalgamated graph","text":"<p>Currently only showing kubestellar and ocm-status-addon.</p> <p>TODO: finish this</p> flowchart LR     subgraph osa_repo[\"ocm-status-addon@GitHub\"]     osa_code[OSA source code]     osa_hc_src[OSA Helm chart source]     end     osa_ctr_image[OSA container image] --&gt; osa_code     osa_hc_repo[published OSA Helm Chart] --&gt; osa_hc_src     osa_hc_src -.-&gt; osa_ctr_image     osa_hc_repo -.-&gt; osa_ctr_image     subgraph cladm_repo[\"ocm/clusteradm@GitHub\"]     cladm_src[\"clusteradm source\"]     end     subgraph ks_repo[\"kubestellar@GitHub\"]     kcm_code[KCM source code]     kcm_hc_src[KCM Helm chart source]     ks_pch[kubestellar PostCreateHook]     ocm_pch[\"ocm PostCreateHook\"]     ks_scripts -.-&gt; ocm_pch     ks_scripts -.-&gt; ks_pch     end     osa_repo -.-&gt; ks_repo     cladm_image[\"ks/clusteradm image\"] --&gt; cladm_src     ocm_pch -.-&gt; cladm_image     kcm_ctr_image[KCM container image] --&gt; kcm_code     kcm_hc_repo[published KCM Helm Chart] --&gt; kcm_hc_src     kcm_hc_src -.-&gt; kcm_ctr_image     kcm_hc_repo -.-&gt; kcm_ctr_image     ks_pch  -.-&gt; kcm_hc_repo     ks_scripts -.-&gt; osa_hc_repo[published OSA Helm Chart]     ks_scripts -.-&gt; otp_something[\"published OTP ??\"]     ks_scripts -.-&gt; KubeFlex  <p>Every dotted line is a reference that must be versioned. How do we keep all those versions right?</p> <p>Normally a git tag is an immutable reference to an immutable git commit. Let's not violate that.</p> <p>Can/should we say that an OCI image (or whatever) tag equals the tag of the commit that said image (or whatever) was built from? While keeping <code>main</code> always a working system?</p>"},{"location":"direct/pre-reqs/","title":"KubeStellar prerequisites","text":"<p>The following prerequisites are required. You can use the check-pre-req script, to validate if all needed pre-requisites are installed. </p>"},{"location":"direct/pre-reqs/#for-using-kubestellar","title":"For Using KubeStellar","text":"<ul> <li> <p>kubeflex version 0.4.2 or higher     To install kubeflex go to https://github.com/kubestellar/kubeflex/blob/main/docs/users.md#installation. To upgrade from an existing installation, follow these instructions. At the end of the install make sure that the kubeflex CLI, kflex, is in your path.</p> </li> <li> <p>OCM CLI (clusteradm)     To install OCM CLI use:</p> <pre><code>curl -L https://raw.githubusercontent.com/open-cluster-management-io/clusteradm/main/install.sh | bash\n</code></pre> <p>Note that the default installation of clusteradm will install in /usr/local/bin which will require root access. If you prefer to avoid root, you can specify an alternative installation path using the INSTALL_DIR environment variable, as follows:</p> <pre><code>mkdir -p ocm\nexport INSTALL_DIR=\"$PWD/ocm\"\ncurl -L https://raw.githubusercontent.com/open-cluster-management-io/clusteradm/main/install.sh | bash\nexport PATH=$PWD/ocm:$PATH\n</code></pre> <p>At the end of the install make sure that the OCM CLI, clusteradm, is in your path.</p> </li> <li> <p>helm - to deploy the kubestellar and kubeflex charts</p> </li> <li>kubectl - to access the kubernetes clusters</li> <li>docker (or compatible docker engine that works with kind)</li> </ul>"},{"location":"direct/pre-reqs/#for-running-the-examples","title":"For running the examples","text":"<ul> <li>kind - to create a few small kubernetes clusters</li> <li>argocd - for the examples that use it</li> </ul>"},{"location":"direct/pre-reqs/#for-building-kubestellar","title":"For Building KubeStellar","text":"<ul> <li>go version 1.20 or higher - to build kubestellar</li> <li>make - to build kubestellar and create the kubestellar image</li> <li>ko - to create the kubestellar image</li> </ul>"},{"location":"direct/pre-reqs/#check-pre-requisites-for-kubestellar","title":"Check pre-requisites for KubeStellar","text":"<p>The check_pre_req script offers a convenient way to check for the pre-requisites eeded for KubeStellar deployment and use case scenarios.</p> <p>The script checks for a pre-requisite presence in the path, by using the <code>which</code> command, and it can optionally provide version and path information for pre-requisites that are present, or installation information for missing pre-requisites.</p> <p>We envision that this script could be useful for user-side debugging as well as for asserting the presence of pre-requisites in higher-level automation scripts.</p> <p>The script accepts a list of optional flags and arguments.</p> <p>Supported flags:</p> <ul> <li><code>-A|--assert</code>: exits with error code 2 upon finding the fist missing pre-requisite</li> <li><code>-L|--list</code>: prints a list of supported pre-requisites</li> <li><code>-V|--verbose</code>: displays version and path information for installed pre-requisites or installation information for missing pre-requisites</li> <li><code>-X</code>: enable <code>set -x</code> for debugging the script</li> </ul> <p>Supported arguments:</p> <p>The script accepts a list of specific pre-requisites to check, among the list of available ones:</p> <pre><code>$ check_pre_req.sh --list\nargo brew docker go helm jq kflex kind ko kubectl make ocm yq\n</code></pre> <p>For example, list of pre-requisites required by KubeStellar can be checked with the command below (add the <code>-V</code> flag to get the version of each program and a suggestions on how to install missing pre-requisites):</p> <pre><code>$ hack/check_pre_req.sh\nChecking pre-requisites for using KubeStellar:\n\u2714 Docker\n\u2714 kubectl\n\u2714 KubeFlex\n\u2714 OCM CLI\n\u2714 Helm\nChecking additional pre-requisites for running the examples:\n\u2714 Kind\nX ArgoCD CLI\nChecking pre-requisites for building KubeStellar:\n\u2714 GNU Make\n\u2714 Go\n\u2714 KO\n</code></pre> <p>In another example, a specific list of pre-requisites could be asserted by an higher-level script, while providing some installation information, with the command below (note that the script will terminate upon finding a missing pre-requisite):</p> <pre><code>$ check_pre_req.sh --assert --verbose helm argo docker kind\nChecking KubeStellar pre-requisites:\n\u2714 Helm\n  version: version.BuildInfo{Version:\"v3.14.0\", GitCommit:\"3fc9f4b2638e76f26739cd77c7017139be81d0ea\", GitTreeState:\"clean\", GoVersion:\"go1.21.5\"}\npath: /usr/sbin/helm\nX ArgoCD CLI\n  how to install: https://argo-cd.readthedocs.io/en/stable/cli_installation/\n</code></pre>"},{"location":"direct/release-notes/","title":"Release notes","text":"<p>The following sections list the known issues for each release. The issue list is not differential (i.e., compared to previous releases) but a full list representing the overall state of the specific release. </p>"},{"location":"direct/release-notes/#0212-and-its-release-candidates","title":"0.21.2 and its release candidates","text":"<p>The changes since 0.21.1 include efficiency improvements, reducing costs of running the kubestellar-controller-manager for a WDS that is an OpenShift cluster. There are also bug fixes and documentation improvements.</p>"},{"location":"direct/release-notes/#0211","title":"0.21.1","text":"<p>This release mainly updates the documentation exposed under kubestellar.io.</p>"},{"location":"direct/release-notes/#0210-and-its-release-candidates","title":"0.21.0 and its release candidates","text":""},{"location":"direct/release-notes/#major-changes-for-0210-and-its-release-candidates","title":"Major changes for 0.21.0 and its release candidates","text":"<ul> <li>This release introduces pluggable transport. Currently the only plugin is the OCM transport plugin.</li> </ul>"},{"location":"direct/release-notes/#bug-fixes-in-0210-and-its-release-candidates","title":"Bug fixes in 0.21.0 and its release candidates","text":"<ul> <li>dynamic changes to WECs are supported. Existing Bindings and ManifestWorks will be updated when new WECs are added/updated/delete or when labels are added/updated/deleted on existing WECs</li> <li>An update to a workload object that removes some BindingPolicies from the matching set is handled correctly.</li> <li>These changes that happen while a controller is down are handled correctly:</li> <li>If a workload object is deleted, or changed to remove some BindingPolicies from the matching set;</li> <li>A BindingPolicy update that removes workload objects or clusters from their respective matching sets.</li> </ul>"},{"location":"direct/release-notes/#remaining-limitations-in-0210-and-its-release-candidates","title":"Remaining limitations in 0.21.0 and its release candidates","text":"<ul> <li>Removing of WorkStatus objects (on the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSs shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> </ul>"},{"location":"direct/release-notes/#0200-and-its-release-candidates","title":"0.20.0 and its release candidates","text":"<ul> <li>Dynamic changes to WECs are not supported. Existing ManifestWorks will not be updated when new WECs are added or when labels are added/deleted on existing WECs</li> <li>Removing of WorkStatus objects (on the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSs shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> <li>An update to a workload object that removes some BindingPolicies from the matching set is not handled correctly.</li> <li>Some operations are not handled correctly while the controller is down:</li> <li>If a workload object is deleted, or changed to remove some BindingPolicies from the matching set, it will not be handled correctly.</li> <li>A BindingPolicy update that removes workload objects or clusters from their respective matching sets is not handled correctly.</li> </ul>"},{"location":"direct/release/","title":"Making KubeStellar Releases","text":"<p>This document defines how releases of the KubeStellar repository are made. This document is a work-in-progress. In particular, the dependency cycle between the <code>kubestellar</code> and <code>ocm-tansport-plugin</code> repos is not well documented and it is still not clear how to test and release in the presence of this cycle.</p> <p>See the associated packaging and delivery doc for some clues about the problem.</p>"},{"location":"direct/release/#goals-and-limitations","title":"Goals and limitations","text":"<p>The release process has the following goals.</p> <ul> <li>A release is identified using semantic versioning. This means that the associated semantics are followed, in terms of what sort of changes to the repo require what sort of changes to the release identifier.</li> <li>A user can pick up and use a given existing release without being perturbed by on-going contributor work. A release is an immutable thing.</li> <li>A release with a given semver identifier is built from a commit of this Git repository tagged with a tag whose name is \"v\" followed by the release identifier.</li> <li>There is a concept of a release branch, with a name following the pattern <code>release-$major.$minor</code>. This allows contributors to do maintenance work on older release branches. This also allows a user to track work along such a line.</li> <li>The contents of <code>main</code> always work. This includes passing CI tests. This includes documentation being accurate. We allow point-in-time specific documentation, such as a document that says \"Here is how to use release 1.2.3\" --- which would refer to a release made in the past. We do not require the documentation in <code>main</code> to document all releases.</li> <li>A git tag is immutable. Once associated with a given Git commit, that association is not changed later.</li> <li>We do not put self-references into Git. For example, making release <code>1.2.3</code> does not require changing any file in Git to have the string <code>1.2.3</code> in it.</li> </ul> <p>We have the following limitations.</p> <ul> <li>The only way to publish artifacts (broadly construed, not (necessarily) GitHub \"release artifacts\") is to make a release.</li> <li>The only way to test published artifacts is to make a release and test it.</li> <li>Thus, it is necessary to keep users clearly appraised of the quality (or status of evaluating the quality) of each release.</li> <li>Because of the lack of self references, most user instructions (e.g., examples) and tests do not have concrete release identifiers in them; instead, the user has to chose and supply the release identifier. There can also be documentation of a specific past release (e.g., the latest stable release) that uses the literal identifier for that past release.</li> <li>PAY ATTENTION TO THIS ONE: Because of the prohibition of self references, Git will not contain the exact bytes of our Helm chart definitions. Where a Helm chart states its own version or has a container image reference to an image built from the same release, the bytes in Git have a placeholder for that image's tag and the process of creating the published release artifacts fills in that placeholder. Think of this as being analogous to the linking done when building a binary executable file.</li> <li>The design below falls short of the goal of not putting self-references in files under Git control. One way is in the KubeFlex PostCreateHook that installs the kubestellar-controller-manager (KCM), where the version of the container image for the KCM appears. Another is in the examples document, which also holds references to its own release. Another is in the examples.md file, and another is in the <code>docs/content/direct/README.md</code> file.</li> </ul>"},{"location":"direct/release/#technology","title":"Technology","text":"<p>There is a GitHub workflow that creates the published artifacts for each Git tag whose name starts with \"v\". The rest of the tag name is required to be a semver release identifier. Note that this document does not (yet, anyway) specify how that GitHub workflow gets its job done. This workflow is confusingly named \"goreleaser\" and in a file named \"goreleaser.yml\" and has a job named \"goreleaser\" despite the fact that it does more than use goreleaser.</p> <p>For each tag <code>v$version</code> the following published artifacts will be created.</p> <ul> <li>The container image for the kubestellar-controller-manager (KCM), at <code>ghcr.io/kubestellar/kubestellar/controller-manager</code>. Image tag will be <code>$version</code>. This GitHub \"package\" will be connected to the ks/ks repo (this connection is something that an admin will do once, it will stick for all versions).</li> <li>The Helm chart (for installing the KCM for a WDS), at <code>ghcr.io/kubestellar/kubestellar/controller-manager-chart</code> with version <code>$version</code> and Helm \"appVersion\" <code>$version</code>. This GitHub \"package\" will also be connected to the ks/ks repo. The chart has a reference to container image for the KCM and that reference is <code>ghcr.io/kubestellar/kubestellar/controller-manager:$version</code>. In Git the chart has only placeholders in these places, not <code>$version</code>; the <code>$version</code> is inserted into a distinct copy by the GitHub workflow, which then publishes this specialized copy.</li> <li>Note that there is no automation (yet) concerning the KubeFlex PostCreateHook that installs the KCM.</li> </ul>"},{"location":"direct/release/#website","title":"Website","text":"<p>We use <code>mike</code> and <code>MkDocs</code> to derive and publish GitHub pages. See <code>docs/README.md</code> for details.</p> <p>The published GitHub pages are organized into \"releases\".  Each release in the GitHub pages corresponds to a git branch whose name begins with \"release-\" or is \"main\".</p> <p>Our documentation is, mostly, viewable in either of two ways. The source documents can be viewed directly through GitHub's web UI for files. The other way is through the website.</p>"},{"location":"direct/release/#testing-and-examples","title":"Testing and Examples","text":"<p>The unit tests (of which we have almost none right now), integration tests (of which we also have very few), and end-to-end (E2E) tests in this repository are run in the context of a local copy of this repository and test that version of this repository --- not using any published release artifacts. Additionally, some E2E tests have the option to test published artifacts instead of the local copy of this repo.</p> <p>The end-to-end tests include ones written in <code>bash</code>, and these are the only documentation telling a user how to use the present version of this repository. Again, these tests do not use any published artifacts from a release of this repo.</p> <p>We have another category of tests, release tests. These test a given release, using the published artifacts of that release. Currently all the release tests are a subset of the E2E tests --- those that can be told to test published artifacts. In particular, they can test the published artifacts reached through the kubestellar PostCreatHook, which contains an explicit reference to one particular release (as explained elsewhere in this document).</p> <p>We have GitHub workflows that exercise the E2E tests, normally on the copy of the repo that the workflow applies to. However, these workflows are parameterized and can be told to test the released artifacts instead.</p> <p>We also have a GitHub workflow, named \"Test latest release\" in <code>.github/workflows/test-latest-release.yml</code>, that invokes those E2E tests on the latest release. This workflow can be triggered manually, and is also configured to run after completion of the workflow (\"goreleaser\") that publishes release artifacts.</p> <p>We will maintain a document that lists releases that pass our quality bar. The latest of those is thus the latest stable release. This document is updated in <code>main</code> as quality evaluations come in.</p> <p>In docs/content/direct/README.md we maintain a statement of what is the latest stable release.</p> <p>We maintain an examples document that tells users how to exercise the release that the document appears in. This requires a self-reference that is updated as part of the release process.</p>"},{"location":"direct/release/#policy","title":"Policy","text":"<p>We aim for all regular releases to be working. In order to do that, we have to make test releases and test them. The widely recognized pattern for doing that is to make \"release candidates\" (i.e., releases for testing purposes) <code>1.2.3-rc0</code>, <code>1.2.3-rc1</code>, <code>1.2.3-rc2</code>, and so on, while trying to get to a quality release <code>1.2.3</code>. Once one of them is judged to be of passing quality, we make a release without the <code>-rc&lt;N&gt;</code> suffix. Due to the self-reference in the KCM PostCreateHook, this will involve making a new commit.</p> <p>Right after making a release we test it thoroughly.</p>"},{"location":"direct/release/#deliberately-feature-incomplete-releases","title":"Deliberately feature-incomplete releases","text":"<p>We plan a few deliberately feature-incomplete releases. They will be regular releases as far as the technology here is concerned. They will be announced only to selected users who acknowledge that they are getting something that is incomplete. In GitHub, these will be marked as \"pre-releases\". The status of these releases will be made clear in their documentation (which currently appears in the release notes.</p>"},{"location":"direct/release/#website_1","title":"Website","text":"<p>We aim to keep the documents viewable both through the website and GitHub's web UI for viewing files. We aim for all of the documentation to be reachable on the website and in the GitHub file UI starting from the repo's README.md.</p> <p>We create a release in the GitHub pages for every release. A patch release is a release. A test release is a release. Creating that GHP release is done by creating a git branch named <code>release-$version</code>.</p>"},{"location":"direct/release/#step-by-step","title":"Step-by-Step","text":"<p>Making a new release requires a contributor to do the following things. Here <code>$version</code> is the semver identifier for the release (e.g., <code>1.2.3-rc2</code>).</p> <ul> <li> <p>Edit the source for the KCM PCH (in <code>config/postcreate-hooks/kubestellar.yaml</code>) and update the tag in the reference to the KCM container image (it appears in the last object, a <code>Job</code>).</p> </li> <li> <p>Edit the examples document to update the self-references for the coming release.</p> </li> <li> <p>Until we have our first stable release, edit the README where it wishes it could cite a stable release but instead cites the latest release, to rever to the coming release.</p> </li> <li> <p>Edit the release notes.</p> </li> <li> <p>Make a new Git commit with those changes and get it into the right branch in the shared repo (through the regular PR process if not authorized to cheat).</p> </li> <li> <p>Apply the Git tag <code>v$version</code> to that new commit in the shared repo.</p> </li> <li> <p>After that, the \"goreleaser\" GitHub workflow then creates and publishes the artifacts for that release (as discussed above) and then the \"Test latest release\" workflow will run the E2E tests using those artifacts.</p> </li> <li> <p>After the release artifacts have been published, create and push to the shared repo a branch named <code>release-$version</code>. This will also trigger the workflow that tests the latest release. Every push to a branch with such a name triggers that workflow, in case there has been a change in an E2E test for that release.</p> </li> <li> <p>Do additional testing: more scenarios, more platforms.</p> </li> </ul>"},{"location":"direct/release/#future-process-development","title":"Future Process Development","text":"<p>We intend to get rid of the self-reference in the KCM PCH, as follows. Define a Helm chart for installing the PCH. Update the release workflow to specialize that Helm chart, similarly to the specialization done for the KCM Helm chart.</p>"},{"location":"direct/release/#open-questions","title":"Open questions","text":"<p>What to do about the dependency cycle between ks/ks and ks/ocm-transport-plugin?</p> <p>Exactly when does a new release branch diverge from <code>main</code>? What about cherry-picking between <code>main</code> and the latest (or also earlier?) release branch?</p> <p>What about the clusteradm container image?</p>"},{"location":"direct/thirdparties/","title":"Third Parties Integrations","text":""},{"location":"direct/thirdparties/#install-and-configure-argocd","title":"Install and configure ArgoCD","text":"<p>Install ArgoCD on kind-kubeflex:</p> <pre><code>kubectl --context kind-kubeflex create namespace argocd\nkubectl --context kind-kubeflex apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre> <p>Install CLI:</p> <p>on MacOS:</p> <pre><code>brew install argocd\n</code></pre> <p>on Linux:</p> <pre><code>curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\nrm argocd-linux-amd64\n</code></pre> <p>Check the ArgoCD releases page for the obtaining the latest  stable release for other architectures and operating systems.</p> <p>Configure Argo to work with the ingress installed in the hosting cluster:</p> <pre><code>kubectl --context kind-kubeflex apply -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argocd-server-ingress\n  namespace: argocd\n  annotations:\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: argocd.localtest.me\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: argocd-server\n            port:\n              name: https\nEOF\n</code></pre> <p>Open a browser to ArgoCD console:</p> <pre><code>open https://argocd.localtest.me:9443\n</code></pre> <p>Note: if you are working on a VM via SSH, just take the IP of the VM (VM_IP) and add the line ' argocd.localtest.me' to your '/etc/hosts' file, replacing  with the actual IP of your desktop. <p>Get the password for Argo with:</p> <pre><code>kubectl config use-context kind-kubeflex\nargocd admin initial-password -n argocd\n</code></pre> <p>Login into the ArgoCD console with <code>admin</code> and the password just retrieved. Type the following on a shell terminal in your desktop (or just enter the address https://argocd.localtest.me:9443 on your browser):</p> <pre><code>open https://argocd.localtest.me:9443\n</code></pre> <p>Also, login with the argocd CLI with the same credentials.</p> <pre><code>argocd login --insecure argocd.localtest.me:9443\n</code></pre> <p>Add the <code>wds1</code> space as cluster to ArgoCD:</p> <pre><code>CONTEXT=wds1\nkubectl config view --minify --context=${CONTEXT} --flatten &gt; /tmp/${CONTEXT}.kubeconfig\nkubectl config --kubeconfig=/tmp/${CONTEXT}.kubeconfig set-cluster ${CONTEXT}-cluster --server=https://${CONTEXT}.${CONTEXT}-system 2&gt;/dev/null\nkubectl config use-context kind-kubeflex\nARGO_SERVER_POD=$(kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server -o 'jsonpath={.items[0].metadata.name}')\nkubectl cp /tmp/${CONTEXT}.kubeconfig -n argocd ${ARGO_SERVER_POD}:/tmp\nPASSWORD=$(argocd admin initial-password -n argocd | cut -d \" \" -f 1)\nkubectl exec -it -n argocd $ARGO_SERVER_POD -- argocd login argocd-server.argocd --username admin --password $PASSWORD --insecure\nkubectl exec -it -n argocd $ARGO_SERVER_POD -- argocd cluster add ${CONTEXT} --kubeconfig /tmp/${CONTEXT}.kubeconfig -y\n</code></pre> <p>Configure Argo to label resources with the \"argocd.argoproj.io/instance\" label:</p> <pre><code>kubectl --context kind-kubeflex patch cm -n argocd argocd-cm -p '{\"data\": {\"application.instanceLabelKey\": \"argocd.argoproj.io/instance\"}}'\n</code></pre>"},{"location":"direct/images/png-files-readme/","title":"How to edit these pictures","text":"<p>The pictures have been created with draw.io, and have been saved in the editable png format. You can use draw.io to modify and save back these pictures using the png editable format.</p>"}]}