{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#multicluster-configuration-management-for-edge-multi-cloud-and-hybrid-cloud","title":"Multicluster Configuration Management for Edge, Multi-Cloud, and Hybrid Cloud","text":""},{"location":"#overview","title":"Overview","text":"<p>KubeStellar is an opensource project focused on concerns arising multicluster configuration management for edge, multi-cloud, and hybrid cloud use cases:</p> <ul> <li>Hierarchy, infrastructure &amp; platform, roles &amp; responsibilities, integration architecture, security issues</li> <li>Runtime in[ter]dependence: An edge location may need to operate independently of the center and other edge locations\u200b</li> <li>Non-namespaced objects: need general support</li> <li>Cardinality of destinations: A source object may propagate to many thousands of destinations. </li> </ul>"},{"location":"#goals","title":"Goals","text":"<ul> <li>Collaboratively design a component set similar to those found in the current kcp TMC implementation (dedicated Workspace type, scheduler, syncer-like mechanism, edge placement object definition, status collection strategy, etc.)</li> <li>Specify a multi-phased proof-of-concept inclusive of component architecture, interfaces, and example workloads</li> <li>Validate phases of proof-of-concept with kcp, Kube SIG-Multicluster, and CNCF community members interested in Edge</li> </ul>"},{"location":"#areas-of-exploration","title":"Areas of exploration","text":"<ul> <li>Desired placement expression\u200b: Need a way for one center object to express large number of desired copies\u200b</li> <li>Scheduling/syncing interface\u200b: Need something that scales to large number of destinations\u200b</li> <li>Rollout control\u200b: Client needs programmatic control of rollout, possibly including domain-specific logic\u200b</li> <li>Customization: Need a way for one pattern in the center to express how to customize for all the desired destinations\u200b</li> <li>Status from many destinations\u200b: Center clients may need a way to access status from individual edge copies</li> <li>Status summarization\u200b: Client needs a way to specify how statuses from edge copies are processed/reduced along the way from edge to center\u200b.</li> </ul>"},{"location":"#quickstart","title":"QuickStart","text":"<p>Checkout our QuickStart Guide</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We \u2764\ufe0f our contributors! If you're interested in helping us out, please head over to our Contributing guide.</p>"},{"location":"#getting-in-touch","title":"Getting in touch","text":"<p>There are several ways to communicate with us:</p> <ul> <li>The <code>#kcp-dev</code> channel in the Kubernetes Slack workspace</li> <li>Our mailing lists:<ul> <li>kubestellar-dev for development discussions</li> <li>kubestellar-users for discussions among users and potential users</li> </ul> </li> <li>Subscribe to the community calendar for community meetings and events<ul> <li>The kubestellar-dev mailing list is subscribed to this calendar</li> </ul> </li> <li>See recordings of past KubeStellar community meetings on YouTube</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Browse the shared Google Drive to share design docs, notes, etc.<ul> <li>Members of the kubestellar-dev mailing list can view this drive</li> </ul> </li> <li>Read our documentation</li> <li>Follow us on:</li> <li>LinkedIn - #kubestellar</li> <li>Medium - kubestellar.medium.com</li> </ul>"},{"location":"#contributors","title":"\u2764\ufe0f Contributors","text":"<p>Thanks go to these wonderful people:</p> Jun Duan\ud83d\udc40 Braulio Dumba\ud83d\udc40 Mike Spreitzer\ud83d\udc40 Paolo Dettori\ud83d\udc40 Andy Anderson\ud83d\udc40 Franco Stellari\ud83d\udc40 Ezra Silvera\ud83d\udc40 Bob Filepp\ud83d\udc40 Alexei Karve\ud83d\udc40 Maria Camila Ruiz Cardenas\ud83d\udc40 Aleksander Slominski\ud83d\udc40"},{"location":"Coding%20Milestones/PoC2023q1/coding-milestone-invite-q1/","title":"Invitation","text":"<p>Dear Contributors,</p> <p>We are excited to invite you to join the first KubeStellar opensource community coding sprint. We will be focus on several key projects that are critical to the development of state-based edge solutions. Our collective work will be showcased to the opensource community on Thursday, April 27th.</p> <p>This coding sprint will provide a great opportunity for you to showcase your skills, learn new techniques, and collaborate with other experienced engineers in the KubeStellar community. We believe that your contributions will be invaluable in helping us achieve our goals and making a lasting impact in the field of state-based edge technology.</p> <p>The coding sprint will be dedicated to completing the following workload management elements:</p> <ul> <li>Implementing an edge scheduler and placement translator, including customization options,</li> <li>Incorporating existing customization API into the KubeStellar repo,</li> <li>Investigating implementation of a status summarizer, starting with basic implicit status, and later adding programmed summarization,</li> <li>Updating summarization API and integrating it into the KubeStellar repo,</li> <li>Defining the API for identifying associated objects and its interaction with summarization, and implementing these,</li> <li>Streamlining the creation of workload management workspaces,</li> <li>Examining the use of Postgresql through Kine instead of etcd for scalability,</li> <li>Revising the milestone outline with regards to defining bootstrapping and support for cluster-scoped resources.</li> </ul> <p>In addition to workload management, we will also be working on inventory management for the demo, as well as designing various demo scenarios, including a baseline demo with kubectl, demos with ArgoCD, FluxCD, and the European Space Agency (ESA). To support the engineers and demonstrations we will also need to automate the process of creating infrastructure, deploying demo pieces and instrumentation, bootstrapping, running scenarios, and collecting data.</p> <p>If you are interested in joining us for this exciting coding sprint, please check out our 'good first issue' list, or slack me @Andy Anderson so I can connect you with others in your area of interest.  There is a place for every skillset to contribute. Not quite sure?  You can join our bi-weekly community meetings to watch our progress.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/","title":"Commands","text":"<p>This PoC includes two sorts of commands for users to use.  Most are executables delivered in the <code>bin</code> directory.  The other sort of command for users is a <code>bash</code> script that is designed to be fetched from github and fed directly into <code>bash</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#executables","title":"Executables","text":"<p>The command lines exhibited below presume that you have added the <code>bin</code> directory to your <code>$PATH</code>.  Alternatively: these executables can be invoked directly using any pathname (not in your <code>$PATH</code>).</p> <p>NOTE: all of the kubectl plugin usages described here certainly or potentially change the setting of which kcp workspace is \"current\" in your chosen kubeconfig file; for this reason, they are not suitable for executing concurrently with anything that depends on that setting in that file.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#platform-control","title":"Platform control","text":"<p>The <code>kubestellar</code> command has three subcommands, one to finish setup and two for process control.</p> <p>The usage synopsis is as follows.</p> <pre><code>kubestellar [flags] subcommand [flags]\n</code></pre> <p>This command accepts the following command line flags, which can appear before and/or after the subcommand.  The <code>--log-folder</code> flag is only used in the <code>start</code> subcommand.</p> <ul> <li><code>-V</code> or <code>--verbose</code>: calls for more verbose output.  This is a   binary choice, not a matter of degree.</li> <li><code>-X</code>: turns on echoing of script lines</li> <li><code>--log-folder $pathname</code>: says where to put the logs from the   controllers.  Will be <code>mkdir -p</code> if absent.  Defaults to   <code>${PWD}/kubestellar-logs</code>.</li> <li><code>-h</code> or <code>--help</code>: print a brief usage message and terminate.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-init","title":"Kubestellar init","text":"<p>This subcommand is used after installation to finish setup and does two things.  One is to ensure that the edge service provider workspace (ESPW) exists and has the required contents.  The other is to ensure that the <code>root:compute</code> workspace has been extended with the RBAC objects that enable the syncer to propagate reported state for downsynced objects defined by the APIExport from that workspace of a subset of the Kubernetes API for managing containerized workloads.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-start","title":"KubeStellar start","text":"<p>This subcommand is used after installation or process stops.</p> <p>This subcommand stops any running kubestellar controllers and then starts them all.  It also does the same thing as <code>kubestellar init</code>.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-stop","title":"KubeStellar stop","text":"<p>This subcommand undoes the primary function of <code>kubestellar start</code>, stopping any running KubeStellar controllers.  It does not tear down the ESPW.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-release","title":"KubeStellar-release","text":"<p>This command just echoes the semantic version of the release used.  This command is only available in archives built for a release.  Following is an example usage.</p> <p><pre><code>kubestellar-release\n</code></pre> <pre><code>v0.2.3-preview\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#kubestellar-version","title":"Kubestellar-version","text":"<p>This executable prints information about itself captured at build time.  If built by <code>make</code> then this is information conveyed by the Makefile; otherwise it is the Kubernetes defaults.</p> <p>It will either print one requested property or a JSON object containing many.</p> <p><pre><code>kubestellar-version help\n</code></pre> <pre><code>Invalid component requested: \"help\"\nUsage: kubestellar-version [buildDate|gitCommit|gitTreeState|platform]\n</code></pre></p> <p><pre><code>kubestellar-version buildDate\n</code></pre> <pre><code>2023-05-19T02:54:01Z\n</code></pre></p> <p><pre><code>kubestellar-version gitCommit\n</code></pre> <pre><code>1747254b\n</code></pre></p> <p><pre><code>kubestellar-version          </code></pre> <pre><code>{\"major\":\"1\",\"minor\":\"24\",\"gitVersion\":\"v1.24.3+kcp-v0.2.1-20-g1747254b880cb7\",\"gitCommit\":\"1747254b\",\"gitTreeState\":\"dirty\",\"buildDate\":\"2023-05-19T02:54:01Z\",\"goVersion\":\"go1.19.9\",\"compiler\":\"gc\",\"platform\":\"darwin/amd64\"}\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#creating-synctargetlocation-pairs","title":"Creating SyncTarget/Location pairs","text":"<p>In this PoC, the interface between infrastructure and workload management is inventory API objects.  Specifically, for each edge cluster there is a unique pair of SyncTarget and Location objects in a so-called inventory management workspace.  The following command helps with making that pair of objects.</p> <p>The usage synopsis is as follows.</p> <pre><code>kubectl kubestellar ensure location flag... objname labelname=labelvalue...\n</code></pre> <p>Here <code>objname</code> is the name for the SyncTarget object and also the name for the Location object.  This command ensures that these objects exist and have at least the given labels.</p> <p>The flags can also appear anywhere later on the command line.</p> <p>The acceptable flags include all those of <code>kubectl</code> except for <code>--context</code>.  This command also accepts the following flags.</p> <ul> <li><code>--imw workspace_path</code>: specifies which workspace to use as the   inventory management workspace.  The default value is the current   workspace.</li> </ul> <p>The current workspaces does not matter if the IMW is explicitly specified.  Upon completion, the current workspace will be your chosen IMW.</p> <p>This command does not depend on the action of any of the  KubeStellar controllers.</p> <p>An example usage follows.</p> <p><pre><code>kubectl kubestellar ensure location --imw root:imw-1 demo1 foo=bar the-word=the-bird\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nsynctarget.workload.kcp.io/demo1 created\nlocation.scheduling.kcp.io/demo1 created\nsynctarget.workload.kcp.io/demo1 labeled\nlocation.scheduling.kcp.io/demo1 labeled\nsynctarget.workload.kcp.io/demo1 labeled\nlocation.scheduling.kcp.io/demo1 labeled\n</code></pre></p> <p>The above example shows using this script to create a SyncTarget and a Location named <code>demo1</code> with labels <code>foo=bar</code> and <code>the-word=the-bird</code>. This was equivalent to the following commands.</p> <pre><code>kubectl ws root:imw-1\nkubectl create -f -&lt;&lt;EOF\napiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\n  name: demo1\n  labels:\n    id: demo1\n    foo: bar\n    the-word: the-bird\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\n  name: demo1\n  labels:\n    foo: bar\n    the-word: the-bird\nspec:\n  resource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\n  instanceSelector:\n    matchLabels: {\"id\":\"demo1\"}\nEOF\n</code></pre> <p>This command operates in idempotent style, making whatever changes (if any) are needed to move from the current state to the desired state. Current limitation: it does not cast a skeptical eye on the spec of a pre-existing Location.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#removing-synctargetlocation-pairs","title":"Removing SyncTarget/Location pairs","text":"<p>The following script undoes whatever remains from a corresponding usage of <code>kubectl kubestellar ensure location</code>.  It has all the same command line syntax and semantics except that the <code>labelname=labelvalue</code> pairs do not appear.</p> <p>This command does not depend on the action of any of the KubeStellar controllers.</p> <p>The following session demonstrates usage, including idempotency.</p> <p><pre><code>kubectl ws root:imw-1\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\n</code></pre></p> <p><pre><code>kubectl kubestellar remove location demo1\n</code></pre> <pre><code>synctarget.workload.kcp.io \"demo1\" deleted\nlocation.scheduling.kcp.io \"demo1\" deleted\n</code></pre></p> <pre><code>kubectl kubestellar remove location demo1\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#syncer-preparation-and-installation","title":"Syncer preparation and installation","text":"<p>The syncer runs in each edge cluster and also talks to the corresponding mailbox workspace.  In order for it to be able to do that, there is some work to do in the mailbox workspace to create a ServiceAccount for the syncer to authenticate as and create RBAC objects to give the syncer the privileges that it needs.  The following script does those things and also outputs YAML to be used to install the syncer in the edge cluster.</p> <p>The usage synopsis is as follows.</p> <pre><code>kubectl kubestellar prep-for-syncer flag... synctarget_name\n</code></pre> <p>Here <code>synctarget_name</code> is the name of the <code>SyncTarget</code> object, in the relevant IMW, corresponding to the relevant edge cluster.</p> <p>The flags can also appear anywhere later on the command line.</p> <p>The acceptable flags include all those of <code>kubectl</code> except for <code>--context</code>.  This command also accepts the following flags.</p> <ul> <li><code>--imw workspace_path</code>: specifies which workspace holds the relevant   SyncTarget object.  The default value is the current workspace.</li> <li><code>--espw workspace_path</code>: specifies where to find the edge service   provider workspace.  The default is the standard location,   <code>root:espw</code>.</li> <li><code>--syncer-image image_ref</code>: specifies the container image that runs   the syncer.  The default is <code>quay.io/kubestellar/syncer:v0.2.1</code>.</li> <li><code>-o output_pathname</code>: specifies where to write the YAML definitions   of the API objects to create in the edge cluster in order to deploy   the syncer there.  The default is <code>synctarget_name +   \"-syncer.yaml\"</code>.</li> </ul> <p>The current workspaces does not matter if the IMW is explicitly specified.  Upon completion, the current workspace will be what it was when the command started.</p> <p>This command will only succeed if the mailbox controller has created and conditioned the mailbox workspace for the given SyncTarget.  This command will wait for 10 to 70 seconds for that to happen.</p> <p>An example usage follows.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw-1 demo1\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\"\nCurrent workspace is \"root:espw:4yqm57kx0m6mn76c-mb-406c54d1-64ce-4fdc-99b3-cef9c4fc5010\" (type root:universal).\nCreating service account \"kubestellar-syncer-demo1-28at01r3\"\nCreating cluster role \"kubestellar-syncer-demo1-28at01r3\" to give service account \"kubestellar-syncer-demo1-28at01r3\"\n1. write and sync access to the synctarget \"kubestellar-syncer-demo1-28at01r3\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-demo1-28at01r3\" to bind service account \"kubestellar-syncer-demo1-28at01r3\" to cluster role \"kubestellar-syncer-demo1-28at01r3\".\n\nWrote physical cluster manifest to demo1-syncer.yaml for namespace \"kubestellar-syncer-demo1-28at01r3\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"demo1-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-demo1-28at01r3\" kubestellar-syncer-demo1-28at01r3\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Once that script has run, the YAML for the objects to create in the edge cluster is in your chosen output file.  The default for the output file is the name of the SyncTarget object with \"-syncer.yaml\" appended.</p> <p>Create those objects with a command like the following; adjust as needed to configure <code>kubectl</code> to modify the edge cluster and read your chosen output file.</p> <pre><code>KUBECONFIG=$demo1_kubeconfig kubectl apply -f demo1-syncer.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#edge-cluster-on-boarding","title":"Edge cluster on-boarding","text":"<p>The following command is a combination of <code>kubectl kubestellar ensure-location</code> and <code>kubectl kubestellar prep-for-syncer</code>, and takes the union of their command line flags and arguments.  Upon completion, the kcp current workspace will be what it was at the start.</p> <p>An example usage follows.</p> <p><pre><code>kubectl kubestellar prep-for-cluster --imw root:imw-1 demo2 key1=val1\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nsynctarget.workload.kcp.io/demo2 created\nlocation.scheduling.kcp.io/demo2 created\nsynctarget.workload.kcp.io/demo2 labeled\nlocation.scheduling.kcp.io/demo2 labeled\nCurrent workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1cpf1cd4ydy13vo1-mb-3c354acd-ed86-45bb-a60d-cee8e59973f7\" (type root:universal).\nCreating service account \"kubestellar-syncer-demo2-15nq4e94\"\nCreating cluster role \"kubestellar-syncer-demo2-15nq4e94\" to give service account \"kubestellar-syncer-demo2-15nq4e94\"\n1. write and sync access to the synctarget \"kubestellar-syncer-demo2-15nq4e94\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-demo2-15nq4e94\" to bind service account \"kubestellar-syncer-demo2-15nq4e94\" to cluster role \"kubestellar-syncer-demo2-15nq4e94\".\n\nWrote physical cluster manifest to demo2-syncer.yaml for namespace \"kubestellar-syncer-demo2-15nq4e94\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"demo2-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-demo2-15nq4e94\" kubestellar-syncer-demo2-15nq4e94\n\nto verify the syncer pod is running.\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#creating-a-workload-management-workspace","title":"Creating a Workload Management Workspace","text":"<p>Such a workspace needs not only to be created but also populated with an <code>APIBinding</code> to the edge API and, if desired, an <code>APIBinding</code> to the Kubernetes API for management of containerized workloads.</p> <p>NOTE: currently, only a subset of the Kubernetes containerized workload management API is supported.  In particular, only the following object kinds are supported: <code>Deployment</code>, <code>Pod</code>, <code>Service</code>, <code>Ingress</code>.  To be clear: this is in addition to the generic object kinds that are supported; illustrative examples include RBAC objects, <code>CustomResourceDefinition</code>, and <code>ConfigMap</code>.  For a full description, see the categorization in the design.</p> <p>The usage synopsis for this command is as follows.</p> <pre><code>kubectl ws parent_pathname; kubectl kubestellar ensure wmw flag... wm_workspace_name\n</code></pre> <p>Here <code>parent_pathname</code> is the workspace pathname of the parent of the WMW, and <code>wm_workspace_name</code> is the name (not pathname, just a bare one-segment name) of the WMW to ensure.  Thus, the pathname of the WMW will be <code>parent_pathname:wm_workspace_name</code>.</p> <p>Upon completion, the WMW will be the current workspace.</p> <p>The flags can also appear anywhere later on the command line.</p> <p>The acceptable flags include all those of <code>kubectl</code> except for <code>--context</code>.  This command also accepts the following flags.</p> <ul> <li><code>--with-kube boolean</code>: specifies whether or not the WMW should   include an APIBinding to the Kubernetes API for management of   containerized workloads.</li> </ul> <p>This script works in idempotent style, doing whatever work remains to be done.</p> <p>The following session shows some example usages, including demonstration of idempotency and changing whether the kube APIBinding is included.</p> <p><pre><code>kubectl ws .\n</code></pre> <pre><code>Current workspace is \"root:my-org\".\n</code></pre></p> <p><pre><code>kubectl kubestellar ensure wmw example-wmw\n</code></pre> <pre><code>Current workspace is \"root\".\nCurrent workspace is \"root:my-org\".\nWorkspace \"example-wmw\" (type root:universal) created. Waiting for it to be ready...\nWorkspace \"example-wmw\" (type root:universal) is ready to use.\nCurrent workspace is \"root:my-org:example-wmw\" (type root:universal).\napibinding.apis.kcp.io/bind-espw created\napibinding.apis.kcp.io/bind-kube created\n</code></pre></p> <p><pre><code>kubectl ws ..\n</code></pre> <pre><code>Current workspace is \"root:my-org\".\n</code></pre></p> <p><pre><code>kubectl kubestellar ensure wmw example-wmw\n</code></pre> <pre><code>Current workspace is \"root\".\nCurrent workspace is \"root:my-org\".\nCurrent workspace is \"root:my-org:example-wmw\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl ws ..\n</code></pre> <pre><code>Current workspace is \"root:my-org\".\n</code></pre></p> <p><pre><code>kubectl kubestellar ensure wmw example-wmw --with-kube false\n</code></pre> <pre><code>Current workspace is \"root\".\nCurrent workspace is \"root:my-org\".\nCurrent workspace is \"root:my-org:example-wmw\" (type root:universal).\napibinding.apis.kcp.io \"bind-kube\" deleted </code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#removing-a-workload-management-workspace","title":"Removing a Workload Management Workspace","text":"<p>Deleting a WMW can be done by simply deleting its <code>Workspace</code> object from the parent.</p> <p><pre><code>kubectl ws .\n</code></pre> <pre><code>Current workspace is \"root:my-org:example-wmw\".\n</code></pre></p> <p><pre><code>kubectl ws ..\n</code></pre> <pre><code>Current workspace is \"root:my-org\".\n</code></pre></p> <p><pre><code>kubectl delete Workspace example-wmw\n</code></pre> <pre><code>workspace.tenancy.kcp.io \"example-wmw\" deleted </code></pre></p> <p>Alternatively, you can use the following command line whose design completes the square here.  Invoke it when the current workspace is the parent of the workload management workspace to delete.</p> <p><pre><code>kubectl kubestellar remove wmw -h\n</code></pre> <pre><code>Usage: kubectl ws parent; kubectl kubestellar remove wmw kubectl_flag... wm_workspace_name\n</code></pre></p> <p><pre><code>kubectl ws root:my-org\n</code></pre> <pre><code>Current workspace is \"root:my-org\".\n</code></pre></p> <p><pre><code>kubectl kubestellar remove wmw demo1\n</code></pre> <pre><code>workspace.tenancy.kcp.io \"demo1\" deleted\n</code></pre></p> <p><pre><code>kubectl ws .\n</code></pre> <pre><code>Current workspace is \"root:my-org\".\n</code></pre></p> <pre><code>kubectl kubestellar remove wmw demo1\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#web-to-bash","title":"Web-to-bash","text":""},{"location":"Coding%20Milestones/PoC2023q1/commands/#quick-setup","title":"Quick Setup","text":"<p>This is a combination of some installation and setup steps, for use in the QuickStart.</p> <p>The script can be read directly from https://raw.githubusercontent.com/kcp-dev/edge-mc/main/bootstrap/bootstrap-kubestellar.sh and does the following things.</p> <ol> <li>Downloads and installs kcp if it is not already evident on <code>$PATH</code>    (using the script below.</li> <li>Starts a kcp server if one is not already running.</li> <li>Downloads and installs kubestellar if it is not already evident on    <code>$PATH</code> (using the script below.</li> <li><code>kubestellar start</code> if the KubeStellar controllers are not already    running or the ESPW does not (yet) exist.</li> </ol> <p>This script accepts the following command line flags; all are optional.</p> <ul> <li><code>--kubestellar-version $version</code>: specifies the release of   KubeStellar to use.  When using a specific version, include the   leading \"v\".  The default is the latest regular release, and the   value \"latest\" means the same thing.</li> <li><code>--kcp-version $version</code>: specifies the kcp release to use.  The   default is the one that works with the chosen release of   KubeStellar.</li> <li><code>--os $OS</code>: specifies the operating system to use in selecting the   executables to download and install.  Choices are <code>linux</code> and   <code>darwin</code>.  Auto-detected if omitted.</li> <li><code>--arch $IAS</code>: specifies the instruction set architecture to use in   selecting the executables to download and install.  Choices are   <code>amd64</code> and <code>arm64</code>.  Auto-detected if omitted.</li> <li><code>--bind-address $IPADDR</code>: directs that the kcp server (a) write that   address for itself in the kubeconfig file that it constructs and (b)   listens only at that address.  The default is to pick one of the   host's non-loopback addresses to write into the kubeconfig file and   not bind a listening address.</li> <li><code>--ensure-folder $install_parent_dir</code>: specifies the parent folder   for downloads.  Will be <code>mkdir -p</code>.  The default is the current   working directory.  The download of kcp, if any, will go in   <code>$install_parent_dir/kcp</code>.  The download of KubeStellar will go in   <code>$install_parent_dir/kubestellar</code>.</li> <li><code>-V</code> or <code>--verbose</code>: increases the verbosity of output.  This is a   binary thing, not a matter of degree.</li> <li><code>-X</code>: makes the script <code>set -x</code> internally, for debugging.</li> <li><code>-h</code> or <code>--help</code>: print brief usage message and exit.</li> </ul> <p>Here \"install\" means only to (a) unpack the distribution archives into the relevant places under <code>$install_parent_dir</code> and (b) enhance the <code>PATH</code>, and <code>KUBECONFIG</code> in the case of kcp, environment variables in the shell running the script.  Of course, if you run the script in a sub-shell then those environment effects terminate with that sub-shell; this script also prints out messages showing how to update the environment in another shell.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#install-kcp-and-its-kubectl-plugins","title":"Install kcp and its kubectl plugins","text":"<p>This script is directly available at https://github.com/kcp-dev/edge-mc/blob/main/bootstrap/install-kubestellar.sh and does the following things.</p> <ul> <li>Fetch and install the <code>kcp</code> server executable.</li> <li>Fetch and install the kubectl plugins of kcp.</li> </ul> <p>This script accepts the following command line flags; all are optional.</p> <ul> <li><code>--version $version</code>: specifies the kcp release to use.  The default   is the latest.</li> <li><code>--OS $OS</code>: specifies the operating system to use in selecting the   executables to fetch and install.  Choices are <code>darwin</code> and <code>linux</code>.   Auto-detected if omitted.</li> <li><code>--arch $ARCH</code>: specifies the instruction set architecture to use in   selecting the executables to fetch and install.  Choices are <code>arm64</code>   and <code>amd64</code>.  Auto-detected if omitted.</li> <li><code>--ensure-folder $install_parent_dir</code>: specifies where to install   to.  This will be <code>mkdir -p</code>.  The default is <code>./kcp</code>.</li> <li><code>-V</code> or <code>--verbose</code>: increases the verbosity of output.  This is a   binary thing, not a matter of degree.</li> <li><code>-X</code>: makes the script <code>set -x</code> internally, for debugging.</li> <li><code>-h</code> or <code>--help</code>: print brief usage message and exit.</li> </ul> <p>Here install means only to unpack the downloaded archives, creating <code>$install_parent_dir/bin</code>.  If <code>$install_parent_dir/bin</code> is not already on your <code>$PATH</code> then this script will print out a message telling you to add it.</p>"},{"location":"Coding%20Milestones/PoC2023q1/commands/#install-kubestellar","title":"Install KubeStellar","text":"<p>This script is directly available at https://github.com/kcp-dev/edge-mc/blob/main/bootstrap/install-kubestellar.sh and will download and install KubeStellar.</p> <p>This script accepts the following command line arguments; all are optional.</p> <ul> <li><code>--version $version</code>: specifies the release of KubeStellar to use.   Defaults to the latest regular release.</li> <li><code>--OS $OS</code>: specifies the operating system to use in selecting the   executables to fetch and install.  Choices are <code>darwin</code> and <code>linux</code>.   Auto-detected if omitted.</li> <li><code>--arch $ARCH</code>: specifies the instruction set architecture to use in   selecting the executables to fetch and install.  Choices are <code>arm64</code>   and <code>amd64</code>.  Auto-detected if omitted.</li> <li><code>--ensure-folder $install_parent_dir</code>: specifies where to install   to.  This will be <code>mkdir -p</code>.  The default is <code>./kubestellar</code>.</li> <li><code>-V</code> or <code>--verbose</code>: increases the verbosity of output.  This is a   binary thing, not a matter of degree.</li> <li><code>-X</code>: makes the script <code>set -x</code> internally, for debugging.</li> <li><code>-h</code> or <code>--help</code>: print brief usage message and exit.</li> </ul> <p>Here install means only to unpack the downloaded archive, creating <code>$install_parent_dir/bin</code>.  If <code>$install_parent_dir/bin</code> is not already on your <code>$PATH</code> then this script will print out a message telling you to add it.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/","title":"Extended Example","text":"<p>Required Packages:</p> MacUbuntuRHELWSL <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19<pre><code>curl -L \"https://go.dev/dl/go1.19.5.linux-$(dpkg --print-architecture).tar.gz\" -o go.tar.gz\ntar -C /usr/local -xzf go.tar.gz\nrm go.tar.gz\necho 'export PATH=$PATH:/usr/local/go/bin' &gt;&gt; /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64 &amp;&amp; chmod +x ./kind &amp;&amp; mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\nchoco install curl -y\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -n -b main https://github.com/kcp-dev/edge-mc --depth 1 KubeStellar-example1\ncd KubeStellar-example1\ngit restore --staged Makefile Makefile.venv go.mod docs/mkdocs.yml docs/content docs/scripts/docs-ecutable.sh\ngit checkout Makefile Makefile.venv go.mod docs/mkdocs.yml docs/content docs/scripts/docs-ecutable.sh\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/example1.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ../\nrm -rf KubeStellar-example1\n</code></pre> <p>This doc shows a detailed example usage of the KubeStellar components.</p> <p>This example involves two edge clusters and two workloads.  One workload goes on both edge clusters and one workload goes on only one edge cluster.  Nothing changes after the initial activity.</p> <p>This example is presented in stages.  The controllers involved are always maintaining relationships.  This document focuses on changes as they appear in this example.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-1","title":"Stage 1","text":"<p>Stage 1 creates the infrastructure and the edge service provider workspace (ESPW) and lets that react to the inventory.  Then the KubeStellar syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox workspaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>cat &gt; florin-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>cat &gt; guilder-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#start-kcp","title":"Start kcp","text":"<p>Download and build or install kcp, according to your preference.</p> <p>In some shell that will be used only for this purpose, issue the <code>kcp start</code> command.  If you have junk from previous runs laying around, you should probably <code>rm -rf .kcp</code> first.</p> <p>In the shell commands in all the following steps it is assumed that <code>kcp</code> is running and <code>$KUBECONFIG</code> is set to the <code>.kcp/admin.kubeconfig</code> that <code>kcp</code> produces, except where explicitly noted that the florin or guilder cluster is being accessed.</p> <p>It is also assumed that you have the usual kcp kubectl plugins on your <code>$PATH</code>.</p> <pre><code>git clone https://github.com/kcp-dev/edge-mc KubeStellar\n</code></pre> <p>clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>cd kcp\nmake build\n</code></pre></p> <p>run kcp (kcp will spit out tons of information and stay running in this terminal window) <pre><code>export KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\nexport PATH=$(pwd)/bin:$PATH\nkcp start &amp;&gt; /dev/null &amp;\nsleep 30 </code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-an-inventory-management-workspace","title":"Create an inventory management workspace.","text":"<p>Use the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create imw-1 --enter\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#get-kubestellar","title":"Get KubeStellar","text":"<p>Download and build or install edge-mc, according to your preference.  That is, either (a) <code>git clone</code> the repo and then <code>make build</code> to populate its <code>bin</code> directory, or (b) fetch the binary archive appropriate for your machine from a release and unpack it (creating a <code>bin</code> directory).  In the following exhibited command lines, the commands described as \"KubeStellar commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the KubeStellar <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames.  The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p> <pre><code>cd ../KubeStellar\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=si</code>.</p> <pre><code>kubectl kubestellar ensure location florin  loc-name=florin  env=prod\nkubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=si\n</code></pre> <p>Those two script invocations are equivalent to creating the following four objects.</p> <pre><code>apiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: si\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: si\nspec:\nresource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-the-edge-service-provider-workspace","title":"Create the edge service provider workspace","text":"<p>Use the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create espw --enter\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#populate-the-edge-service-provider-workspace","title":"Populate the edge service provider workspace","text":"<p>This puts the definition and export of the KubeStellar API in the edge service provider workspace.</p> <p>Use the following command.</p> <pre><code>kubectl create -f config/exports\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#the-mailbox-controller","title":"The mailbox controller","text":"<p>Running the mailbox controller will be conveniently automated. Eventually.  In the meantime, you can use the KubeStellar command shown here.</p> <p><pre><code>go run ./cmd/mailbox-controller -v=2 &amp;\nsleep 45\n</code></pre> <pre><code>...\nI0423 01:09:37.991080   10624 main.go:196] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/root/workload.kcp.io\"\n...\nI0423 01:09:38.449395   10624 controller.go:299] \"Created APIBinding\" worker=1 mbwsName=\"apmziqj9p9fqlflm-mb-bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\" mbwsCluster=\"yk9a66vjms1pi8hu\" bindingName=\"bind-edge\" resourceVersion=\"914\"\n...\nI0423 01:09:38.842881   10624 controller.go:299] \"Created APIBinding\" worker=3 mbwsName=\"apmziqj9p9fqlflm-mb-b8c64c64-070c-435b-b3bd-9c0f0c040a54\" mbwsCluster=\"12299slctppnhjnn\" bindingName=\"bind-edge\" resourceVersion=\"968\"\n^C\n</code></pre></p> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox workspaces.</p> <p>This controller creates a mailbox workspace for each SyncTarget and puts an APIBinding to the edge API in each of those mailbox workspaces.  For this simple scenario, you do not need to keep this controller running after it does those things (hence the <code>^C</code> above); normally it would run continuously.</p> <p>You can get a listing of those mailbox workspaces as follows.</p> <p><pre><code>kubectl get Workspaces\n</code></pre> <pre><code>NAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   universal            Ready   https://192.168.58.123:6443/clusters/1najcltzt2nqax47   50s\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   universal            Ready   https://192.168.58.123:6443/clusters/1y7wll1dz806h3sb   50s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>kubectl get Workspace -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kcp\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                                       SYNCTARGET   CLUSTER\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   florin       1najcltzt2nqax47\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   guilder      1y7wll1dz806h3sb\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox workspace by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kcp.io/sync-target-name\"] == \"guilder\") | .name')\n</code></pre> <pre><code>1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\n</code></pre></p> <p><pre><code>FLORIN_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kcp.io/sync-target-name\"] == \"florin\") | .name')\n</code></pre> <pre><code>1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#connect-guilder-edge-cluster-with-its-mailbox-workspace","title":"Connect guilder edge cluster with its mailbox workspace","text":"<p>The following command will (a) create, in the mailbox workspace for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw-1 guilder\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote physical cluster manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>You might check that the syncer is running, as follows.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                        coredns                            2/2     2            2           35m\nlocal-path-storage                 local-path-provisioner             1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#connect-florin-edge-cluster-with-its-mailbox-workspace","title":"Connect florin edge cluster with its mailbox workspace","text":"<p>Do the analogous stuff for the florin cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw-1 florin\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote physical cluster manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>And deploy the syncer in the florin cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml </code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the scheduler react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a common parent for those two workspaces, with the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create my-org --enter\n</code></pre> <p>Next, create the WMW for the common workload.  The following command will do that, if issued while \"root:my-org\" is the current workspace.</p> <pre><code>kubectl kubestellar ensure wmw wmw-c\n</code></pre> <p>This is equivalent to creating that workspace and then entering it and creating the following two <code>APIBinding</code> objects.</p> <p><pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-espw\nspec:\nreference:\nexport:\npath: root:espw\nname: edge.kcp.io\n---\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-kube\nspec:\nreference:\nexport:\npath: \"root:compute\"\nname: kubernetes\n</code></pre> <pre><code>sleep 15\n</code></pre></p> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n  labels: {common: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kcp.io/v1alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kcp.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kcp.io/v1alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  namespaceSelector:\n    matchLabels: {\"common\":\"si\"}\n  nonNamespacedObjects:\n  - apiGroup: apis.kcp.io\n    resources: [ \"apibindings\" ]\n    resourceNames: [ \"bind-kube\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code>kubectl ws root:my-org\nkubectl kubestellar ensure wmw wmw-s\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kcp.io/v1alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kcp.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kcp.io/v1alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"si\"}\n  namespaceSelector: \n    matchLabels: {\"special\":\"si\"}\n  nonNamespacedObjects:\n  - apiGroup: apis.kcp.io\n    resources: [ \"apibindings\" ]\n    resourceNames: [ \"bind-kube\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#edge-scheduling","title":"Edge scheduling","text":"<p>In response to each EdgePlacement, the scheduler will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>Eventually there will be automation that conveniently runs the scheduler.  In the meantime, you can run it by hand: switch to the ESPW and invoke the KubeStellar command that runs the scheduler.</p> <p><pre><code>kubectl ws root:espw\n</code></pre> <pre><code>Current workspace is \"root:espw\".\n</code></pre> <pre><code>go run ./cmd/kubestellar-scheduler &amp;\nsleep 45\n</code></pre> <pre><code>I0423 01:33:37.036752   11305 kubestellar-scheduler.go:212] \"Found APIExport view\" exportName=\"edge.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/7qkse309upzrv0fy/edge.kcp.io\"\n...\nI0423 01:33:37.320859   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-scheduler\" triggeringKind=Location key=\"apmziqj9p9fqlflm|florin\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"florin\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n...\nI0423 01:33:37.391772   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-scheduler\" triggeringKind=Location key=\"apmziqj9p9fqlflm|guilder\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"guilder\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n^C\n</code></pre></p> <p>In this simple scenario you do not need to keep the scheduler running after it gets its initial work done; normally it would run continually.</p> <p>Check out the SinglePlacementSlice objects as follows.</p> <p><pre><code>kubectl ws root:my-org:wmw-c\n</code></pre> <pre><code>Current workspace is \"root:my-org:wmw-c\".\n</code></pre></p> <p><pre><code>kubectl get SinglePlacementSlice -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: edge.kcp.io/v1alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\ngeneration: 4\nname: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kcp.io/v1alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\nuid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre></p> <p>Also check out the SinglePlacementSlice objects in <code>root:my-org:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-3","title":"Stage 3","text":"<p>In Stage 3, in response to the EdgePlacement and SinglePlacementSlice objects, the placement translator will copy the workload prescriptions into the mailbox workspaces and create <code>SyncerConfig</code> objects there.</p> <p>Eventually there will be convenient automation running the placement translator.  In the meantime, you can run it manually: switch to the ESPW and use the KubeStellar command that runs the placement translator.</p> <p><pre><code>kubectl ws root:espw\n</code></pre> <pre><code>Current workspace is \"root:espw\".\n</code></pre> <pre><code>go run ./cmd/placement-translator &amp;\nsleep 120\n</code></pre> <pre><code>I0423 01:39:56.362722   11644 shared_informer.go:282] Waiting for caches to sync for placement-translator\n...\n</code></pre></p> <p>After it stops logging stuff, wait another minute and then you can ^C it or use another shell to continue exploring.</p> <p>The florin cluster gets only the common workload.  Examine florin's <code>SyncerConfig</code> as follows.  Utilize florin's name (which you stored in Stage 1) here.</p> <pre><code>kubectl ws $FLORIN_WS\n</code></pre> <pre><code>Current workspace is \"root:espw:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\n</code></pre> <pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12299slctppnhjnn\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1323\"\nuid: 8840fee6-37dc-407e-ad01-2ad59389d4ff\nspec:\n  namespaceScope:\n    namespaces:\n    - commonstuff\n    resources:\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n    - apiVersion: v1\n      group: \"\"\nresource: configmaps\n    - apiVersion: v1\n      group: \"\"\nresource: limitranges\n    - apiVersion: v1\n      group: \"\"\nresource: secrets\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\nresource: pods\n    - apiVersion: v1\n      group: \"\"\nresource: serviceaccounts\n    - apiVersion: v1\n      group: \"\"\nresource: services\n    - apiVersion: v1\n      group: \"\"\nresource: resourcequotas\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n  upsync:\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre> <p>You can check that the workload got there too.</p> <p><pre><code>kubectl get ns\n</code></pre> <pre><code>NAME          STATUS   AGE\ncommonstuff   Active   6m34s\ndefault       Active   32m\n</code></pre></p> <p><pre><code>kubectl get deployments -A\n</code></pre> <pre><code>NAMESPACE     NAME      READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff   commond   0/0     0            0           6m44s\n</code></pre></p> <p>The guilder cluster gets both the common and special workloads. Examine guilder's <code>SyncerConfig</code> object and workloads as follows, using the name that you stored in Stage 1.</p> <p><pre><code>kubectl ws root:espw\n</code></pre> <pre><code>Current workspace is \"root:espw\".\n</code></pre></p> <p><pre><code>kubectl ws $GUILDER_WS\n</code></pre> <pre><code>Current workspace is \"root:espw:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: yk9a66vjms1pi8hu\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 4\nname: the-one\n  resourceVersion: \"1325\"\nuid: 3da056c7-0d5c-45a3-9d91-d04f04415f30\nspec:\n  namespaceScope:\n    namespaces:\n    - commonstuff\n    - specialstuff\n    resources:\n    - apiVersion: v1\n      group: \"\"\nresource: services\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\nresource: pods\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: \"\"\nresource: limitranges\n    - apiVersion: v1\n      group: \"\"\nresource: serviceaccounts\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n    - apiVersion: v1\n      group: \"\"\nresource: configmaps\n    - apiVersion: v1\n      group: \"\"\nresource: secrets\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n    - apiVersion: v1\n      group: \"\"\nresource: resourcequotas\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre></p> <p><pre><code>kubectl get deployments -A\n</code></pre> <pre><code>NAMESPACE      NAME       READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff    commond    0/0     0            0           6m1s\nspecialstuff   speciald   0/0     0            0           5m58s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-4","title":"Stage 4","text":"<p>In Stage 4, the edge syncer does its thing.  Actually, it should have done it as soon as the relevant inputs became available in stage 3. Now we examine what happened.</p> <p>You can check that the workloads are running in the edge clusters as they should be.</p> <p>The syncer does its thing between the florin cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>The syncer does its thing between the guilder cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>Using the kubeconfig that <code>kind</code> modified, examine the florin cluster. Find just the <code>commonstuff</code> namespace and the <code>commond</code> Deployment.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get ns\n</code></pre> <pre><code>NAME                                 STATUS   AGE\ncommonstuff                          Active   6m51s\ndefault                              Active   57m\nkubestellar-syncer-florin-1t9zgidy   Active   17m\nkube-node-lease                      Active   57m\nkube-public                          Active   57m\nkube-system                          Active   57m\nlocal-path-storage                   Active   57m\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy -A | egrep 'NAME|stuff'\n</code></pre> <pre><code>NAMESPACE                         NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                       commond                           1/1     1            1           7m59s\n</code></pre></p> <p>Examine the guilder cluster.  Find both workload namespaces and both Deployments.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get ns | egrep NAME\\|stuff\n</code></pre> <pre><code>NAME                               STATUS   AGE\ncommonstuff                        Active   8m33s\nspecialstuff                       Active   8m33s\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A | egrep NAME\\|stuff\n</code></pre> <pre><code>NAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                        commond                            1/1     1            1           8m37s\nspecialstuff                       speciald                           1/1     1            1           8m55s\n</code></pre></p> <p>Examining the common workload in the guilder cluster, for example, will show that the replacement-style customization happened.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -n commonstuff commond -o yaml\n</code></pre> <pre><code>...\n      containers:\n      - env:\n        - name: EXAMPLE_VAR\n          value: env is prod\n        image: library/httpd:2.4\n        imagePullPolicy: IfNotPresent\n        name: httpd\n...\n</code></pre></p> <p>Check that the common workload on the florin cluster is working.</p> <p><pre><code>sleep 10\n</code></pre> <pre><code>curl http://localhost:8094\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in florin.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the special workload on the guilder cluster is working. <pre><code>sleep 10\n</code></pre> <pre><code>curl http://localhost:8097\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a special web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the common workload on the guilder cluster is working.</p> <p><pre><code>curl http://localhost:8096\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#stage-5","title":"Stage 5","text":"<p>The status summarizer, driven by the EdgePlacement and SinglePlacementSlice for the special workload, creates a status summary object in the specialstuff namespace in the special workload workspace holding a summary of the corresponding Deployment objects. In this case there is just one such object, in the mailbox workspace for the guilder cluster.</p> <p></p> <p>The status summarizer, driven by the EdgePlacement and SinglePlacementSlice for the common workload, creates a status summary object in the commonstuff namespace in the common workload workspace holding a summary of the corresponding Deployment objects.  Those are the <code>commond</code> Deployment objects in the two mailbox workspaces.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl ws root:my-org\nkubectl kubestellar remove wmw example-wmw\nkubectl ws root\nkubectl delete workspace my-org\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Stop and uninstall KubeStellar use the following command:</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/","title":"KubeStellar Scheduler","text":"<p>Required Packages:</p> MacUbuntuRHELWSL <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19<pre><code>curl -L \"https://go.dev/dl/go1.19.5.linux-$(dpkg --print-architecture).tar.gz\" -o go.tar.gz\ntar -C /usr/local -xzf go.tar.gz\nrm go.tar.gz\necho 'export PATH=$PATH:/usr/local/go/bin' &gt;&gt; /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64 &amp;&amp; chmod +x ./kind &amp;&amp; mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\nchoco install curl -y\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -n -b main https://github.com/kcp-dev/edge-mc --depth 1 KubeStellar-kubestellar-scheduler\ncd KubeStellar-kubestellar-scheduler\ngit restore --staged Makefile Makefile.venv go.mod docs/mkdocs.yml docs/content docs/scripts/docs-ecutable.sh\ngit checkout Makefile Makefile.venv go.mod docs/mkdocs.yml docs/content docs/scripts/docs-ecutable.sh\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/kubestellar-scheduler.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ../\nrm -rf KubeStellar-kubestellar-scheduler\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#steps-to-try-the-scheduler","title":"Steps to try the scheduler","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#pull-the-kcp-and-kubestellar-source-code-build-the-kubectl-ws-binary-and-start-kcp","title":"Pull the kcp and KubeStellar source code, build the kubectl-ws binary, and start kcp","text":"<p>Open a terminal window(1) and clone the latest KubeStellar source:</p> <pre><code>git clone https://github.com/kcp-dev/edge-mc KubeStellar\n</code></pre> <p>Clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> Build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>cd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Run kcp (kcp will spit out tons of information and stay running in this terminal window) <pre><code>kcp start &amp;&gt; /dev/null &amp;\nsleep 30\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#create-the-edge-service-provider-workspace-espw-and-populate-it-with-crds-and-apis","title":"Create the Edge Service Provider Workspace (ESPW) and populate it with CRDs and APIs","text":"<p>Open another terminal window(2) and point <code>$KUBECONFIG</code> to the admin kubeconfig for the kcp server and include the location of kubectl-ws in <code>$PATH</code>.</p> <pre><code>export KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>Next, create the edge service provider workspace:</p> <p>Use workspace <code>root:espw</code> as the Edge Service Provider Workspace (ESPW). <pre><code>kubectl ws root\nkubectl ws create espw\n</code></pre></p> <p>Install CRDs and APIExport.</p> <pre><code>kubectl ws root:espw\nkubectl apply -f ../KubeStellar/config/exports/\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#create-the-workload-management-workspace-wmw-and-bind-it-to-the-espw-apis","title":"Create the Workload Management Workspace (WMW) and bind it to the ESPW APIs","text":"<p>Use the user home workspace (\\~) as the workload management workspace (WMW). <pre><code>kubectl ws \\~\n</code></pre></p> <p>Bind APIs. <pre><code>kubectl apply -f ../KubeStellar/config/imports/\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#run-the-kubestellar-scheduler-against-the-espw","title":"Run the KubeStellar Scheduler against the ESPW","text":"<p>Go to the <code>root:espw</code> workspace and run the edge scheduler.</p> <pre><code>kubectl ws root:espw\ncd ../KubeStellar\ngo run cmd/kubestellar-scheduler/main.go -v 2 &amp;\nsleep 45\n</code></pre> <p>The outputs from the edge scheduler should be similar to: <pre><code>I0605 10:53:00.156100   29786 scheduler.go:212] \"Found APIExport view\" exportName=\"edge.kcp.io\" serverURL=\"https://192.168.1.13:6443/services/apiexport/jxch2kyb3c1h6bac/edge.kcp.io\"\nI0605 10:53:00.157874   29786 scheduler.go:212] \"Found APIExport view\" exportName=\"scheduling.kcp.io\" serverURL=\"https://192.168.1.13:6443/services/apiexport/root/scheduling.kcp.io\"\nI0605 10:53:00.159242   29786 scheduler.go:212] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.1.13:6443/services/apiexport/root/workload.kcp.io\"\nI0605 10:53:00.261128   29786 controller.go:201] \"starting controller\" controller=\"kubestellar-scheduler\"\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#create-the-inventory-management-workspace-imw-and-populate-it-with-locations-and-synctargets","title":"Create the Inventory Management Workspace (IMW) and populate it with locations and synctargets","text":"<p>open another terminal window(3) and point <code>$KUBECONFIG</code> to the admin kubeconfig for the kcp server and include the location of kubectl-ws in $PATH. <pre><code>cd ../kcp\nexport KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Use workspace <code>root:compute</code> as the Inventory Management Workspace (IMW). <pre><code>kubectl ws root:compute\n</code></pre></p> <p>Create two Locations and two SyncTargets. <pre><code>kubectl create -f ../KubeStellar/config/samples/location_prod.yaml\nkubectl create -f ../KubeStellar/config/samples/location_dev.yaml\nkubectl create -f ../KubeStellar/config/samples/synctarget_prod.yaml\nkubectl create -f ../KubeStellar/config/samples/synctarget_dev.yaml\nsleep 5\n</code></pre></p> <p>Note that kcp automatically creates a Location <code>default</code>. So there are 3 Locations and 2 SyncTargets in <code>root:compute</code>. <pre><code>kubectl get locations,synctargets\n</code></pre> <pre><code>NAME                                 RESOURCE      AVAILABLE   INSTANCES   LABELS   AGE\nlocation.scheduling.kcp.io/default   synctargets   0           2                    2m12s\nlocation.scheduling.kcp.io/dev       synctargets   0           1                    2m39s\nlocation.scheduling.kcp.io/prod      synctargets   0           1                    3m13s\n\nNAME                              AGE\nsynctarget.workload.kcp.io/dev    110s\nsynctarget.workload.kcp.io/prod   2m12s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#create-some-edgeplacements-in-the-wmw","title":"Create some EdgePlacements in the WMW","text":"<p>Go to Workload Management Workspace (WMW) and create an EdgePlacement <code>all2all</code>. <pre><code>kubectl ws \\~\nkubectl create -f ../KubeStellar/config/samples/edgeplacement_all2all.yaml\nsleep 3\n</code></pre></p> <p>The scheduler maintains a SinglePlacementSlice for an EdgePlacement in the same workspace. <pre><code>kubectl get sps all2all -oyaml\n</code></pre> <pre><code>apiVersion: edge.kcp.io/v1alpha1\ndestinations:\n- cluster: 1yotsgod0d2p3xa5\n  locationName: prod\n  syncTargetName: prod\n  syncTargetUID: 13841ffd-33f2-4cf4-9114-6156f73aa5c8\n- cluster: 1yotsgod0d2p3xa5\n  locationName: dev\n  syncTargetName: dev\n  syncTargetUID: ea5492ec-44af-4173-a4ca-9c5cd59afcb1\n- cluster: 1yotsgod0d2p3xa5\n  locationName: default\n  syncTargetName: dev\n  syncTargetUID: ea5492ec-44af-4173-a4ca-9c5cd59afcb1\n- cluster: 1yotsgod0d2p3xa5\n  locationName: default\n  syncTargetName: prod\n  syncTargetUID: 13841ffd-33f2-4cf4-9114-6156f73aa5c8\nkind: SinglePlacementSlice\nmetadata:\n  annotations:\n    kcp.io/cluster: kvdk2spgmbix\n  creationTimestamp: \"2023-06-05T14:55:20Z\"\ngeneration: 1\nname: all2all\n  ownerReferences:\n  - apiVersion: edge.kcp.io/v1alpha1\n    kind: EdgePlacement\n    name: all2all\n    uid: 31915018-6a25-4f01-943e-b8a0a0ed35ba\n  resourceVersion: \"875\"\nuid: a2b8224d-5feb-40a1-adb2-67c07965f13b\n</code></pre> EdgePlacement <code>all2all</code> selects all the 3 Locations in <code>root:compute</code>.</p> <p>Create a more specific EdgePlacement which selects Locations labeled by <code>env: dev</code>. <pre><code>kubectl create -f ../KubeStellar/config/samples/edgeplacement_dev.yaml\nsleep 3\n</code></pre></p> <p>The corresponding SinglePlacementSlice has a shorter list of <code>destinations</code>: <pre><code>kubectl get sps dev -oyaml\n</code></pre> <pre><code>apiVersion: edge.kcp.io/v1alpha1\ndestinations:\n- cluster: 1yotsgod0d2p3xa5\n  locationName: dev\n  syncTargetName: dev\n  syncTargetUID: ea5492ec-44af-4173-a4ca-9c5cd59afcb1\nkind: SinglePlacementSlice\nmetadata:\n  annotations:\n    kcp.io/cluster: kvdk2spgmbix\n  creationTimestamp: \"2023-06-05T14:57:00Z\"\ngeneration: 1\nname: dev\n  ownerReferences:\n  - apiVersion: edge.kcp.io/v1alpha1\n    kind: EdgePlacement\n    name: dev\n    uid: 1ac4b7f5-5521-4b5a-a0fa-cc2ec87b458b\n  resourceVersion: \"877\"\nuid: c9c0c2fc-d721-4c73-9788-e10711bad23a\n</code></pre></p> <p>Feel free to change the Locations, SyncTargets, and EdgePlacements and see how the scheduler reacts.</p> <p>Your next step is to deliver a workload to a mailbox (that represents an edge location).  Go here to take the next step... (TBD)</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl ws root:my-org\nkubectl kubestellar remove wmw example-wmw\nkubectl ws root\nkubectl delete workspace my-org\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Stop and uninstall KubeStellar use the following command:</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/","title":"KubeStellar-Syncer","text":"<p>Required Packages:</p> MacUbuntuRHELWSL <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19<pre><code>curl -L \"https://go.dev/dl/go1.19.5.linux-$(dpkg --print-architecture).tar.gz\" -o go.tar.gz\ntar -C /usr/local -xzf go.tar.gz\nrm go.tar.gz\necho 'export PATH=$PATH:/usr/local/go/bin' &gt;&gt; /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64 &amp;&amp; chmod +x ./kind &amp;&amp; mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\nchoco install curl -y\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -n -b main https://github.com/kcp-dev/edge-mc --depth 1 KubeStellar-kubestellar-syncer\ncd KubeStellar-kubestellar-syncer\ngit restore --staged Makefile Makefile.venv go.mod docs/mkdocs.yml docs/content docs/scripts/docs-ecutable.sh\ngit checkout Makefile Makefile.venv go.mod docs/mkdocs.yml docs/content docs/scripts/docs-ecutable.sh\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/kubestellar-syncer.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ../\nrm -rf KubeStellar-kubestellar-syncer\n</code></pre> <p>KubeStellar-Syncer runs in the target cluster and sync kubernetes resource objects from the target cluster to a mailbox workspace and vice versa.</p> <p></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#steps-to-try-the-syncer","title":"Steps to try the Syncer","text":"<p>The KubeStellar-Syncer can be exercised after setting up KubeStellar mailbox workspaces. Firstly we'll follow to similar steps in example1 until <code>The mailbox controller</code> in stage 2. </p> <p></p> <p>Stage 1 creates the infrastructure and the edge service provider workspace (ESPW) and lets that react to the inventory.  Then the KubeStellar syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox workspaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>cat &gt; florin-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>cat &gt; guilder-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#start-kcp","title":"Start kcp","text":"<p>Download and build or install kcp, according to your preference.</p> <p>In some shell that will be used only for this purpose, issue the <code>kcp start</code> command.  If you have junk from previous runs laying around, you should probably <code>rm -rf .kcp</code> first.</p> <p>In the shell commands in all the following steps it is assumed that <code>kcp</code> is running and <code>$KUBECONFIG</code> is set to the <code>.kcp/admin.kubeconfig</code> that <code>kcp</code> produces, except where explicitly noted that the florin or guilder cluster is being accessed.</p> <p>It is also assumed that you have the usual kcp kubectl plugins on your <code>$PATH</code>.</p> <pre><code>git clone https://github.com/kcp-dev/edge-mc KubeStellar\n</code></pre> <p>clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>cd kcp\nmake build\n</code></pre></p> <p>run kcp (kcp will spit out tons of information and stay running in this terminal window) <pre><code>export KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\nexport PATH=$(pwd)/bin:$PATH\nkcp start &amp;&gt; /dev/null &amp;\nsleep 30 </code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#create-an-inventory-management-workspace","title":"Create an inventory management workspace.","text":"<p>Use the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create imw-1 --enter\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#get-kubestellar","title":"Get KubeStellar","text":"<p>Download and build or install edge-mc, according to your preference.  That is, either (a) <code>git clone</code> the repo and then <code>make build</code> to populate its <code>bin</code> directory, or (b) fetch the binary archive appropriate for your machine from a release and unpack it (creating a <code>bin</code> directory).  In the following exhibited command lines, the commands described as \"KubeStellar commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the KubeStellar <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames.  The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p> <pre><code>cd ../KubeStellar\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=si</code>.</p> <pre><code>kubectl kubestellar ensure location florin  loc-name=florin  env=prod\nkubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=si\n</code></pre> <p>Those two script invocations are equivalent to creating the following four objects.</p> <pre><code>apiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: si\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: si\nspec:\nresource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#create-the-edge-service-provider-workspace","title":"Create the edge service provider workspace","text":"<p>Use the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create espw --enter\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#populate-the-edge-service-provider-workspace","title":"Populate the edge service provider workspace","text":"<p>This puts the definition and export of the KubeStellar API in the edge service provider workspace.</p> <p>Use the following command.</p> <pre><code>kubectl create -f config/exports\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#the-mailbox-controller","title":"The mailbox controller","text":"<p>Running the mailbox controller will be conveniently automated. Eventually.  In the meantime, you can use the KubeStellar command shown here.</p> <p><pre><code>go run ./cmd/mailbox-controller -v=2 &amp;\nsleep 45\n</code></pre> <pre><code>...\nI0423 01:09:37.991080   10624 main.go:196] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/root/workload.kcp.io\"\n...\nI0423 01:09:38.449395   10624 controller.go:299] \"Created APIBinding\" worker=1 mbwsName=\"apmziqj9p9fqlflm-mb-bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\" mbwsCluster=\"yk9a66vjms1pi8hu\" bindingName=\"bind-edge\" resourceVersion=\"914\"\n...\nI0423 01:09:38.842881   10624 controller.go:299] \"Created APIBinding\" worker=3 mbwsName=\"apmziqj9p9fqlflm-mb-b8c64c64-070c-435b-b3bd-9c0f0c040a54\" mbwsCluster=\"12299slctppnhjnn\" bindingName=\"bind-edge\" resourceVersion=\"968\"\n^C\n</code></pre></p> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox workspaces.</p> <p>This controller creates a mailbox workspace for each SyncTarget and puts an APIBinding to the edge API in each of those mailbox workspaces.  For this simple scenario, you do not need to keep this controller running after it does those things (hence the <code>^C</code> above); normally it would run continuously.</p> <p>You can get a listing of those mailbox workspaces as follows.</p> <p><pre><code>kubectl get Workspaces\n</code></pre> <pre><code>NAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   universal            Ready   https://192.168.58.123:6443/clusters/1najcltzt2nqax47   50s\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   universal            Ready   https://192.168.58.123:6443/clusters/1y7wll1dz806h3sb   50s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>kubectl get Workspace -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kcp\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                                       SYNCTARGET   CLUSTER\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   florin       1najcltzt2nqax47\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   guilder      1y7wll1dz806h3sb\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox workspace by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kcp.io/sync-target-name\"] == \"guilder\") | .name')\n</code></pre> <pre><code>1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\n</code></pre></p> <p><pre><code>FLORIN_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kcp.io/sync-target-name\"] == \"florin\") | .name')\n</code></pre> <pre><code>1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#register-kubestellar-syncer-on-the-target-clusters","title":"Register KubeStellar-Syncer on the target clusters","text":"<p>Once KubeStellar setup is done, KubeStellar-Syncer can be deployed on the target cluster easily by the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#for-the-target-cluster-of-guilder","title":"For the target cluster of <code>guilder</code>,","text":"<p>Go to inventory management workspace and find the mailbox workspace name. <pre><code>kubectl ws root:imw-1\nmbws=`kubectl get SyncTarget guilder -o jsonpath=\"{.metadata.annotations['kcp\\.io/cluster']}-mb-{.metadata.uid}\"`\necho \"mailbox workspace name = $mbws\"\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nmailbox workspace name = vosh9816n2xmpdwm-mb-bf1277df-0da9-4a26-b0fc-3318862b1a5e\n</code></pre></p> <p>Go to the mailbox workspace and run the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer. <pre><code>kubectl ws root:espw:$mbws\n./bin/kubectl-kubestellar-syncer_gen guilder --syncer-image quay.io/kubestellar/syncer:v0.2.2 -o guilder-syncer.yaml\n</code></pre> <pre><code>Current workspace is \"root:espw:vosh9816n2xmpdwm-mb-bf1277df-0da9-4a26-b0fc-3318862b1a5e\".\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote physical cluster manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Deploy the generated yaml manifest to the target cluster. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>Check that the syncer is running, as follows. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                           coredns                               2/2     2            2           35m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#for-the-target-cluster-of-florin","title":"For the target cluster of <code>florin</code>,","text":"<p>Go to inventory management workspace and find the mailbox workspace name. <pre><code>kubectl ws root:imw-1\nmbws=`kubectl get SyncTarget florin -o jsonpath=\"{.metadata.annotations['kcp\\.io/cluster']}-mb-{.metadata.uid}\"`\necho \"mailbox workspace name = $mbws\"\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nmailbox workspace name = vosh9816n2xmpdwm-mb-bb47149d-52d3-4f14-84dd-7b64ac01c97f\n</code></pre></p> <p>Go to the mailbox workspace and run the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer. <pre><code>kubectl ws root:espw:$mbws\n./bin/kubectl-kubestellar-syncer_gen florin --syncer-image quay.io/kubestellar/syncer:v0.2.2 -o florin-syncer.yaml\n</code></pre> <pre><code>Current workspace is \"root:espw:vosh9816n2xmpdwm-mb-bb47149d-52d3-4f14-84dd-7b64ac01c97f\".\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote physical cluster manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Deploy the generated yaml manifest to the target cluster. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p> <p>Check that the syncer is running, as follows. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy -A\n</code></pre> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-florin-32uaph9l    kubestellar-syncer-florin-32uaph9l    1/1     1            1           42s\nkube-system                           coredns                               2/2     2            2           41m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           41m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl ws root:my-org\nkubectl kubestellar remove wmw example-wmw\nkubectl ws root\nkubectl delete workspace my-org\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Stop and uninstall KubeStellar use the following command:</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#deep-dive","title":"Deep-dive","text":""},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#the-details-about-the-registration-of-kubestellar-syncer-on-an-edge-cluster-and-a-workspace","title":"The details about the registration of KubeStellar-Syncer on an Edge cluster and a workspace","text":"<p>KubeStellar-Syncer is deployed on an Edge cluster easily by the following steps.</p> <ol> <li>Create SyncTarget and Location<ul> <li>Mailbox controller creates the mailbox workspace automatically </li> </ul> </li> <li>Get the mailbox workspace name</li> <li>Use the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer     <pre><code>kubectl ws &lt;mb-ws name&gt;\nbin/kubectl-kubestellar-syncer_gen &lt;Edge Sync Target name&gt; --syncer-image &lt;KubeStellar-Syncer image&gt; -o kubestellar-syncer.yaml\n</code></pre>     Here, <code>bin/kubectl-kubestellar-syncer_gen</code> refers to a special variant of KubeStellar's     kubectl plugin that includes the implementation of the functionality needed     here.  This variant, under the special name shown here, is a normal part of     the <code>bin</code> of KubeStellar.     For the KubeStellar-Syncer image, please select an official image from https://quay.io/repository/kubestellar/syncer?tab=tags. For example, <code>--syncer-image quay.io/kubestellar/syncer:v0.2.2</code>. You can also create a syncer image from the source following Build KubeStellar-Syncer Image.</li> <li>Deploy KubeStellar-Syncer on an Edge cluster</li> <li>Syncer starts to run on the Edge cluster<ul> <li>KubeStellar-Syncer starts watching and consuming SyncerConfig</li> </ul> </li> </ol> <p>The overall diagram is as follows:</p> <p></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#what-kubestellar-syncer-gen-plugin-does","title":"What KubeStellar syncer-gen plugin does","text":"<p>In order for Syncer to sync resources between upstream (workspace) and downstream (physical cluster), access information for both is required. </p> <p>For the upstream access, Syncer's registration command (<code>kubectl kubestellar syncer-gen</code>) creates a ServiceAccount, ClusterRole, and ClusterRoleBinding in the workspace, and then generates a kubeconfig manifest from the ServiceAccount token, KCP server URL, and the server certificates. The kubeconfig manifest is embedded in a secret manifest and the secret is mounted to <code>/kcp/</code> in Syncer pod. The command generates such deployment manifest as Syncer reads <code>/kcp/</code> for the upstream Kubeconfig. </p> <p>On the other hand, for the downstream part, in addition to the deployment manifest, the command generates a ServiceAccount, Role/ClusterRole, RoleBinding/ClusterRoleBinding for Syncer to access resources on the physical cluster. These resources for the downstream part are the resources to be deployed to the downstream cluster. The ServiceAccount is set to <code>serviceAccountName</code> in the deployment manifest.</p> <p>Note: In addition to that, the command creates an EdgeSyncConfig CRD if it does not exist, and creates a default EdgeSyncConfig resource with the name specified in the command (<code>kubectl kubestellar syncer-gen &lt;name&gt;</code>). The default EdgeSyncConfig is no longer needed since Syncer now consumes all EdgeSyncConfigs in the workspace. Furthermore, creation of the EdgeSyncConfig CRD will also no longer be needed since we will switch to using SyncerConfig rather than EdgeSyncConfig in near future.</p> <p>The source code of the command is https://github.com/kcp-dev/edge-mc/blob/main/pkg/cliplugins/kubestellar/syncer-gen/edgesync.go.</p> <p>The equivalent manual steps are as follows: </p> <p>Generate UUID for Syncer identification. <pre><code>syncer_id=\"syncer-\"`uuidgen | tr '[:upper:]' '[:lower:]'`\n</code></pre></p> <p>Go to a workspace. <pre><code>kubectl ws root\nkubectl ws create ws1 --enter\n</code></pre></p> <p>Create the following APIBinding in the workspace (Note that in the case of mailbox workspaces, it's done by mailbox controller at creating the mailbox workspace.) <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\n  name: bind-espw\nspec:\n  reference:\n    export:\n      path: root:espw\n      name: edge.kcp.io\nEOL\n</code></pre></p> <p>Create a serviceaccount in the workspace. <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: $syncer_id\nEOL\n</code></pre></p> <p>Create clusterrole and clusterrolebinding to bind the serviceaccount to the role. <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: $syncer_id\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- nonResourceURLs: [\"/\"]\n  verbs: [\"access\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: $syncer_id\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: $syncer_id\nsubjects:\n- apiGroup: \"\"\n  kind: ServiceAccount\n  name: $syncer_id\n  namespace: default\nEOL\n</code></pre></p> <p>Get the serviceaccount token that will be set in the upstream kubeconfig manifest. <pre><code>secret_name=`kubectl get secret -o custom-columns=\":.metadata.name\"| grep $syncer_id`\ntoken=`kubectl get secret $secret_name -o jsonpath='{.data.token}' | base64 -d`\n</code></pre></p> <p>Get the certificates that will be set in the upstream kubeconfig manifest. <pre><code>cacrt=`kubectl config view --minify --raw | yq \".clusters[0].cluster.certificate-authority-data\"`\n</code></pre></p> <p>Get server_url that will be set in the upstream kubeconfig manifest. <pre><code>server_url=`kubectl config view --minify --raw | yq \".clusters[0].cluster.server\" | sed -e 's|https://\\(.*\\):\\([^/]*\\)/.*|https://\\1:\\2|g'`\n</code></pre></p> <p>Set some other parameters. a. downstream_namespace where Syncer Pod runs <pre><code>downstream_namespace=\"kubestellar-$syncer_id\"\n</code></pre> b. Syncer image <pre><code>image=\"quay.io/kubestellar/syncer:v0.2.2\"\n</code></pre> c. Logical cluster name <pre><code>cluster_name=`kubectl get logicalclusters.core.kcp.io cluster -o custom-columns=\":.metadata.annotations.kcp\\.io\\/cluster\" --no-headers`\n</code></pre></p> <p>Download manifest template. <pre><code>curl -LO https://raw.githubusercontent.com/kcp-dev/edge-mc/main/pkg/syncer/scripts/kubestellar-syncer-bootstrap.template.yaml\n</code></pre></p> <p>Generate manifests to bootstrap KubeStellar-Syncer. <pre><code>syncer_id=$syncer_id cacrt=$cacrt token=$token server_url=$server_url downstream_namespace=$downstream_namespace image=$image cluster_name=$cluster_name envsubst &lt; kubestellar-syncer-bootstrap.template.yaml\n</code></pre> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: kubestellar-syncer-9ee90de6-eb76-4ddb-9346-c4c8d92075e1\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n...\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#deploy-workload-objects-from-kubestellar-to-edge-cluster","title":"Deploy workload objects from KubeStellar to Edge cluster","text":"<p>To deploy resources to Edge clusters, create the following in workload management workspace - workload objects   - Some objects are denatured if needed.   - Other objects are as it is - APIExport/API Schema corresponding to CRD such as Kubernetes ClusterPolicyReport.   - TBD: Conversion from CRD to APIExport/APISchema could be automated by using MutatingAdmissionWebhook on workload management workspace. This automation is already available (see the script here).  - EdgePlacement</p> <p></p> <p>After this, Edge-mc will put the following in the mailbox workspace. - Workload objects (both denatured one and not-denatured one) - SyncerConfig CR</p> <p>TODO: This is something we should clarify..e.g. which existing controller(s) in KubeStellar will cover, or just create a new controller to handle uncovered one. @MikeSpreitzer gave the following suggestions.   - The placement translator will put the workload objects and syncer config into the mailbox workspaces.   - The placement translator will create syncer config based on the EdgePlacement objects and what they match.   - The mailbox controller will put API Binding into the mailbox workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#edgesyncconfig-will-be-replaced-to-syncerconfig","title":"EdgeSyncConfig (will be replaced to SyncerConfig)","text":"<ul> <li>The example of EdgeSyncConfig CR is here. Its CRD is here.</li> <li>The CR here is used from edge syncer. </li> <li>The CR is placed at mb-ws to define</li> <li>object selector</li> <li>need of renaturing</li> <li>need of returning reported states of downsynced objects</li> <li>need of delete propagation for downsyncing</li> <li>The CR is managed by KubeStellar (placement transformer).</li> <li>At the initial implementation before KubeStellar side controller become ready, we assume SyncerConfig is on workload management workspace (wm-ws), and then which will be copied into mb-ws like other workload objects.</li> <li>This should be changed to be generated according to EdgePlacement spec. </li> <li>This CR is a placeholder for defining how KubeStellar-Syncer behaves, and will be extended/split/merged according to further design discussion.</li> <li>One CR is initially created by the command for KubeStellar-Syncer enablement in mb-ws (<code>kubectl kubestellar syncer-gen &lt;name&gt;</code>)</li> <li>The CR name is <code>&lt;name&gt;</code> and the contents are empty.</li> <li>This name is registered in the bootstrap manifests for KubeStellar-Syncer install and KubeStellar-Syncer is told to watch the CR of this name.</li> <li>Currently KubeStellar-Syncer watches all CRs in the workspace</li> <li>KubeStellar-Syncer merges them and decides which resources are down/up synced based on the merged information. </li> <li>This behavior may be changed to only watching the default CR once Placement Translator is to be the component that generates the CR from EdgePlacement: related issue</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#syncerconfig","title":"SyncerConfig","text":"<ul> <li>The spec is defined in https://github.com/kcp-dev/edge-mc/blob/main/pkg/apis/edge/v1alpha1/syncer-config.go</li> <li><code>namespaceScope</code> field is for namespace scoped objects.<ul> <li><code>namespaces</code> is field for which namespaces to be downsynced.</li> <li><code>resources</code> is field for what resource's objects in the above namespaces are downsynced. All objects in the selected resource are downsynced.</li> </ul> </li> <li><code>clusterScope</code> field is for cluster scoped objects<ul> <li>It's an array of <code>apiVersion</code>, <code>group</code>, <code>resource</code>, and <code>objects</code>.</li> <li><code>objects</code> can be specified by wildcard (<code>*</code>) meaning all objects.</li> </ul> </li> <li><code>upsync</code> field is for upsynced objects including both namespace and cluster scoped objects.<ul> <li>It's an array of <code>apiGroup</code>, <code>resources</code>, <code>namespaces</code>, and <code>names</code>.</li> <li><code>apiGroup</code> is group.</li> <li><code>resources</code> is an array of upsynced resource.</li> <li><code>namespaces</code> is an array of namespace for namespace objects.</li> <li><code>names</code> is an array of upsynced object name. Wildcard (<code>*</code>) is available.</li> </ul> </li> <li>The example CR is https://github.com/kcp-dev/edge-mc/blob/main/test/e2e/kubestellar-syncer/testdata/kyverno/syncer-config.yaml</li> <li>The CR is used from KubeStellar-Syncer</li> <li>The CR is placed in mb-ws to define</li> <li>object selector</li> <li>need of renaturing (May not scope in PoC2023q1)</li> <li>need of returning reported states of downsynced objects (May not scope in PoC2023q1)</li> <li>need of delete propagation for downsyncing (May not scope in PoC2023q1)</li> <li>The CR is managed by KubeStellar (placement translator).</li> <li>At the initial implementation before KubeStellar side controller become ready, we assume SyncerConfig is on workload management workspace (wm-ws), and then which will be copied into mb-ws like other workload objects.</li> <li>This should be changed to be generated according to EdgePlacement spec. </li> <li>This CR is a placeholder for defining how KubeStellar-Syncer behaves, and will be extended/split/merged according to further design discussion.</li> <li>Currently KubeStellar-Syncer watches all CRs in the workspace</li> <li>KubeStellar-Syncer merges them and decides which resources are down/up synced based on the merged information. </li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#downsyncing","title":"Downsyncing","text":"<ul> <li>KubeStellar-Syncer does downsyncing, which copy workload objects on mailbox workspace to Edge cluster</li> <li>If workload objects are deleted on mailbox workspace, the corresponding objects on the Edge cluster will be also deleted according to SyncerConfig. </li> <li>SyncerConfig specifies which objects should be downsynced.</li> <li>object selector: group, version, kind, name, namespace (for namespaced objects), label, annotation</li> <li>Cover cluster-scope objects and CRD</li> <li>CRD needs to be denatured if downsyncing is required. (May not scope in PoC2023q1 since no usage)</li> <li>Renaturing is applied if required (specified in SyncerConfig). (May not scope in PoC2023q1 since no usage)</li> <li>Current implementation is using polling to detect changes on mailbox workspace, but will be changed to use Informers. </li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#renaturing-may-not-scope-in-poc2023q1-since-no-usage","title":"Renaturing (May not scope in PoC2023q1 since no usage)","text":"<ul> <li>KubeStellar-Syncer does renaturing, which converts workload objects to different forms of objects on a Edge cluster. </li> <li>The conversion rules (downstream/upstream mapping) is specified in SyncerConfig.</li> <li>Some objects need to be denatured. </li> <li>CRD needs to be denatured when conflicting with APIBinding.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#return-of-reported-state","title":"Return of reported state","text":"<ul> <li>KubeStellar-Syncer return the reported state of downsynced objects at Edge cluster to the status of objects on the mailbox workspace periodically. </li> <li>TODO: Failing to returning reported state of some resources (e.g. deployment and service). Need more investigation. </li> <li>reported state returning on/off is configurable in SyncerConfig. (default is on)</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#resource-upsyncing","title":"Resource Upsyncing","text":"<ul> <li>KubeStellar-Syncer does upsyncing resources at Edge cluster to the corresponding mailbox workspace periodically. </li> <li>SyncerConfig specifies which objects should be upsynced from Edge cluster.</li> <li>object selector: group, version, kind, name, namespace (for namespaced objects), label, annotation (, and more such as ownership reference?)</li> <li>Upsyncing CRD is out of scope for now. This means when upsyncing a CR, corresponding APIBinding (not CRD) is available on the mailbox workspace. This limitation might be revisited later. </li> <li>~Upsynced objects can be accessed from APIExport set on the workload management workspace bound to the mailbox workspace (with APIBinding). This access pattern might be changed when other APIs such as summarization are provided in KubeStellar.~ =&gt; Upsynced objects are accessed through Mailbox informer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#feasibility-study","title":"Feasibility study","text":"<p>We will verify if the design described here could cover the following 4 scenarios.  - I can register a KubeStellar-Syncer on a Edge cluster to connect a mailbox workspace specified by name. (KubeStellar-Syncer registration) - I can deploy Kyverno and its policy from mailbox workspace to Edge cluster just by using manifests (generated from Kyverno helm chart) rather than using OLM. (workload deployment by KubeStellar-Syncer's downsyncing) - I can see the policy report generated at Edge cluster via API Export on workload management workspace. (resource upsyncing by KubeStellar-Syncer)  - I can deploy the denatured objects on mailbox workspace to Edge cluster by renaturing them automatically in KubeStellar-Syncer. (workload deployment by renaturing)</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#build-kubestellar-syncer-image","title":"Build KubeStellar-Syncer image","text":"<p>Prerequisite - Install ko (https://ko.build/install/)</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#how-to-build-the-image-in-your-local","title":"How to build the image in your local","text":"<ol> <li><code>make build-kubestellar-syncer-image-local</code> e.g. <pre><code>$ make build-kubestellar-syncer-image-local\n2023/04/24 11:50:37 Using base distroless.dev/static:latest@sha256:81018475098138883b80dcc9c1242eb02b53465297724b18e88591a752d2a49c for github.com/kcp-dev/edge-mc/cmd/syncer\n2023/04/24 11:50:38 Building github.com/kcp-dev/edge-mc/cmd/syncer for linux/arm64\n2023/04/24 11:50:39 Loading ko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n2023/04/24 11:50:40 Loaded ko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n2023/04/24 11:50:40 Adding tag latest\n2023/04/24 11:50:40 Added tag latest\nkubestellar-syncer image:\nko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n</code></pre> <code>ko.local/syncer-273dfcc28dbb16dfcde62c61d54e1ca9:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96</code> is the image stored in your local Docker registry.</li> </ol> <p>You can also set a shell variable to the output of this Make task.</p> <p>For example <pre><code>image=`make build-kubestellar-syncer-image-local`\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#how-to-build-the-image-with-multiple-architectures-and-push-it-to-docker-registry","title":"How to build the image with multiple architectures and push it to Docker registry","text":"<ol> <li><code>make build-kubestellar-syncer-image DOCKER_REPO=ghcr.io/yana1205/edge-mc/syncer IMAGE_TAG=dev-2023-04-24-x ARCHS=linux/amd64,linux/arm64</code></li> </ol> <p>For example <pre><code>$ make build-kubestellar-syncer-image DOCKER_REPO=ghcr.io/yana1205/edge-mc/syncer IMAGE_TAG=dev-2023-04-24-x ARCHS=linux/amd64,linux/arm64\n2023/04/24 11:50:16 Using base distroless.dev/static:latest@sha256:81018475098138883b80dcc9c1242eb02b53465297724b18e88591a752d2a49c for github.com/kcp-dev/edge-mc/cmd/syncer\n2023/04/24 11:50:17 Building github.com/kcp-dev/edge-mc/cmd/syncer for linux/arm64\n2023/04/24 11:50:17 Building github.com/kcp-dev/edge-mc/cmd/syncer for linux/amd64\n2023/04/24 11:50:18 Publishing ghcr.io/yana1205/edge-mc/syncer:dev-2023-04-24-x\n2023/04/24 11:50:19 existing blob: sha256:85a5162a65b9641711623fa747dab446265400043a75c7dfa42c34b740dfdaba\n2023/04/24 11:50:20 pushed blob: sha256:00b7b3ca30fa5ee9336a9bc962efef2001c076a3149c936b436f409df710b06f\n2023/04/24 11:50:21 ghcr.io/yana1205/edge-mc/syncer:sha256-a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510.sbom: digest: sha256:4b1407327a486c0506188b67ad24222ed7924ba57576e47b59a4c1ac73dacd40 size: 368\n2023/04/24 11:50:21 Published SBOM ghcr.io/yana1205/edge-mc/syncer:sha256-a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510.sbom\n2023/04/24 11:50:21 existing blob: sha256:930413008565fd110e7ab2d37aab538449f058e7d83e7091d1aa0930a0086f58\n2023/04/24 11:50:22 pushed blob: sha256:bd830efcc6c0a934a273202ffab27b1a8927368a7b99c4ae0cf850fadb865ead\n2023/04/24 11:50:23 ghcr.io/yana1205/edge-mc/syncer:sha256-02db9874546b79ee765611474eb647128292e8cda92f86ca1b7342012eb79abe.sbom: digest: sha256:5c79e632396b893c3ecabf6b9ba43d8f20bb3990b0c6259f975bf81c63f0e41e size: 369\n2023/04/24 11:50:23 Published SBOM ghcr.io/yana1205/edge-mc/syncer:sha256-02db9874546b79ee765611474eb647128292e8cda92f86ca1b7342012eb79abe.sbom\n2023/04/24 11:50:24 existing blob: sha256:bb5ef9628a98afa48a9133f5890c43ed1499eb82a33fe173dd9067d7a9cdfb0a\n2023/04/24 11:50:25 pushed blob: sha256:61f19080792ae91e8b37ecf003376497b790a411d7a8fa4435c7457b0e15874c\n2023/04/24 11:50:25 ghcr.io/yana1205/edge-mc/syncer:sha256-c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96.sbom: digest: sha256:8d82388bb534933d7193c661743fca8378cc561a2ad8583c0107f687acb37c1b size: 369\n2023/04/24 11:50:25 Published SBOM ghcr.io/yana1205/edge-mc/syncer:sha256-c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96.sbom\n2023/04/24 11:50:26 existing manifest: sha256:02db9874546b79ee765611474eb647128292e8cda92f86ca1b7342012eb79abe\n2023/04/24 11:50:26 existing manifest: sha256:c4759f6f841075649a22ff08bdf4afe32600f8bb31743d1aa553454e07375c96\n2023/04/24 11:50:27 ghcr.io/yana1205/edge-mc/syncer:dev-2023-04-24-x: digest: sha256:a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510 size: 690\n2023/04/24 11:50:27 Published ghcr.io/yana1205/edge-mc/syncer:dev-2023-04-24-x@sha256:a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510\necho KO_DOCKER_REPO=ghcr.io/yana1205/edge-mc/syncer ko build --platform=linux/amd64,linux/arm64 --bare --tags ./cmd/syncer\nKO_DOCKER_REPO=ghcr.io/yana1205/edge-mc/syncer ko build --platform=linux/amd64,linux/arm64 --bare --tags ./cmd/syncer\nkubestellar-syncer image\nghcr.io/yana1205/edge-mc/syncer:dev-2023-04-24-x@sha256:a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510\n</code></pre> <code>ghcr.io/yana1205/edge-mc/syncer:dev-2023-04-24-x@sha256:a52fb1cf432d321b278ac83600d3b83be3b8e6985f30e5a0f6f30c594bc42510</code> is the image pushed to the registry.</p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer/#teardown-the-environment_1","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl ws root:my-org\nkubectl kubestellar remove wmw example-wmw\nkubectl ws root\nkubectl delete workspace my-org\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Stop and uninstall KubeStellar use the following command:</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/","title":"KubeStellar Mailbox Controller","text":"<p>Required Packages:</p> MacUbuntuRHELWSL <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19<pre><code>curl -L \"https://go.dev/dl/go1.19.5.linux-$(dpkg --print-architecture).tar.gz\" -o go.tar.gz\ntar -C /usr/local -xzf go.tar.gz\nrm go.tar.gz\necho 'export PATH=$PATH:/usr/local/go/bin' &gt;&gt; /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64 &amp;&amp; chmod +x ./kind &amp;&amp; mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\nchoco install curl -y\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -n -b main https://github.com/kcp-dev/edge-mc --depth 1 KubeStellar-mailbox-controller\ncd KubeStellar-mailbox-controller\ngit restore --staged Makefile Makefile.venv go.mod docs/mkdocs.yml docs/content docs/scripts/docs-ecutable.sh\ngit checkout Makefile Makefile.venv go.mod docs/mkdocs.yml docs/content docs/scripts/docs-ecutable.sh\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/mailbox-controller.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ../\nrm -rf KubeStellar-mailbox-controller\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#linking-synctarget-with-mailbox-workspace","title":"Linking SyncTarget with Mailbox Workspace","text":"<p>For a given SyncTarget T, the mailbox controller currently chooses the name of the corresponding workspace to be the concatenation of the following:</p> <ul> <li>the ID of the logical cluster containing T</li> <li>the string \"-mb-\"</li> <li>T's UID</li> </ul> <p>The mailbox workspace gets an annotation whose key is <code>edge.kcp.io/sync-target-name</code> and whose value is the name of the workspace object (as seen in its parent workspace, the edge service provider workspace).</p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#usage","title":"Usage","text":"<p>The mailbox controller needs three Kubernetes client configurations. One --- concerned with reading inventory --- is to access the APIExport view of the <code>workload.kcp.io</code> API group, to read the <code>SyncTarget</code> objects.  This must be a client config that is pointed at the workspace (which is always <code>root</code>, as far as I know) that has this APIExport and is authorized to read its view.  Another client config is needed to give read/write access to all the mailbox workspaces, so that the controller can create <code>APIBinding</code> objects to the edge APIExport in those workspaces; this should be a client config that is able to read/write in all clusters.  For example, that is in the kubeconfig context named <code>base</code> in the kubeconfig created by <code>kcp start</code>.  Finally, the controller also needs a kube client config that is pointed at the edge service provider workspace and is authorized to consume the <code>Workspace</code> objects from there.</p> <p>The command line flags, beyond the basics, are as follows.</p> <pre><code>      --concurrency int                  number of syncs to run in parallel (default 4)\n--espw-path string                 the pathname of the edge service provider workspace (default \"root:espw\")\n--inventory-cluster string         The name of the kubeconfig cluster to use for access to APIExport view of SyncTarget objects\n      --inventory-context string         The name of the kubeconfig context to use for access to APIExport view of SyncTarget objects (default \"root\")\n--inventory-kubeconfig string      Path to the kubeconfig file to use for access to APIExport view of SyncTarget objects\n      --inventory-user string            The name of the kubeconfig user to use for access to APIExport view of SyncTarget objects\n\n--mbws-cluster string              The name of the kubeconfig cluster to use for access to mailbox workspaces (really all clusters)\n--mbws-context string              The name of the kubeconfig context to use for access to mailbox workspaces (really all clusters) (default \"base\")\n--mbws-kubeconfig string           Path to the kubeconfig file to use for access to mailbox workspaces (really all clusters)\n--mbws-user string                 The name of the kubeconfig user to use for access to mailbox workspaces (really all clusters)\n--server-bind-address ipport       The IP address with port at which to serve /metrics and /debug/pprof/ (default :10203)\n--workload-cluster string          The name of the kubeconfig cluster to use for access to edge service provider workspace\n      --workload-context string          The name of the kubeconfig context to use for access to edge service provider workspace\n      --workload-kubeconfig string       Path to the kubeconfig file to use for access to edge service provider workspace\n      --workload-user string             The name of the kubeconfig user to use for access to edge service provider workspace\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#try-out-the-mailbox-controller","title":"Try out the mailbox controller","text":""},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#pull-the-kcp-and-kubestellar-source-code-build-the-kubectl-ws-binary-and-start-kcp","title":"Pull the kcp and KubeStellar source code, build the kubectl-ws binary, and start kcp","text":"<p>Open a terminal window(1) and clone the latest KubeStellar source:</p> <pre><code>git clone https://github.com/kcp-dev/edge-mc KubeStellar\n</code></pre> <p>Clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> Build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>cd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Run kcp (kcp will spit out tons of information and stay running in this terminal window) <pre><code>kcp start &amp;&gt; /dev/null &amp;\nsleep 30\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#create-the-edge-service-provider-workspace-espw","title":"Create the Edge Service Provider Workspace (ESPW)","text":"<p>Open another terminal window(2) and point <code>$KUBECONFIG</code> to the admin kubeconfig for the kcp server and include the location of kubectl-ws in <code>$PATH</code>.</p> <pre><code>export KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\nexport PATH=$(pwd)/bin:$PATH\n</code></pre> <p>Next, create the edge service provider workspace:</p> <p>Use workspace <code>root:espw</code> as the Edge Service Provider Workspace (ESPW). <pre><code>kubectl ws root\nkubectl ws create espw\n</code></pre></p> <p>After that, a run of the controller should look like the following.</p> <pre><code>kubectl ws root:espw\ncd ../KubeStellar\ngo run ./cmd/mailbox-controller -v=2 &amp;\nsleep 45\n</code></pre> <pre><code>I0305 18:06:20.046741   85556 main.go:110] \"Command line flag\" add_dir_header=\"false\"\nI0305 18:06:20.046954   85556 main.go:110] \"Command line flag\" alsologtostderr=\"false\"\nI0305 18:06:20.046960   85556 main.go:110] \"Command line flag\" concurrency=\"4\"\nI0305 18:06:20.046965   85556 main.go:110] \"Command line flag\" inventory-context=\"root\"\nI0305 18:06:20.046971   85556 main.go:110] \"Command line flag\" inventory-kubeconfig=\"\"\nI0305 18:06:20.046976   85556 main.go:110] \"Command line flag\" log_backtrace_at=\":0\"\nI0305 18:06:20.046980   85556 main.go:110] \"Command line flag\" log_dir=\"\"\nI0305 18:06:20.046985   85556 main.go:110] \"Command line flag\" log_file=\"\"\nI0305 18:06:20.046989   85556 main.go:110] \"Command line flag\" log_file_max_size=\"1800\"\nI0305 18:06:20.046993   85556 main.go:110] \"Command line flag\" logtostderr=\"true\"\nI0305 18:06:20.046997   85556 main.go:110] \"Command line flag\" one_output=\"false\"\nI0305 18:06:20.047002   85556 main.go:110] \"Command line flag\" server-bind-address=\":10203\"\nI0305 18:06:20.047006   85556 main.go:110] \"Command line flag\" skip_headers=\"false\"\nI0305 18:06:20.047011   85556 main.go:110] \"Command line flag\" skip_log_headers=\"false\"\nI0305 18:06:20.047015   85556 main.go:110] \"Command line flag\" stderrthreshold=\"2\"\nI0305 18:06:20.047019   85556 main.go:110] \"Command line flag\" v=\"2\"\nI0305 18:06:20.047023   85556 main.go:110] \"Command line flag\" vmodule=\"\"\nI0305 18:06:20.047027   85556 main.go:110] \"Command line flag\" workload-context=\"\"\nI0305 18:06:20.047031   85556 main.go:110] \"Command line flag\" workload-kubeconfig=\"\"\nI0305 18:06:20.070071   85556 main.go:247] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/root/workload.kcp.io\"\nI0305 18:06:20.072088   85556 shared_informer.go:282] Waiting for caches to sync for mailbox-controller\nI0305 18:06:20.172169   85556 shared_informer.go:289] Caches are synced for mailbox-controller\nI0305 18:06:20.172196   85556 main.go:210] \"Informers synced\"\n</code></pre> <p>In a separate terminal window(3), create an inventory management workspace as follows.</p> <pre><code>kubectl ws \\~\nkubectl ws create imw --enter\n</code></pre> <p>Then in that workspace, run the following command to create a <code>SyncTarget</code> object.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\n  name: stest1\nspec:\n  cells:\n    foo: bar\nEOF\n</code></pre> <p>That should provoke logging like the following from the mailbox controller.</p> <pre><code>I0305 18:07:20.490417   85556 main.go:369] \"Created missing workspace\" worker=0 mbwsName=\"niqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368\"\n</code></pre> <p>And you can verify that as follows:</p> <p><pre><code>kubectl ws root:espw\n</code></pre> <pre><code>Current workspace is \"root:espw\".\n</code></pre></p> <p><pre><code>kubectl get workspaces\n</code></pre> <pre><code>NAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\nniqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368   universal            Ready   https://192.168.58.123:6443/clusters/0ay27fcwuo2sv6ht   22s\n</code></pre></p> <p>FYI, if you look inside that workspace you will see an <code>APIBinding</code> named <code>bind-edge</code> that binds to the <code>APIExport</code> named <code>edge.kcp.io</code> from the edge service provider workspace (and this is why the controller needs to know the pathname of that workspace), so that the edge API is available in the mailbox workspace.</p> <p>Next, <code>kubectl delete</code> that workspace, and watch the mailbox controller wait for it to be gone and then re-create it.</p> <pre><code>I0305 18:08:15.428884   85556 main.go:369] \"Created missing workspace\" worker=2 mbwsName=\"niqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368\"\n</code></pre> <p>Finally, go back to your inventory workspace to delete the <code>SyncTarget</code>:</p> <p><pre><code>kubectl ws \\~\nkubectl ws imw\nkubectl delete SyncTarget stest1\n</code></pre> and watch the mailbox controller react as follows.</p> <pre><code>I0305 18:08:44.380421   85556 main.go:352] \"Deleted unwanted workspace\" worker=0 mbwsName=\"niqdko2g2pwoadfb-mb-f99e773f-3db2-439e-8054-827c4ac55368\"\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl ws root:my-org\nkubectl kubestellar remove wmw example-wmw\nkubectl ws root\nkubectl delete workspace my-org\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Stop and uninstall KubeStellar use the following command:</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/outline/","title":"Details","text":"<p>Want to get involved? Check out our good-first-issue list.</p> <p></p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#status-of-this-memo","title":"Status of this memo","text":"<p>This summarizes the current state of design work that is still in progress.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#introduction","title":"Introduction","text":"<p>This is a quick demo of a fragment of what we think is needed for edge multi-cluster.  It is intended to demonstrate the following points.</p> <ul> <li>Separation of infrastructure and workload management.</li> <li>The focus here is on workload management, and that strictly reads   an inventory of infrastructure.</li> <li>What passes from inventory to workload management is kcp TMC   Location and SyncTarget objects.</li> <li>Use of a kcp workspace as the container for the central spec of a workload.</li> <li>Propagation of desired state from center to edge, as directed by   EdgePlacement objects and the Location and SyncTarget objects they reference.</li> <li>Interfaces designed for a large number of edge clusters.</li> <li>Interfaces designed with the intention that edge clusters operate   independently of each other and the center (e.g., can tolerate only   occasional connectivity) and thus any \"service providers\" (in the   technical sense from kcp) in the center or elsewhere.</li> <li>Rule-based customization of desired state.</li> <li>Propagation of reported state from edge to center.</li> <li>Summarization of reported state in the center.</li> <li>Return and/or summarization of state from associated objects (e.g.,   ReplicaSet or Pod objects associated with a given Deployment   object).</li> <li>The edge opens connections to the center, not vice-versa.</li> <li>An edge computing platform \"product\" that can be deployed (as   opposed to a service that is used).</li> </ul> <p>Some important things that are not attempted in this PoC include the following.</p> <ul> <li>An implementation that supports a large number of edge clusters or   any other thing that requires sharding for scale. In this PoC we   will use a single kcp server to hold all the workspaces, and will   not shard any controller.</li> <li>More than one SyncTarget per Location.</li> <li>A hierarchy with more than two levels.</li> <li>User control over ordering of propagation from center to edge,   either among destinations or kinds of objects.</li> <li>More than baseline security (baseline being, e.g., HTTPS, Secret   objects, non-rotating bearer token based service authentication).</li> <li>A good design for bootstrapping the workload management in the edge   clusters.</li> <li>Very strong isolation between tenants in the edge computing   platform.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#development-roadmap","title":"Development Roadmap","text":"<p>Some features will get implemented later than others, so that we can start being able to run interesting end-to-end scenarios relatively soon.  Following is a list of features that will not be implemented at first.</p> <p>Of the following features, customization will be needed before the others.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#customization","title":"Customization","text":"<p>We can have a complete system that ignores customization, as long as it is only used for workloads that need no customization.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#summarization","title":"Summarization","text":"<p>We can omit summarization at first.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#return-andor-summarization-of-state-from-associated-objects","title":"Return and/or summarization of state from associated objects","text":"<p>This will involve both defining a scalable interface for declaring what should be returned as well as implementing it.  This will certainly affect the syncer between mailbox workspace and edge cluster, and the summarization part will affect the status summarizer.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#good-handling-of-workload-conflicts","title":"Good handling of workload conflicts","text":"<p>We could start by handling workload conflicts in a very simple way: treating each as an error.  Later development can handle them better, as outlined later.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#denaturingrenaturing","title":"Denaturing/renaturing","text":"<p>We could start by not doing this.  For some resources, the effect of leaving these resources natured in the center is only to add authorizations in the center that are not needed and are undesired in a well-secured environment but tolerable in early demonstrations --- provided that there is not a conflict with an object of the same name that is positively desired in the center.  In particular, these are: <code>ClusterRole</code>, <code>ClusterRoleBinding</code>, <code>Role</code>, <code>RoleBinding</code>, and, depending on the Kubernetes release and usage style, <code>ServiceAccount</code>. The extra consideration for <code>ServiceAccount</code> is when an associated <code>Secret</code> is a natural consequence.  However, that is not a practical problem because such <code>Secret</code> objects are recognized as system infrastructure (see below).  Another consideration for <code>ServiceAccount</code> objects, as for <code>Secret</code> and <code>ConfigMap</code> objects, is that some are in some sense \"reverse-natured\": some are created by some other thing as part of the nature of that other thing (object or external system).  Another way of looking at these particular objects is that they are system infrastructure.</p> <p>For some kinds of object, lack of denaturing/renaturing means that KubeStellar will simply not be able to support workloads that contain such objects.  These are: <code>MutatingWebhookConfiguration</code>, <code>ValidatingWebhookConfiguration</code>, <code>LimitRange</code>, <code>ResourceQuota</code>.</p> <p>For some resources, the need to denature is only a matter of anticipation.  <code>FlowSchema</code> and <code>PriorityLevelConfiguration</code> currently are not interpreted by kcp servers and so are effectively already denatured there.  Hopefully they will be given interpretations in the future, and then those resources will join the previous category.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#roles-and-responsibilities","title":"Roles and Responsibilities","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#developersdeployersadminsusers-of-the-infrastructure-management-layer","title":"Developers/deployers/admins/users of the infrastructure management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#developers-of-the-workload-management-layer","title":"Developers of the workload management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#deployersadmins-of-the-workload-management-layer","title":"Deployers/admins of the workload management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#users-of-the-workload-management-layer","title":"Users of the workload management layer","text":""},{"location":"Coding%20Milestones/PoC2023q1/outline/#design-overview","title":"Design overview","text":"<p>In very brief: the design approach is to achieve the multicast semantics of edge placement by two layers of activity.  Between the two layers sit mailbox workspaces: these exist in the center, and there is one for each edge cluster.  One layer of activity runs in the center and relates the edge placement problems to mailbox workspace contents.  The other layer is syncers, one in each edge cluster, that relate the corresponding mailbox contents with their local clusters.</p> <p>As in TMC, in this design we have downsync and upsync --- but they are a little more complicated here.  Downsync involves propagation of desired state from workload management workspace through mailbox workspaces to edge and return/summarization of reported state.  Upsync involves return/summarization of desired and reported state of objects born on the edge clusters.  On the inward path, the reported or full state goes from edge to the mailbox workspace and then is summarized to the workload management workspace.  State propagation is maintained in an eventually consistent way, it is not just one-and-done.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#inventory-management-workspaces","title":"Inventory Management workspaces","text":"<p>In this design the primary interface between infrastructure management and workload management is API objects in inventory management workspaces.  We abuse the <code>Location</code> and <code>SyncTarget</code> object types from kcp TMC for this purpose.  The people doing infrastructure management are responsible for creating the inventory management workspaces and populating them with <code>Location</code> and <code>SyncTarget</code> objects, one <code>Location</code> and one <code>SyncTarget</code> per edge cluster.  These inventory management workspaces need to use APIBindings to APIExports defining <code>Location</code> and <code>SyncTarget</code> so that the workload management layer can use one APIExport view for each API group to read those objects.</p> <p>To complete the plumbing of the syncers, each inventory workspace that contains a SyncTarget needs to also contain the following associated objects.  FYI, these are the things that <code>kubectl kcp workload sync</code> directly creates besides the SyncTarget.  Ensuring their presence is part of the problem of bootstrapping the workload management layer and is not among the things that this PoC takes a position on.</p> <ol> <li>A ServiceAccount that the syncer will authenticate as.</li> <li>A ClusterRole manipulating that SyncTarget and the    APIResourceImports (what are these?).</li> <li>A ClusterRoleBinding that links that ServiceAccount with that    ClusterRole.</li> </ol>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#edge-service-provider-workspace","title":"Edge Service Provider workspace","text":"<p>The edge multi-cluster service is provided by one workspace that includes the following things.</p> <ul> <li>An APIExport of the edge API group.</li> <li>The edge controllers: scheduler, placement translator, mailbox   controller, and status summarizer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#workload-management-workspaces","title":"Workload Management workspaces","text":"<p>The users of edge multi-cluster primarily maintain these.  Each one of these has both control (API objects that direct the behavior of the edge computing platform) and data (API objects that hold workload desired and reported state).</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#data-objects","title":"Data objects","text":"<p>The workload desired state is represented by kube-style API objects, in the way that is usual in the Kubernetes milieu.  For edge computing we need to support both cluster-scoped (AKA non-namespaced) kinds as well as namespaced kinds of objects.</p> <p>The use of a workspace as a mere container presents a challenge, because some kinds of kubernetes API objects at not merely data but also modify the behavior of the apiserver holding them.  To resolve this dilemma, the edge users of such a workspace will use a special view of the workspace that holds only data objects.  The ones that modify apiserver behavior will be translated by the view into \"denatured\" versions of those objects in the actual workspace so that they have no effect on it.  And for these objects, the transport from center-to-edge will do the inverse: translate the denatured versions into the regular (\"natured\"?) versions for appearance in the edge cluster.  Furthermore, for some kinds of objects that modify apiserver behavior we want them \"natured\" at both center and edge.  There are thus a few categories of kinds of objects.  Following is a listing, with with the particular kinds that appear in kcp or plain kubernetes.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#needs-to-be-denatured-in-center-natured-in-edge","title":"Needs to be denatured in center, natured in edge","text":"<p>For these kinds of objects: clients of the real workload management workspace can manipulate some such objects that will modify the behavior of the workspace, while clients of the edge computing view will manipulate distinct objects that have no effect on the behavior of the workspace.  These are kinds of objects to which kcp normally associates some behavior.  To be fully precise, the concern here is with behavior that is externally visible (including externally visible behavior of the server itself); we do not care to dissociate server-internal behavior such as storing encrypted at rest.  The edge computing platform will have to implement that view which dissociates the normal kcp behavior.</p> APIVERSION KIND NAMESPACED admissionregistration.k8s.io/v1 MutatingWebhookConfiguration false admissionregistration.k8s.io/v1 ValidatingWebhookConfiguration false flowcontrol.apiserver.k8s.io/v1beta2 FlowSchema false flowcontrol.apiserver.k8s.io/v1beta2 PriorityLevelConfiguration false rbac.authorization.k8s.io/v1 ClusterRole false rbac.authorization.k8s.io/v1 ClusterRoleBinding false rbac.authorization.k8s.io/v1 Role true rbac.authorization.k8s.io/v1 RoleBinding true v1 LimitRange true v1 ResourceQuota true v1 ServiceAccount true"},{"location":"Coding%20Milestones/PoC2023q1/outline/#needs-to-be-natured-in-center-and-edge","title":"Needs to be natured in center and edge","text":"<p>These should have their usual effect in both center and edge; they need no distinct treatment.</p> <p>Note, however, that they do have some sequencing implications.  They have to be created before any dependent objects, deleted after all dependent objects.</p> APIVERSION KIND NAMESPACED apiextensions.k8s.io/v1 CustomResourceDefinition false v1 Namespace false"},{"location":"Coding%20Milestones/PoC2023q1/outline/#needs-to-be-natured-in-center-not-destined-for-edge","title":"Needs to be natured in center, not destined for edge","text":"APIVERSION KIND NAMESPACED apis.kcp.io/v1alpha1 APIBinding false <p>A workload management workspace needs an APIBinding to the APIExport of the API group <code>edge.kcp.io</code> from the edge service provider workspace, in order to be able to contain EdgePlacement and related objects.  These objects and that APIBinding are not destined for the edge clusters.</p> <p>The edge clusters are not presumed to be kcp workspaces, so APIBindings do not propagate to the edge clusters.  However, it is possible that APIBindings for workload APIs may exist in a workload management workspace and be selected for downsync to mailbox workspaces while the edge clusters have the same resources defined by CRDs (as mentioned later in the discussion of built-in resources and namespaces).</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#for-features-not-supported","title":"For features not supported","text":"<p>These are part of k8s or kcp APIs that are not supported by the edge computing platform.</p> APIVERSION KIND NAMESPACED apiregistration.k8s.io/v1 APIService false apiresource.kcp.io/v1alpha1 APIResourceImport false apiresource.kcp.io/v1alpha1 NegotiatedAPIResource false apis.kcp.io/v1alpha1 APIConversion false <p>The APIService objects are of two sorts: (a) those that are built-in and describe object types built into the apiserver and (b) those that are added by admins to add API groups served by custom external servers.  Sort (b) is not supported because this PoC does not support custom external servers in the edge clusters.  Sort (a) is not programmable in this PoC, but it might be inspectable.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#not-destined-for-edge","title":"Not destined for edge","text":"<p>These kinds of objects are concerned with either (a) TMC control or (b) workload data that should only exist in the edge clusters.  These will not be available in the view used by edge clients to maintain their workload desired and reported state.</p> APIVERSION KIND NAMESPACED apis.kcp.io/v1alpha1 APIExport false apis.kcp.io/v1alpha1 APIExportEndpointSlice false apis.kcp.io/v1alpha1 APIResourceSchema false apps/v1 ControllerRevision true authentication.k8s.io/v1 TokenReview false authorization.k8s.io/v1 LocalSubjectAccessReview true authorization.k8s.io/v1 SelfSubjectAccessReview false authorization.k8s.io/v1 SelfSubjectRulesReview false authorization.k8s.io/v1 SubjectAccessReview false certificates.k8s.io/v1 CertificateSigningRequest false coordination.k8s.io/v1 Lease true core.kcp.io/v1alpha1 LogicalCluster false core.kcp.io/v1alpha1 Shard false events.k8s.io/v1 Event true scheduling.kcp.io/v1alpha1 Location false scheduling.kcp.io/v1alpha1 Placement false tenancy.kcp.io/v1alpha1 ClusterWorkspace false tenancy.kcp.io/v1alpha1 Workspace false tenancy.kcp.io/v1alpha1 WorkspaceType false topology.kcp.io/v1alpha1 Partition false topology.kcp.io/v1alpha1 PartitionSet false v1 Binding true v1 ComponentStatus false v1 Event true v1 Node false workload.kcp.io/v1alpha1 SyncTarget false"},{"location":"Coding%20Milestones/PoC2023q1/outline/#already-denatured-in-center-want-natured-in-edge","title":"Already denatured in center, want natured in edge","text":"<p>These are kinds of objects that kcp already gives no interpretation to, and that is what KubeStellar needs from the center workspaces.</p> <p>This is the default category of kind of object --- any kind of data object not specifically listed in another category is implicitly in this category.  Following are the kinds from k8s and kcp that fall in this category.</p> APIVERSION KIND NAMESPACED apps/v1 DaemonSet true apps/v1 Deployment true apps/v1 ReplicaSet true apps/v1 StatefulSet true autoscaling/v2 HorizontalPodAutoscaler true batch/v1 CronJob true batch/v1 Job true networking.k8s.io/v1 Ingress true networking.k8s.io/v1 IngressClass false networking.k8s.io/v1 NetworkPolicy true node.k8s.io/v1 RuntimeClass false policy/v1 PodDisruptionBudget true scheduling.k8s.io/v1 PriorityClass false storage.k8s.io/v1 CSIDriver false storage.k8s.io/v1 CSINode false storage.k8s.io/v1 CSIStorageCapacity true storage.k8s.io/v1 StorageClass false storage.k8s.io/v1 VolumeAttachment false v1 ConfigMap true v1 Endpoints true v1 PersistentVolume false v1 PersistentVolumeClaim true v1 Pod true v1 PodTemplate true v1 ReplicationController true v1 Secret true v1 Service true <p>Note that some <code>ConfigMap</code> and <code>Secret</code> objects are treated differently, as explained in the next section.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#system-infrastructure-objects","title":"System infrastructure objects","text":"<p>Even in a kcp workspace, some certain objects --- called here \"system infrastructure objects\" --- are created as a consequence of certain other objects or things.  The system infrastructure objects are tolerated in the center and do not propagate toward the edge.  Here is an initial list of system infrastructure objects:</p> <ul> <li><code>Secret</code> objects whose type is   <code>kubernetes.io/service-account-token</code> (these are automatically   created to support a <code>ServiceAccount</code> in some circumstances) or   <code>bootstrap.kubernetes.io/token</code>;</li> <li><code>ConfigMap</code> objects named <code>kube-root-ca.crt</code>;</li> <li><code>ServiceAcount</code> objects named <code>default</code> (these are automatically   created as a consequence of a namespace being created).</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#built-in-resources-and-namespaces","title":"Built-in resources and namespaces","text":"<p>An edge cluster has some built-in resources (i.e, kinds of objects) and namespaces.  A resource may be built-in by any of several ways: it can be built-in to the apiserver, it can be defined by a CRD, its API group can be delegated by an APIService to a custom external server (each of the latter two is sometimes called \"aggregation\").  Note also that a resource may be defined in edge clusters one way (e.g., by being built into kube-apiserver) and in the workload management workspace another way (e.g., by a CustomResourceDefinition).</p> <p>In this PoC, all edge clusters are considered to have the same built-in resources and namespaces.</p> <p>As a matter of scoping the work here, it is also assumed that each API group built into the edge clusters supports the API versions chosen by the conflict resolution rules below when they are applied to the workload sources.</p> <p>At deployment time the workload management platform is configured with lists of resources and namespaces built into the edge clusters.</p> <p>Propagation from center to edge does not attempt to manage the resource and namespace definitions that are built into the edge clusters.</p> <p>The mailbox workspaces will have built-in resources and namespaces that are a subset of those built into the edge clusters.  The propagation from workload management workspace to mailbox workspace does not attempt to manage the resource and namespace definitions that are built into the mailbox workspaces.</p> <p>The above wording is deliberately restrained, for the sake of flexibility regarding resources that are defined one way in the edge clusters and another way in workload management workspace.  For example, the following scenario is allowed.</p> <ul> <li>Some central team owns an API group and produces some   CustomResourceDefinition (CRD) objects that populate that API group.</li> <li>That team derives APIResourceSchemas from those CRDs and a   corresponding APIExport of their API group.</li> <li>That team maintains a kcp workspace holding those APIResourceSchemas   and that APIExport.</li> <li>Some workload management workspaces have APIBindings to that   APIExport, and EdgePlacement objects that (1) select those   APIBinding objects for downsync and (2) select objects of kinds   defined through those APIBindings for either downsync or upsync.</li> <li>Those resources are built into the edge clusters by pre-deploying   the aforementioned CRDs there.</li> <li>Those resources are not built into the mailbox workspaces.  In   this case the APIBindings would propagate from workload management   workspace to mailbox workspaces but not edge clusters.</li> <li>As a consequence of those propagated APIBindings, the APIExport's   view includes all of the objects (in workload management workspaces,   in mailbox workspaces, and in any other workspaces where they   appear) whose kind is defined through those APIBindings.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#control-objects","title":"Control objects","text":"<p>These are the EdgePlacement objects, their associated SinglePlacementSlice objects, and the objects that direct customization and summarization.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#edgeplacement-objects","title":"EdgePlacement objects","text":"<p>One of these is a binding between a \"what\" predicate and a \"where\" predicate.</p> <p>Overlaps between EdgePlacement objects are explicitly allowed.  Two EdgePlacement objects may have \"where\" predicates that both match some of the same destinations.  Two EdgePlacement objects may have \"what\" predicates that match some of the same workload descriptions.  Two EdgePlacement objects may overlap in both ways.</p> <p>An EdgePlacement object deliberately only binds \"what\" and \"where\", without any adverbs (such as prescriptions of customization or summarization).  This means that overlapping EdgePlacement objects can not conflict in those adverbs.</p> <p>However, another sort of conflict remains possible.  This is because the user controls the IDs --- that is, the names --- of the parts of the workload.  In full, a Kubernetes API object is identified by API group, API major version, Kind (equivalently, resource name), namespace if relevant, and name.  For simplicity in this PoC we will not worry about differences in API major version; each API group in Kubernetes and/or kcp currently has only one major version.</p> <p>Two different workload descriptions can have objects with the same ID (i.e., if they appear in different workspaces).  These objects, when rendered to the same API version, might have different values.  And the objects may be available in different API versions in different source workspaces.  See client-go for what an API server says about which versions it can serve for a given API group, and meta/v1 for the supporting details on an APIGroup struct.</p> <p>When multiple workload objects with the same APIGroup, Kind, namespace (if namespaced), and name are directed to the same edge cluster, they are merged with conflicts handled by (a) a rule for resolution and (b) reporting via both error logging and Kubernetes Event creation.  These conflicts are serious matters: they mean user expectations are not being met (because they are inconsistent); this is why the placement translator tries hard to make the user aware.</p> <p>The first part of merging a set of objects is to read them all at the same API version.  The placement translator solves the problem of picking API version at the level of API groups rather than object-by-object.  The API version for an given API group is chosen as follows.  First, take the intersection of the supported versions from the various sources.  If this intersection is empty then this is a conflict.  It is resolved by throwing out the APIGroup with the lowest version and repeating with the reduced set of APIGroup structs. Next, take the union of the preferred versions. If this union has a non-empty intersection with the intersection of the supported versions, take the following steps with this intersection; otherwise proceed with just the intersection of the supported versions.  When first (since process startup) presented with an instance of this problem, the placement translator picks the highest version from this intersection.  Subsequently for the same API group, the placement translator sticks with its previous decision as long as that is still in the intersection.  If the previous choice is no longer available, the highest version is picked.  This preference for highest version is based on the expectation that rolling forward will be more common than rolling back; using the intersection ensures that both work (as long as the collection of sources has an overlap in supported versions, which is basic sanity).</p> <p>A workload prescription object that is in the process of graceful deletion (i.e., with <code>DeletionTimestamp</code> set to something) is considered here to already be gone.</p> <p>Once they have been read at a consistent API version, merging of multiple objects is done as follows.  Different parts of the object are handled differently.</p> <ul> <li>TypeMeta.  This can not conflict because it is part of what   identifies an object.</li> <li>ObjectMeta.</li> <li>Labels and Annotations.  These are handled on a key-by-key     basis.  Distinct keys do not conflict.  When multiple objects have     a label or annotation with the same key, the corresponding value     in the result is the value from the most recently updated of those     objects.</li> <li>OwnerReferences.  This is handled analogously to labels and     annotations.  The key is the combination of APIVersion, Kind, and     Name.</li> <li>Finalizers.  This is simply a set of strings.  The result of     merging is the union of the sets.</li> <li>ManagedFields.  This is metadata that is not propagated.</li> <li>Spec.  Beyond TypeMeta and ObjectMeta, the remaining object   fields are specific to the kind of object.  Many have a field named   \"Spec\" in the Go language source, \"spec\" in the JSON representation.   For objects that have Spec fields, merging has a conflict if those   field values are not all equal when considered as JSON data, and the   resolution is to take the value from the most recently updated   object.</li> <li>Status.  Status is handled analogously to Spec.  For both, we   consider a missing field to be the same as a field with a value of   <code>nil</code>.  That is expected to be the common case for the Status of   these workload prescription objects.</li> <li>Other fields.  If all the values are maps (objects in   JavaScript) then they are merged key-by-key, as for labels and   annotations.  Otherwise they are treated as monoliths, as for Spec   and Status.</li> </ul> <p>For the above, the most recently updated object is determined by parsing the ResourceVersion as an <code>int64</code> and picking the highest value.  This is meaningful under the assumption that all the source workspaces are from the same kcp server --- which will be true for this PoC but is not a reasonable assumption in general.  Also: interpreting ResourceVersion breaks a rule for Kubernetes clients --- but this is dismayingly common.  Beyond this PoC we could hope to do better by looking at the ManagedFields.  But currently kcp does not include https://github.com/kubernetes/kubernetes/pull/110058 so the ManagedFields often do not properly reflect the update times.  Even so, those timestamps have only 1-second precision --- so conflicts will remain possible (although, hopefully, unlikely).</p> <p>There is special handling for Namespace objects.  When a workload includes namespaced objects, the propagation has to include ensuring that the corresponding Namespace object exists in the destination.  An EdgePlacement's \"what\" predicate MAY fail to match a relevant Namespace object.  This is taken to mean that this EdgePlacement is not requesting propagation of the details (Spec, labels, etc.) of that Namespace object but only expects propagation to somehow ensure that the namespace exists.  When merging overlapping workloads that have namespaces in common, only the Namespace objects that come from matching a \"what\" predicate need to be merged.</p> <p>The above also provide an answer to the question of what version is used when writing to the mailbox workspace and edge cluster.  The version used for that is the version chosen above.  In the case of no conflicts, this means that the writes are done using the preferred version from the API group from the workload management workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#mailbox-workspaces","title":"Mailbox workspaces","text":"<p>The mailbox controller maintains one mailbox workspace for each SyncTarget.  A mailbox workspace acts as a workload source for the corresponding syncer, prescribing the workload to go to the corresponding edge cluster and the <code>SyncerConfig</code> object that guides the syncer.</p> <p>A mailbox workspace contains the following items.</p> <ol> <li>APIBindings (maintained by the mailbox controller) to APIExports of    workload object types.</li> <li>Workload objects, post customization in the case of downsynced    objects.</li> <li>A <code>SyncerConfig</code> object.</li> </ol>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#edge-cluster","title":"Edge cluster","text":"<p>Also called edge pcluster.</p> <p>One of these contains the following items.  FYI, these are the things in the YAML output by <code>kubectl kcp workload edge-sync</code>.  The responsibility for creating and maintaining these objects is part of the problem of bootstrapping the workload management layer and is not among the things that this PoC takes a position on.</p> <ul> <li>A namespace that holds the syncer and associated objects.</li> <li>A ServiceAccount that the syncer authenticates as when accessing the   views of the center and when accessing the edge cluster.</li> <li>A Secret holding that ServiceAccount's authorization token.</li> <li>A ClusterRole listing the non-namespaced privileges that the   syncer will use in the edge cluster.</li> <li>A ClusterRoleBinding linking the syncer's ServiceAccount and ClusterRole.</li> <li>A Role listing the namespaced privileges that the syncer will use in   the edge cluster.</li> <li>A RoleBinding linking the syncer's ServiceAccount and Role.</li> <li>A Secret holding the kubeconfig that the syncer will use to access   the edge cluster.</li> <li>A Deployment of the syncer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#mailbox-controller","title":"Mailbox Controller","text":"<p>This controller maintains one mailbox workspace per SyncTarget.  Each of these mailbox workspaces is used for a distinct syncing problem: downsynced objects go here from their workload management workspaces, and upsynced objects go here from the edge cluster.  These workspaces are all children of the edge service provider workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#kubestellar-scheduler","title":"KubeStellar Scheduler","text":"<p>This controller monitors the EdgePlacement, Location, and SyncTarget objects and maintains the results of matching.  For each EdgePlacement object this controller maintains an associated collection of SinglePlacementSlice objects holding the matches for that EdgePlacement.  These SinglePlacementSlice objects appear in the same workspace as the corresponding EdgePlacement; the remainder of how they are linked is TBD.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#placement-translator","title":"Placement Translator","text":"<p>This controller continually monitors all the EdgePlacement objects, SinglePlacementSlice objects, and related workload objects, and maintains the proper projections of those into mailbox workspace contents.  The customization, if any, is done in this process.  Note also that everything that has to be denatured in the workload management workspace also has to be denatured in the mailbox workspace.</p> <p>The job of the placement translator can be broken down into the following five parts.</p> <ul> <li>Resolve each EdgePlacement's \"what\" part to a list of particular   workspace items (namespaces and non-namespaced objects).</li> <li>Monitor the SinglePlacementSlice objects that report the scheduler's   resolutions of the \"where\" part of the EdgePlacement objects.</li> <li>Maintain the association between the resolved \"where\" from the   scheduler and the resolved what.</li> <li>Maintain the copies, with customization, of the workload objects   from source workspace to mailbox workspaces.</li> <li>Maintain the SyncerConfig object in each mailbox workspace to direct   the corresponding syncer.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#syncers","title":"Syncers","text":"<p>In this PoC there is a 1:1:1 relation between edge cluster, mailbox workspace, and syncer.  The syncer runs in the edge cluster and does downsync from and upsync to the mailbox workspace.  The syncer monitors a SyncerConfig object in the mailbox workspace to know what to downsync and upsync.</p> <p>For those familiar with kcp's TMC syncer, note that the edge syncer differs in the following ways.</p> <ul> <li>Create self-sufficient edge clusters.</li> <li>Re-nature objects that KubeStellar forcibly denatures at the center.</li> <li>Return reported state from associated objects.</li> <li>Does not access the SyncTarget object.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#status-summarizer","title":"Status Summarizer","text":"<p>For each EdgePlacement object and related objects this controller maintains the directed status summary objects.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#usage-scenario","title":"Usage Scenario","text":"<p>The usage scenario breaks, at the highest level, into two parts: inventory and workload.</p>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#inventory-usage","title":"Inventory Usage","text":"<p>A user with infrastructure authority creates one or more inventory management workspaces.  Each such workspace needs to have the following items, which that user will create if they are not pre-populated by the workspace type.</p> <ul> <li>An APIBinding to the <code>workload.kcp.io</code> APIExport to get   <code>SyncTarget</code>.</li> <li>An APIBinding to the <code>scheduling.kcp.io</code> APIExport to get   <code>Location</code>.</li> <li>A ServiceAccount (with associated token-bearing Secret) (details   TBD) that the mailbox controller authenticates as.</li> <li>A ClusterRole and ClusterRoleBinding that authorize said   ServiceAccount to do what the mailbox controller needs to do.</li> </ul> <p>This user also creates one or more edge clusters.</p> <p>For each of those edge clusters, this user creates the following.</p> <ul> <li>a corresponding SyncTarget, in one of those inventory management   workspaces;</li> <li>a Location, in the same workspace, that matches only that   SyncTarget.</li> </ul>"},{"location":"Coding%20Milestones/PoC2023q1/outline/#workload-usage","title":"Workload usage","text":"<p>A user with workload authority starts by creating one or more workload management workspaces.  Each needs to have the following, which that user creates if the workload type did not already provide.</p> <ul> <li>An APIBinding to the APIExport of <code>edge.kcp.io</code> from the edge   service provider workspace.</li> <li>For each of the Scheduler, the Placement Translator, and the   Status Summarizer:</li> <li>A ServiceAccount for that controller to authenticate as;</li> <li>A ClusterRole granting the privileges needed by that controller;</li> <li>A ClusterRoleBinding that binds those two.</li> </ul> <p>This user also uses the edge-workspace-as-container view of each such workspace to describe the workload desired state.</p> <p>This user creates one or more EdgePlacement objects to say which workload goes where.  These may be accompanied by API objects that specify rule-based customization, specify how status is to be summarized.</p> <p>The KubeStellar implementation propagates the desired state from center to edge and collects the specified information from edge to center.</p> <p>The edge user monitors status summary objects in their workload management workspaces.</p> <p>The status summaries may include limited-length lists of broken objects.</p> <p>Full status from the edge is available in the mailbox workspaces.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/","title":"KubeStellar Placement Translator","text":"<p>Required Packages:</p> MacUbuntuRHELWSL <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19<pre><code>curl -L \"https://go.dev/dl/go1.19.5.linux-$(dpkg --print-architecture).tar.gz\" -o go.tar.gz\ntar -C /usr/local -xzf go.tar.gz\nrm go.tar.gz\necho 'export PATH=$PATH:/usr/local/go/bin' &gt;&gt; /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64 &amp;&amp; chmod +x ./kind &amp;&amp; mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\nchoco install curl -y\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -n -b main https://github.com/kcp-dev/edge-mc --depth 1 KubeStellar-placement-translator\ncd KubeStellar-placement-translator\ngit restore --staged Makefile Makefile.venv go.mod docs/mkdocs.yml docs/content docs/scripts/docs-ecutable.sh\ngit checkout Makefile Makefile.venv go.mod docs/mkdocs.yml docs/content docs/scripts/docs-ecutable.sh\nmake MANIFEST=\"'docs/content/common-subs/pre-req.md','docs/content/Coding Milestones/PoC2023q1/placement-translator.md'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ../\nrm -rf KubeStellar-placement-translator\n</code></pre> <p>The placement translator runs in the center and translates EMC placement problems into edge sync problems.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#status","title":"Status","text":"<p>The placement translator is a work in progress.  It maintains <code>SyncerConfig</code> objects and downsynced objects in mailbox workspaces, albeit with limitations discussed in the next section.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#additional-design-details","title":"Additional Design Details","text":"<p>The placement translator maintains one <code>SyncerConfig</code> object in each mailbox workspace.  That object is named <code>the-one</code>.  Other <code>SyncerConfig</code> objects may exist; the placement translator ignores them.</p> <p>The placement translator responds to each resource discovery independently.  This makes the behavior jaggy and the logging noisy. For example, it means that the <code>SyncerConfig</code> objects may be rewritten for each resource discovery.  But eventually the right things happen.</p> <p>The placement translator does not yet attempt the full prescribed technique for picking the API version to use when reading and writing. Currently it looks only at the preferred version reported in each workload management workspace, and only succeeds if they all agree.</p> <p>One detail left vague in the design outline is what constitutes the \"desired state\" that propagates from center to edge.  The easy obvious answer is the \"spec\" section of downsynced objects, but that answer ignores some issues.  Following is the current full answer.</p> <p>When creating a workload object in a mailbox workspace, the placement translator uses a copy of the object read from the workload management workspace but with the following changes.</p> <ul> <li>The <code>metadata.managedFields</code> is emptied.</li> <li>The <code>metadata.resourceVersion</code> is emptied.</li> <li>The <code>metadata.selfLlink</code> is emptied.</li> <li>The <code>metadata.uid</code> is emptied.</li> <li>The <code>metadata.ownerReferences</code> is emptied.  (Doing better would   require tracking UID mappings from WMW to MBWS.)</li> <li>In <code>metadata.labels</code>, <code>edge.kcp.io/projected=yes</code> is added.</li> </ul> <p>The placement translator does not react to changes to the workload objects in the mailbox workspace.</p> <p>When downsyncing desired state and the placement translator finds the object already exists in the mailbox workspace, the placement translator does an HTTP PUT (<code>Update</code> in the <code>k8s.io/client-go/dynamic</code> package) using an object value --- called below the \"destination\" object --- constructed by reading the object from the MBWS and making the following changes.</p> <ul> <li>For top-level sections in the source object other than <code>apiVersion</code>,   <code>kind</code>, <code>metadata</code>, and <code>status</code>, the destination object gets the   same contents for that section.</li> <li>If the source object has some annotations then they are merged into   the destination object annotations as follows.</li> <li>A destination annotation that has no corresponding annotation in     the source is unchanged.</li> <li>A destination annotation that has the same value as the     corresponding annotation in the source is unchanged.</li> <li>A \"system\" annotation is unchanged.  The system annotations are     those whose key (a) starts with <code>kcp.io/</code> or other stuff followed     by <code>.kcp.io/</code> and (b) does not start with <code>edge.kcp.io/</code>.</li> <li>The source object's labels are merged into the destination object   using the same rules as for annotations, and <code>edge.kcp.io/projected</code>   is set to <code>yes</code>.</li> <li>The remainder of the <code>metadata</code> is unchanged.</li> </ul> <p>For objects --- other than <code>Namespace</code> objects --- that exist in a mailbox workspace and whose API GroupResource has been relevant to the placement translator since it started, ones that have the <code>edge.kcp.io/projected=yes</code> label but are not currently desired are deleted.  The exclusion for <code>Namespace</code> objects is there because the placement translator does not take full ownership of them, rather it takes the position that there might be other parties that create <code>Namespace</code> objects or rely on their existence.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#usage","title":"Usage","text":"<p>The placement translator needs three kube client configurations.  One points to the edge service provider workspace and provides authority to write into the mailbox workspaces.  Another points to the kcp server base (i.e., does not identify a particular logical cluster nor <code>*</code>) and is authorized to read all clusters.  In the kubeconfig created by <code>kcp start</code> that is satisfied by the context named <code>system:admin</code>.  The third points to the \"scheduling service provider workspace\", the one that has the APIExport of <code>scheduling.kcp.io</code>. This is normally <code>root</code>, which has all the kcp APIExports, and the context named <code>root</code> in the kubeconfig created by <code>kcp start</code> satisfies this.</p> <p>The command line flags, beyond the basics, are as follows.  For a string parameter, if no default is explicitly stated then the default is the empty string, which usually means \"not specified here\".  For both kube client configurations, the usual rules apply: first consider command line parameters, then <code>$KUBECONFIG</code>, then <code>~/.kube/config</code>.</p> <pre><code>      --allclusters-cluster string       The name of the kubeconfig cluster to use for access to all clusters\n      --allclusters-context string       The name of the kubeconfig context to use for access to all clusters (default \"system:admin\")\n--allclusters-kubeconfig string    Path to the kubeconfig file to use for access to all clusters\n      --allclusters-user string          The name of the kubeconfig user to use for access to all clusters\n\n--espw-cluster string              The name of the kubeconfig cluster to use for access to the edge service provider workspace\n      --espw-context string              The name of the kubeconfig context to use for access to the edge service provider workspace\n      --espw-kubeconfig string           Path to the kubeconfig file to use for access to the edge service provider workspace\n      --espw-user string                 The name of the kubeconfig user to use for access to the edge service provider workspace\n\n--server-bind-address ipport       The IP address with port at which to serve /metrics and /debug/pprof/ (default :10204)\n--sspw-cluster string              The name of the kubeconfig cluster to use for access to the scheduling service provider workspace\n      --sspw-context string              The name of the kubeconfig context to use for access to the scheduling service provider workspace (default \"root\")\n--sspw-kubeconfig string           Path to the kubeconfig file to use for access to the scheduling service provider workspace\n      --sspw-user string                 The name of the kubeconfig user to use for access to the scheduling service provider workspace\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#try-it","title":"Try It","text":"<p>The nascent placement translator can be exercised following the scenario in example1.  You will need to run the scheduler and mailbox controller long enough for them to create what this scenario calls for, but they can be terminated after that.</p> <p></p> <p>Stage 1 creates the infrastructure and the edge service provider workspace (ESPW) and lets that react to the inventory.  Then the KubeStellar syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox workspaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>cat &gt; florin-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>cat &gt; guilder-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#start-kcp","title":"Start kcp","text":"<p>Download and build or install kcp, according to your preference.</p> <p>In some shell that will be used only for this purpose, issue the <code>kcp start</code> command.  If you have junk from previous runs laying around, you should probably <code>rm -rf .kcp</code> first.</p> <p>In the shell commands in all the following steps it is assumed that <code>kcp</code> is running and <code>$KUBECONFIG</code> is set to the <code>.kcp/admin.kubeconfig</code> that <code>kcp</code> produces, except where explicitly noted that the florin or guilder cluster is being accessed.</p> <p>It is also assumed that you have the usual kcp kubectl plugins on your <code>$PATH</code>.</p> <pre><code>git clone https://github.com/kcp-dev/edge-mc KubeStellar\n</code></pre> <p>clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>cd kcp\nmake build\n</code></pre></p> <p>run kcp (kcp will spit out tons of information and stay running in this terminal window) <pre><code>export KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\nexport PATH=$(pwd)/bin:$PATH\nkcp start &amp;&gt; /dev/null &amp;\nsleep 30 </code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-an-inventory-management-workspace","title":"Create an inventory management workspace.","text":"<p>Use the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create imw-1 --enter\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#get-kubestellar","title":"Get KubeStellar","text":"<p>Download and build or install edge-mc, according to your preference.  That is, either (a) <code>git clone</code> the repo and then <code>make build</code> to populate its <code>bin</code> directory, or (b) fetch the binary archive appropriate for your machine from a release and unpack it (creating a <code>bin</code> directory).  In the following exhibited command lines, the commands described as \"KubeStellar commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the KubeStellar <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames.  The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p> <pre><code>cd ../KubeStellar\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=si</code>.</p> <pre><code>kubectl kubestellar ensure location florin  loc-name=florin  env=prod\nkubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=si\n</code></pre> <p>Those two script invocations are equivalent to creating the following four objects.</p> <pre><code>apiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: si\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: si\nspec:\nresource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-the-edge-service-provider-workspace","title":"Create the edge service provider workspace","text":"<p>Use the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create espw --enter\n</code></pre> <p>Continue to follow the steps until the start of Stage 3 of the exercise.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#populate-the-edge-service-provider-workspace","title":"Populate the edge service provider workspace","text":"<p>This puts the definition and export of the KubeStellar API in the edge service provider workspace.</p> <p>Use the following command.</p> <pre><code>kubectl create -f config/exports\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#the-mailbox-controller","title":"The mailbox controller","text":"<p>Running the mailbox controller will be conveniently automated. Eventually.  In the meantime, you can use the KubeStellar command shown here.</p> <p><pre><code>go run ./cmd/mailbox-controller -v=2 &amp;\nsleep 45\n</code></pre> <pre><code>...\nI0423 01:09:37.991080   10624 main.go:196] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/root/workload.kcp.io\"\n...\nI0423 01:09:38.449395   10624 controller.go:299] \"Created APIBinding\" worker=1 mbwsName=\"apmziqj9p9fqlflm-mb-bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\" mbwsCluster=\"yk9a66vjms1pi8hu\" bindingName=\"bind-edge\" resourceVersion=\"914\"\n...\nI0423 01:09:38.842881   10624 controller.go:299] \"Created APIBinding\" worker=3 mbwsName=\"apmziqj9p9fqlflm-mb-b8c64c64-070c-435b-b3bd-9c0f0c040a54\" mbwsCluster=\"12299slctppnhjnn\" bindingName=\"bind-edge\" resourceVersion=\"968\"\n^C\n</code></pre></p> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox workspaces.</p> <p>This controller creates a mailbox workspace for each SyncTarget and puts an APIBinding to the edge API in each of those mailbox workspaces.  For this simple scenario, you do not need to keep this controller running after it does those things (hence the <code>^C</code> above); normally it would run continuously.</p> <p>You can get a listing of those mailbox workspaces as follows.</p> <p><pre><code>kubectl get Workspaces\n</code></pre> <pre><code>NAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   universal            Ready   https://192.168.58.123:6443/clusters/1najcltzt2nqax47   50s\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   universal            Ready   https://192.168.58.123:6443/clusters/1y7wll1dz806h3sb   50s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>kubectl get Workspace -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kcp\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                                       SYNCTARGET   CLUSTER\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   florin       1najcltzt2nqax47\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   guilder      1y7wll1dz806h3sb\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox workspace by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kcp.io/sync-target-name\"] == \"guilder\") | .name')\n</code></pre> <pre><code>1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\n</code></pre></p> <p><pre><code>FLORIN_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kcp.io/sync-target-name\"] == \"florin\") | .name')\n</code></pre> <pre><code>1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#connect-guilder-edge-cluster-with-its-mailbox-workspace","title":"Connect guilder edge cluster with its mailbox workspace","text":"<p>The following command will (a) create, in the mailbox workspace for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw-1 guilder\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote physical cluster manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>You might check that the syncer is running, as follows.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                        coredns                            2/2     2            2           35m\nlocal-path-storage                 local-path-provisioner             1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#connect-florin-edge-cluster-with-its-mailbox-workspace","title":"Connect florin edge cluster with its mailbox workspace","text":"<p>Do the analogous stuff for the florin cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw-1 florin\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote physical cluster manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>And deploy the syncer in the florin cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml </code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the scheduler react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a common parent for those two workspaces, with the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create my-org --enter\n</code></pre> <p>Next, create the WMW for the common workload.  The following command will do that, if issued while \"root:my-org\" is the current workspace.</p> <pre><code>kubectl kubestellar ensure wmw wmw-c\n</code></pre> <p>This is equivalent to creating that workspace and then entering it and creating the following two <code>APIBinding</code> objects.</p> <p><pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-espw\nspec:\nreference:\nexport:\npath: root:espw\nname: edge.kcp.io\n---\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-kube\nspec:\nreference:\nexport:\npath: \"root:compute\"\nname: kubernetes\n</code></pre> <pre><code>sleep 15\n</code></pre></p> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n  labels: {common: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kcp.io/v1alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kcp.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kcp.io/v1alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  namespaceSelector:\n    matchLabels: {\"common\":\"si\"}\n  nonNamespacedObjects:\n  - apiGroup: apis.kcp.io\n    resources: [ \"apibindings\" ]\n    resourceNames: [ \"bind-kube\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code>kubectl ws root:my-org\nkubectl kubestellar ensure wmw wmw-s\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kcp.io/v1alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kcp.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kcp.io/v1alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"si\"}\n  namespaceSelector: \n    matchLabels: {\"special\":\"si\"}\n  nonNamespacedObjects:\n  - apiGroup: apis.kcp.io\n    resources: [ \"apibindings\" ]\n    resourceNames: [ \"bind-kube\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#edge-scheduling","title":"Edge scheduling","text":"<p>In response to each EdgePlacement, the scheduler will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>Eventually there will be automation that conveniently runs the scheduler.  In the meantime, you can run it by hand: switch to the ESPW and invoke the KubeStellar command that runs the scheduler.</p> <p><pre><code>kubectl ws root:espw\n</code></pre> <pre><code>Current workspace is \"root:espw\".\n</code></pre> <pre><code>go run ./cmd/kubestellar-scheduler &amp;\nsleep 45\n</code></pre> <pre><code>I0423 01:33:37.036752   11305 kubestellar-scheduler.go:212] \"Found APIExport view\" exportName=\"edge.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/7qkse309upzrv0fy/edge.kcp.io\"\n...\nI0423 01:33:37.320859   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-scheduler\" triggeringKind=Location key=\"apmziqj9p9fqlflm|florin\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"florin\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n...\nI0423 01:33:37.391772   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-scheduler\" triggeringKind=Location key=\"apmziqj9p9fqlflm|guilder\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"guilder\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n^C\n</code></pre></p> <p>In this simple scenario you do not need to keep the scheduler running after it gets its initial work done; normally it would run continually.</p> <p>Check out the SinglePlacementSlice objects as follows.</p> <p><pre><code>kubectl ws root:my-org:wmw-c\n</code></pre> <pre><code>Current workspace is \"root:my-org:wmw-c\".\n</code></pre></p> <p><pre><code>kubectl get SinglePlacementSlice -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: edge.kcp.io/v1alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\ngeneration: 4\nname: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kcp.io/v1alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\nuid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre></p> <p>Also check out the SinglePlacementSlice objects in <code>root:my-org:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p> <p>Finally run the placement translator from the command line.  That should look like the following (possibly including some complaints, which do not necessarily indicate real problems).</p> <pre><code>kubectl ws root:espw\ngo run ./cmd/placement-translator &amp;\nsleep 120\n</code></pre> <pre><code>I0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\nI0412 15:15:57.970014   94634 shared_informer.go:289] Caches are synced for what-resolver\nI0412 15:15:57.970178   94634 shared_informer.go:282] Waiting for caches to sync for where-resolver\nI0412 15:15:57.970192   94634 shared_informer.go:289] Caches are synced for where-resolver\n...\nI0412 15:15:57.972185   94634 map-types.go:338] \"Put\" map=\"where\" key=\"r0bdh9oumjkoag3s:edge-placement-s\" val=\"[&amp;{SinglePlacementSlice edge.kcp.io/v1alpha1} {edge-placement-s    e1b1033d-49f2-45e8-8a90-6d0295b644b6 1184 1 2023-04-12 14:39:21 -0400 EDT &lt;nil&gt; &lt;nil&gt; map[] map[kcp.io/cluster:r0bdh9oumjkoag3s] [{edge.kcp.io/v1alpha1 EdgePlacement edge-placement-s 0e718a31-db21-47f1-b789-cd55835b1418 &lt;nil&gt; &lt;nil&gt;}] []  [{scheduler Update edge.kcp.io/v1alpha1 2023-04-12 14:39:21 -0400 EDT FieldsV1 {\\\"f:destinations\\\":{},\\\"f:metadata\\\":{\\\"f:ownerReferences\\\":{\\\".\\\":{},\\\"k:{\\\\\\\"uid\\\\\\\":\\\\\\\"0e718a31-db21-47f1-b789-cd55835b1418\\\\\\\"}\\\":{}}}} }]} [{1xpg93182scl85te location-g sync-target-g 5ee1c42e-a7d5-4363-ba10-2f13fe578e19}]}]\"\nI0412 15:15:57.973740   94634 map-types.go:338] \"Put\" map=\"where\" key=\"1i1weo8uoea04wxr:edge-placement-c\" val=\"[&amp;{SinglePlacementSlice edge.kcp.io/v1alpha1} {edge-placement-c    c446ca9b-8937-4751-89ab-058bcfb079c1 1183 3 2023-04-12 14:39:21 -0400 EDT &lt;nil&gt; &lt;nil&gt; map[] map[kcp.io/cluster:1i1weo8uoea04wxr] [{edge.kcp.io/v1alpha1 EdgePlacement edge-placement-c c1e038b9-8bd8-4d22-8ab8-916e40c794d1 &lt;nil&gt; &lt;nil&gt;}] []  [{scheduler Update edge.kcp.io/v1alpha1 2023-04-12 14:39:21 -0400 EDT FieldsV1 {\\\"f:destinations\\\":{},\\\"f:metadata\\\":{\\\"f:ownerReferences\\\":{\\\".\\\":{},\\\"k:{\\\\\\\"uid\\\\\\\":\\\\\\\"c1e038b9-8bd8-4d22-8ab8-916e40c794d1\\\\\\\"}\\\":{}}}} }]} [{1xpg93182scl85te location-f sync-target-f e6efb8bd-6755-45ac-b44d-5d38f978f990} {1xpg93182scl85te location-g sync-target-g 5ee1c42e-a7d5-4363-ba10-2f13fe578e19}]}]\"\n...\nI0412 15:15:58.173974   94634 map-types.go:338] \"Put\" map=\"what\" key=\"1i1weo8uoea04wxr:edge-placement-c\" val={Downsync:map[{APIGroup: Resource:namespaces Name:commonstuff}:{APIVersion:v1 IncludeNamespaceObject:false}] Upsync:[{APIGroup:group1.test Resources:[sprockets flanges] Namespaces:[orbital] Names:[george cosmo]} {APIGroup:group2.test Resources:[cogs] Namespaces:[] Names:[William]}]}\nI0412 15:15:58.180380   94634 map-types.go:338] \"Put\" map=\"what\" key=\"r0bdh9oumjkoag3s:edge-placement-s\" val={Downsync:map[{APIGroup: Resource:namespaces Name:specialstuff}:{APIVersion:v1 IncludeNamespaceObject:false}] Upsync:[{APIGroup:group1.test Resources:[sprockets flanges] Namespaces:[orbital] Names:[george cosmo]} {APIGroup:group3.test Resources:[widgets] Namespaces:[] Names:[*]}]}\n...\n</code></pre> <p>The \"Put\" log entries with <code>map=\"what\"</code> show what the \"what resolver\" is reporting.  This reports mappings from <code>ExternalName</code> of an <code>EdgePlacement</code> object to the workload parts that that <code>EdgePlacement</code> says to downsync and upsync.</p> <p>The \"Put\" log entries with <code>map=\"where\"</code> show the <code>SinglePlacementSlice</code> objects associated with each <code>EdgePlacement</code>.</p> <p>Next, using a separate shell, examine the SyncerConfig objects in the mailbox workspaces.  Make sure to use the same kubeconfig as you use to run the placement translator, or any other that is pointed at the edge service provider workspace. The following with switch the focus to mailbox workspace(s).</p> <p>You can get a listing of mailbox workspaces, while in the edge service provider workspace, as follows.</p> <p><pre><code>kubectl get workspace\n</code></pre> <pre><code>NAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\n1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19   universal            Ready   https://192.168.58.123:6443/clusters/12zzf3frkqz2yj39   36m\n1xpg93182scl85te-mb-e6efb8bd-6755-45ac-b44d-5d38f978f990   universal            Ready   https://192.168.58.123:6443/clusters/2v6wl3x41zxmpmhr   36m\n</code></pre></p> <p>Next switch to one of the mailbox workspaces (in my case I picked the one for the guilder cluster) and examine the <code>SyncerConfig</code> object. That should look like the following.</p> <p><pre><code>kubectl ws $(kubectl get workspace | sed -n '2p' | awk '{print $1}')\n</code></pre> <pre><code>Current workspace is \"root:espw:1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml                           </code></pre> <pre><code>apiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12zzf3frkqz2yj39\n  creationTimestamp: \"2023-04-12T19:15:58Z\"\ngeneration: 2\nname: the-one\n  resourceVersion: \"1249\"\nuid: 00bf8d10-393a-4d94-b032-79fae30646f6\nspec:\n  namespaceScope:\n    namespaces:\n    - commonstuff\n    - specialstuff\n    resources:\n    - apiVersion: v1\n      group: \"\"\nresource: limitranges\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n    - apiVersion: v1\n      group: \"\"\nresource: resourcequotas\n    - apiVersion: v1\n      group: \"\"\nresource: configmaps\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: events.k8s.io\n      resource: events\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\nresource: events\n    - apiVersion: v1\n      group: \"\"\nresource: secrets\n    - apiVersion: v1\n      group: \"\"\nresource: services\n    - apiVersion: v1\n      group: \"\"\nresource: pods\n    - apiVersion: v1\n      group: \"\"\nresource: serviceaccounts\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n  upsync:\n  - apiGroup: group2.test\n    names:\n    - William\n    resources:\n    - cogs\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\nstatus: {}\n</code></pre></p> <p>At this point you might veer off from the example scenario and try tweaking things.  For example, try deleting an EdgePlacement as follows.</p> <p><pre><code>kubectl ws root:my-org:wmw-c\n</code></pre> <pre><code>Current workspace is \"root:work-c\"\n</code></pre> <pre><code>kubectl delete EdgePlacement edge-placement-c\n</code></pre> <pre><code>edgeplacement.edge.kcp.io \"edge-placement-c\" deleted\n</code></pre></p> <p>That will cause the placement translator to log updates, as follows.</p> <pre><code>I0412 15:20:43.129842   94634 map-types.go:338] \"Put\" map=\"what\" key=\"1i1weo8uoea04wxr:edge-placement-c\" val={Downsync:map[] Upsync:[]}\nI0412 15:20:43.241674   94634 map-types.go:342] \"Delete\" map=\"where\" key=\"1i1weo8uoea04wxr:edge-placement-c\"\n</code></pre> <p>After that, the SyncerConfig in the florin mailbox should be empty, as in the following (you mailbox workspace names may be different).</p> <p><pre><code>kubectl ws root:espw\n</code></pre> <pre><code>Current workspace is \"root:espw\".\n</code></pre></p> <p><pre><code>kubectl ws $(kubectl get workspace | sed -n '2p' | awk '{print $1}')\n</code></pre> <pre><code>Current workspace is \"root:espw:2lplrryirmv4xug3-mb-89c08764-01ae-4117-8fb0-6b752e76bc2f\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 2cow9p3xogak4n0u\n  creationTimestamp: \"2023-04-11T04:34:22Z\"\ngeneration: 4\nname: the-one\n  resourceVersion: \"2130\"\nuid: 2b66b4bc-4130-4bf0-8524-73d6885f2ad8\nspec:\n  namespaceScope: {}\nstatus: {}\n</code></pre></p> <p>And the SyncerConfig in the guilder mailbox workspace should reflect only the special workload.  That would look something like the following.</p> <p><pre><code>kubectl ws root:espw\nkubectl ws $(kubectl get workspace | sed -n '2p' | awk '{print $1}')\n</code></pre> <pre><code>Current workspace is \"root:espw:1xpg93182scl85te-mb-5ee1c42e-a7d5-4363-ba10-2f13fe578e19\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml                           </code></pre> <pre><code>apiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12zzf3frkqz2yj39\n  creationTimestamp: \"2023-04-12T19:15:58Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1254\"\nuid: 00bf8d10-393a-4d94-b032-79fae30646f6\nspec:\n  namespaceScope:\n    namespaces:\n    - specialstuff\n    resources:\n    - apiVersion: v1\n      group: \"\"\nresource: pods\n    - apiVersion: v1\n      group: \"\"\nresource: events\n    - apiVersion: v1\n      group: \"\"\nresource: limitranges\n    - apiVersion: v1\n      group: \"\"\nresource: services\n    - apiVersion: v1\n      group: \"\"\nresource: configmaps\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\nresource: serviceaccounts\n    - apiVersion: v1\n      group: \"\"\nresource: secrets\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n    - apiVersion: v1\n      group: \"\"\nresource: resourcequotas\n    - apiVersion: v1\n      group: events.k8s.io\n      resource: events\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\nstatus: {}\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator/#teardown-the-environment","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl ws root:my-org\nkubectl kubestellar remove wmw example-wmw\nkubectl ws root\nkubectl delete workspace my-org\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Stop and uninstall KubeStellar use the following command:</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/","title":"Roadmap","text":""},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#background","title":"Background","text":"<p>The outline mentions features that need not be implement at first.  In the following sections we consider some particular use cases.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#mvi","title":"MVI","text":"<p>MVI needs customization.  We can demo an MVI scenario without: self-sufficient edge clusters, summarization, upsync (Return and/or summarization of reported state from associated objects), sophisticated handling of workload conflicts.</p> <p>What about denaturing?</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#compliance-to-policy","title":"Compliance-to-Policy","text":"<p>I am not sure what is workable here.  Following are some possibilities.  They vary in two dimensions.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#how-c2p-controller-consumes-reports","title":"How C2P controller consumes reports","text":""},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-view-of-workload-apiexport-and-workload-apibindings","title":"Through view of workload APIExport and workload APIBindings","text":"<p>As outlined in PR 241: - the C2P team maintains CRDs, APIResourceSchemas, and an APIExport for   the policy and report resources; - the C2P team puts those APIResourceSchemas and that APIExport in a   kcp workspace of their choice; - the workload management workspace has an APIBinding to that APIExport; - the EdgePlacement selects that APIBinding for downsync; - the APIBinding goes to the mailbox workspace but not the edge cluster; - those CRDs are pre-installed on the edge clusters; - the APIExport's view shows the report objects in the mailbox   workspaces (as well as anywhere else they exist).</p> <p>This is not a great choice because of \"those CRDs are pre-installed on the edge clusters\".</p> <p>It is also not a great choice because it requires the C2P team to maintain two copies of the Kyverno resource definitions.</p> <p>This is a bad choice because it is not consistent with the preferred way to demonstrate installation of Kyverno, which is to have Helm install Kyverno into the workload management workspace.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-a-new-kind-of-view","title":"Through a new kind of view","text":"<p>We could define a new kind of view that does what we want.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-view-of-emc-apiexport-and-mailbox-apibindings","title":"Through view of EMC APIExport and mailbox APIBindings","text":"<p>This approach also uses APIExport and APIBinding objects but in a different way than above.  In this approach the placement translator maintains one APIExport in the edge service provider workspace and a corresponding APIBinding object in each mailbox workspace, and they work together as follows.</p> <p>The APIExport has an empty LatestResourceSchemas but a large dynamic PermissionClaims slice.  In particular, there is a PermissionClaim for every resource involved in downsync or upsync in any EdgePlacement object.  Some day we might try something more granular, but today is not that day.</p> <p>In each mailbox workspace, the corresponding APIBinding's list of accepted PermissionClaims has an entry for every resource downsynced or upsynced to that workspace.</p> <p>As a consequence, the APIExport's view holds all the objects whose kind/resource is defined by those APIBindings.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#through-an-api-for-consuming-from-mailboxes","title":"Through an API for consuming from mailboxes","text":"<p>The C2P Controller uses the API proposed in PR 240 to read the report objects from the mailbox workspaces.  This has the downside of exposing the mailbox workspaces as part of the KubeStellar interface --- which they were NOT originally intended to be.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#c2p-controller-consumes-report-summaries-prepared-by-kubestellar","title":"C2P Controller consumes report summaries prepared by KubeStellar","text":"<p>In this scenario: - we have defined and implemented summarization in KubeStellar; - that summarization is adequate for the needs of the C2P Controller; - that controller consumes summaries rather than the reports themselves.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#mailbox-vs-edge","title":"Mailbox vs. Edge","text":"<p>The latest plan is to use full EMC.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#using-the-current-tmc-syncer","title":"Using the current TMC syncer","text":"<p>In this scenario the edge clusters are not self-sufficient; the workload containers in an edge cluster use kube api services from the corresponding mailbox workspace.  The key insight here is that from an outside perspective, a pair of (edge cluster, corresponding mailbox workspace) operates as a unit and the rest of the world does not care about internal details of that unit.  But that is only true if you do not require too much from the networking.  In this scenario, a workload container runs in the edge cluster and a workload Service object is about proxying/load-balancing in the edge cluster.  An admission control webhook normally directs the apiserver to call out to a virtual IP address associated with a Service; that is a problem in this scenario because the apiserver in question is the one holding the mailbox workspace but the Service that gets connections to the workload containers is in the edge cluster.  This scenario will work if the C2P workload does not include admission control webhooks.  Note that Kubernetes release 1.26 introduces CEL-based validating admission control policies, so using them would not involve webhooks.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#expand-tmc-to-support-webhooks","title":"Expand TMC to support webhooks","text":"<p>The problem with webhooks would go away if TMC were expanded to support them, perhaps through some sort of tunneling so that a client in the center can open connections to a Service at the edge.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#pre-deploy-controllers-and-resources-on-edge-clusters","title":"Pre-deploy controllers and resources on edge clusters","text":"<p>In this scenario, the PVP/PEP is pre-deployed on the edge clusters, and the policy and report resources (which are cluster-scoped) are predefined there too.  This scenario would continue to use the TMC syncer, but only need it to downsync the policies and upsync the reports.</p>"},{"location":"Coding%20Milestones/PoC2023q1/roadmap-uses/#use-full-emc","title":"Use full EMC","text":"<p>No shortcuts here, no limitations.</p>"},{"location":"Coding%20Milestones/PoC2023q1/environments/_index/","title":"Environments","text":"<p>There will be 2 environments designed, created, and maintained for KubeStellar: - A cloud environment (cloud-env) which will be used to deploy, scale, and measure metrics associated with small, medium, and large-scale KubeStellar experiments, - A development environment (dev-env) which will be used to deploy a small local KubeStellar installation that is sized for daily use and experimentation on a laptop.</p>"},{"location":"Coding%20Milestones/PoC2023q1/environments/cloud-env/","title":"Cloud-Environment (cloud-env)","text":""},{"location":"Coding%20Milestones/PoC2023q1/environments/cloud-env/#monitoring-tools-for-kubestellar-prometheus-grafana-and-node-exporter","title":"Monitoring Tools for KubeStellar (Prometheus, Grafana and Node Exporter)","text":""},{"location":"Coding%20Milestones/PoC2023q1/environments/cloud-env/#description","title":"Description","text":"<p>This example shows how to deploy monitoring tools (prometheus, grafana and node exporter) for KubeStellar components (core and edge regions) - see architecture image above. Prometheus server is deployed in the core region running the KCP server alongside the components for KubeStellar. A Prometheus agent is deployed in the edge regions running the edge pclusters</p> <ol> <li>Create your hosts file with the list of target hosts (KCP server &amp; Edge pclusters)</li> </ol> <pre><code>[kcp-server]\n192.168.56.2\n\n[edge-pclusters]\n192.160.56.10\n192.160.56.12\n</code></pre> <ol> <li>Configure the prometheus targets endpoints:</li> </ol> <p>a) Prometheus Server: edit the file roles/prometheus/templates/prometheus-config.yaml.j2</p> <pre><code>global:\n  evaluation_interval: 5s\n  external_labels:\n    env: dev\n  scrape_interval: 30s\nscrape_configs:\n- job_name: mailbox-controller\n  scrape_interval: 15s\n  metrics_path: /metrics\n  static_configs:\n  - targets:\n    - '&lt;host-ipAddress&gt;:10203'\n\n- job_name: node-exporter\n  scrape_interval: 15s\n  metrics_path: /metrics\n  static_configs:\n  - targets:\n    - '&lt;host-ipAddress&gt;:9100'\n\n- job_name: kcp\n  scrape_interval: 15s\n  scheme: https\n  metrics_path: /metrics\n  tls_config:\n    insecure_skip_verify: true\n  static_configs:\n  - targets:\n    - '&lt;host-ipAddress&gt;:6443'\n</code></pre> <p>b) Prometheus Agent: edit the file roles/prometheus/templates/prometheus-agent-config.yaml.j2</p> <pre><code># my global config\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s \nscrape_configs:\n  - job_name: \"prometheus-agent\"\n\n    static_configs:\n      - targets: [\"{ ansible_default_ipv4.address }:&lt;port_number&gt;\"]\nremote_write:\n  - url: \"http://&lt;prometheus-server-ip-address&gt;:&lt;port_number&gt;/api/v1/write\"\n</code></pre> <ol> <li>Install prometheus server, grafana and node exporter to the KCP server host using the following playbook:</li> </ol> <pre><code>- hosts: kcp-server\n  remote_user: ubuntu\n  become: yes\n  gather_facts: yes\n  connection: ssh\n  tasks:\n  roles:\n    - node-exporter\n    - prometheus\n    - grafana\n</code></pre> <pre><code>ansible-playbook -i hosts monitoring-kcpServer.yaml\n</code></pre> <ol> <li>Install prometheus agent and node exporter to an edge pcluster using the following playbook:</li> </ol> <pre><code>- hosts: edge-pclusters\n  remote_user: ubuntu\n  become: yes\n  gather_facts: yes\n  connection: ssh\n  vars:\n   agent: 'yes'\n  tasks:\n  roles:\n    - node-exporter\n    - prometheus\n</code></pre> <pre><code>ansible-playbook -i hosts monitoring-pcluster.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/environments/dev-env/","title":"Development-Environment (dev-env)","text":"<p>under construction - coming soon</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/","title":"Example1 post espw","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#populate-the-edge-service-provider-workspace","title":"Populate the edge service provider workspace","text":"<p>This puts the definition and export of the KubeStellar API in the edge service provider workspace.</p> <p>Use the following command.</p> <pre><code>kubectl create -f config/exports\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#the-mailbox-controller","title":"The mailbox controller","text":"<p>Running the mailbox controller will be conveniently automated. Eventually.  In the meantime, you can use the KubeStellar command shown here.</p> <p><pre><code>go run ./cmd/mailbox-controller -v=2 &amp;\nsleep 45\n</code></pre> <pre><code>...\nI0423 01:09:37.991080   10624 main.go:196] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/root/workload.kcp.io\"\n...\nI0423 01:09:38.449395   10624 controller.go:299] \"Created APIBinding\" worker=1 mbwsName=\"apmziqj9p9fqlflm-mb-bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\" mbwsCluster=\"yk9a66vjms1pi8hu\" bindingName=\"bind-edge\" resourceVersion=\"914\"\n...\nI0423 01:09:38.842881   10624 controller.go:299] \"Created APIBinding\" worker=3 mbwsName=\"apmziqj9p9fqlflm-mb-b8c64c64-070c-435b-b3bd-9c0f0c040a54\" mbwsCluster=\"12299slctppnhjnn\" bindingName=\"bind-edge\" resourceVersion=\"968\"\n^C\n</code></pre></p> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox workspaces.</p> <p>This controller creates a mailbox workspace for each SyncTarget and puts an APIBinding to the edge API in each of those mailbox workspaces.  For this simple scenario, you do not need to keep this controller running after it does those things (hence the <code>^C</code> above); normally it would run continuously.</p> <p>You can get a listing of those mailbox workspaces as follows.</p> <p><pre><code>kubectl get Workspaces\n</code></pre> <pre><code>NAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   universal            Ready   https://192.168.58.123:6443/clusters/1najcltzt2nqax47   50s\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   universal            Ready   https://192.168.58.123:6443/clusters/1y7wll1dz806h3sb   50s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>kubectl get Workspace -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kcp\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                                       SYNCTARGET   CLUSTER\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   florin       1najcltzt2nqax47\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   guilder      1y7wll1dz806h3sb\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox workspace by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kcp.io/sync-target-name\"] == \"guilder\") | .name')\n</code></pre> <pre><code>1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\n</code></pre></p> <p><pre><code>FLORIN_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kcp.io/sync-target-name\"] == \"florin\") | .name')\n</code></pre> <pre><code>1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#connect-guilder-edge-cluster-with-its-mailbox-workspace","title":"Connect guilder edge cluster with its mailbox workspace","text":"<p>The following command will (a) create, in the mailbox workspace for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw-1 guilder\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote physical cluster manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>You might check that the syncer is running, as follows.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                        coredns                            2/2     2            2           35m\nlocal-path-storage                 local-path-provisioner             1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#connect-florin-edge-cluster-with-its-mailbox-workspace","title":"Connect florin edge cluster with its mailbox workspace","text":"<p>Do the analogous stuff for the florin cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw-1 florin\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote physical cluster manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>And deploy the syncer in the florin cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml </code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the scheduler react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a common parent for those two workspaces, with the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create my-org --enter\n</code></pre> <p>Next, create the WMW for the common workload.  The following command will do that, if issued while \"root:my-org\" is the current workspace.</p> <pre><code>kubectl kubestellar ensure wmw wmw-c\n</code></pre> <p>This is equivalent to creating that workspace and then entering it and creating the following two <code>APIBinding</code> objects.</p> <p><pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-espw\nspec:\nreference:\nexport:\npath: root:espw\nname: edge.kcp.io\n---\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-kube\nspec:\nreference:\nexport:\npath: \"root:compute\"\nname: kubernetes\n</code></pre> <pre><code>sleep 15\n</code></pre></p> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n  labels: {common: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kcp.io/v1alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kcp.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kcp.io/v1alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  namespaceSelector:\n    matchLabels: {\"common\":\"si\"}\n  nonNamespacedObjects:\n  - apiGroup: apis.kcp.io\n    resources: [ \"apibindings\" ]\n    resourceNames: [ \"bind-kube\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code>kubectl ws root:my-org\nkubectl kubestellar ensure wmw wmw-s\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kcp.io/v1alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kcp.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kcp.io/v1alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"si\"}\n  namespaceSelector: \n    matchLabels: {\"special\":\"si\"}\n  nonNamespacedObjects:\n  - apiGroup: apis.kcp.io\n    resources: [ \"apibindings\" ]\n    resourceNames: [ \"bind-kube\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#edge-scheduling","title":"Edge scheduling","text":"<p>In response to each EdgePlacement, the scheduler will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>Eventually there will be automation that conveniently runs the scheduler.  In the meantime, you can run it by hand: switch to the ESPW and invoke the KubeStellar command that runs the scheduler.</p> <p><pre><code>kubectl ws root:espw\n</code></pre> <pre><code>Current workspace is \"root:espw\".\n</code></pre> <pre><code>go run ./cmd/kubestellar-scheduler &amp;\nsleep 45\n</code></pre> <pre><code>I0423 01:33:37.036752   11305 kubestellar-scheduler.go:212] \"Found APIExport view\" exportName=\"edge.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/7qkse309upzrv0fy/edge.kcp.io\"\n...\nI0423 01:33:37.320859   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-scheduler\" triggeringKind=Location key=\"apmziqj9p9fqlflm|florin\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"florin\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n...\nI0423 01:33:37.391772   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-scheduler\" triggeringKind=Location key=\"apmziqj9p9fqlflm|guilder\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"guilder\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n^C\n</code></pre></p> <p>In this simple scenario you do not need to keep the scheduler running after it gets its initial work done; normally it would run continually.</p> <p>Check out the SinglePlacementSlice objects as follows.</p> <p><pre><code>kubectl ws root:my-org:wmw-c\n</code></pre> <pre><code>Current workspace is \"root:my-org:wmw-c\".\n</code></pre></p> <p><pre><code>kubectl get SinglePlacementSlice -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: edge.kcp.io/v1alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\ngeneration: 4\nname: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kcp.io/v1alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\nuid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre></p> <p>Also check out the SinglePlacementSlice objects in <code>root:my-org:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#stage-3","title":"Stage 3","text":"<p>In Stage 3, in response to the EdgePlacement and SinglePlacementSlice objects, the placement translator will copy the workload prescriptions into the mailbox workspaces and create <code>SyncerConfig</code> objects there.</p> <p>Eventually there will be convenient automation running the placement translator.  In the meantime, you can run it manually: switch to the ESPW and use the KubeStellar command that runs the placement translator.</p> <p><pre><code>kubectl ws root:espw\n</code></pre> <pre><code>Current workspace is \"root:espw\".\n</code></pre> <pre><code>go run ./cmd/placement-translator &amp;\nsleep 120\n</code></pre> <pre><code>I0423 01:39:56.362722   11644 shared_informer.go:282] Waiting for caches to sync for placement-translator\n...\n</code></pre></p> <p>After it stops logging stuff, wait another minute and then you can ^C it or use another shell to continue exploring.</p> <p>The florin cluster gets only the common workload.  Examine florin's <code>SyncerConfig</code> as follows.  Utilize florin's name (which you stored in Stage 1) here.</p> <pre><code>kubectl ws $FLORIN_WS\n</code></pre> <pre><code>Current workspace is \"root:espw:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\n</code></pre> <pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12299slctppnhjnn\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1323\"\nuid: 8840fee6-37dc-407e-ad01-2ad59389d4ff\nspec:\n  namespaceScope:\n    namespaces:\n    - commonstuff\n    resources:\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n    - apiVersion: v1\n      group: \"\"\nresource: configmaps\n    - apiVersion: v1\n      group: \"\"\nresource: limitranges\n    - apiVersion: v1\n      group: \"\"\nresource: secrets\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\nresource: pods\n    - apiVersion: v1\n      group: \"\"\nresource: serviceaccounts\n    - apiVersion: v1\n      group: \"\"\nresource: services\n    - apiVersion: v1\n      group: \"\"\nresource: resourcequotas\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n  upsync:\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre> <p>You can check that the workload got there too.</p> <p><pre><code>kubectl get ns\n</code></pre> <pre><code>NAME          STATUS   AGE\ncommonstuff   Active   6m34s\ndefault       Active   32m\n</code></pre></p> <p><pre><code>kubectl get deployments -A\n</code></pre> <pre><code>NAMESPACE     NAME      READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff   commond   0/0     0            0           6m44s\n</code></pre></p> <p>The guilder cluster gets both the common and special workloads. Examine guilder's <code>SyncerConfig</code> object and workloads as follows, using the name that you stored in Stage 1.</p> <p><pre><code>kubectl ws root:espw\n</code></pre> <pre><code>Current workspace is \"root:espw\".\n</code></pre></p> <p><pre><code>kubectl ws $GUILDER_WS\n</code></pre> <pre><code>Current workspace is \"root:espw:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: yk9a66vjms1pi8hu\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 4\nname: the-one\n  resourceVersion: \"1325\"\nuid: 3da056c7-0d5c-45a3-9d91-d04f04415f30\nspec:\n  namespaceScope:\n    namespaces:\n    - commonstuff\n    - specialstuff\n    resources:\n    - apiVersion: v1\n      group: \"\"\nresource: services\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\nresource: pods\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: \"\"\nresource: limitranges\n    - apiVersion: v1\n      group: \"\"\nresource: serviceaccounts\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n    - apiVersion: v1\n      group: \"\"\nresource: configmaps\n    - apiVersion: v1\n      group: \"\"\nresource: secrets\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n    - apiVersion: v1\n      group: \"\"\nresource: resourcequotas\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre></p> <p><pre><code>kubectl get deployments -A\n</code></pre> <pre><code>NAMESPACE      NAME       READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff    commond    0/0     0            0           6m1s\nspecialstuff   speciald   0/0     0            0           5m58s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-espw/#stage-4","title":"Stage 4","text":"<p>In Stage 4, the edge syncer does its thing.  Actually, it should have done it as soon as the relevant inputs became available in stage 3. Now we examine what happened.</p> <p>You can check that the workloads are running in the edge clusters as they should be.</p> <p>The syncer does its thing between the florin cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>The syncer does its thing between the guilder cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>Using the kubeconfig that <code>kind</code> modified, examine the florin cluster. Find just the <code>commonstuff</code> namespace and the <code>commond</code> Deployment.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get ns\n</code></pre> <pre><code>NAME                                 STATUS   AGE\ncommonstuff                          Active   6m51s\ndefault                              Active   57m\nkubestellar-syncer-florin-1t9zgidy   Active   17m\nkube-node-lease                      Active   57m\nkube-public                          Active   57m\nkube-system                          Active   57m\nlocal-path-storage                   Active   57m\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy -A | egrep 'NAME|stuff'\n</code></pre> <pre><code>NAMESPACE                         NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                       commond                           1/1     1            1           7m59s\n</code></pre></p> <p>Examine the guilder cluster.  Find both workload namespaces and both Deployments.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get ns | egrep NAME\\|stuff\n</code></pre> <pre><code>NAME                               STATUS   AGE\ncommonstuff                        Active   8m33s\nspecialstuff                       Active   8m33s\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A | egrep NAME\\|stuff\n</code></pre> <pre><code>NAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                        commond                            1/1     1            1           8m37s\nspecialstuff                       speciald                           1/1     1            1           8m55s\n</code></pre></p> <p>Examining the common workload in the guilder cluster, for example, will show that the replacement-style customization happened.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -n commonstuff commond -o yaml\n</code></pre> <pre><code>...\n      containers:\n      - env:\n        - name: EXAMPLE_VAR\n          value: env is prod\n        image: library/httpd:2.4\n        imagePullPolicy: IfNotPresent\n        name: httpd\n...\n</code></pre></p> <p>Check that the common workload on the florin cluster is working.</p> <p><pre><code>sleep 10\n</code></pre> <pre><code>curl http://localhost:8094\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in florin.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the special workload on the guilder cluster is working. <pre><code>sleep 10\n</code></pre> <pre><code>curl http://localhost:8097\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a special web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the common workload on the guilder cluster is working.</p> <p><pre><code>curl http://localhost:8096\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-kcp/","title":"Example1 post kcp","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-kcp/#create-an-inventory-management-workspace","title":"Create an inventory management workspace.","text":"<p>Use the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create imw-1 --enter\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-kcp/#get-kubestellar","title":"Get KubeStellar","text":"<p>Download and build or install edge-mc, according to your preference.  That is, either (a) <code>git clone</code> the repo and then <code>make build</code> to populate its <code>bin</code> directory, or (b) fetch the binary archive appropriate for your machine from a release and unpack it (creating a <code>bin</code> directory).  In the following exhibited command lines, the commands described as \"KubeStellar commands\" and the commands that start with <code>kubectl kubestellar</code> rely on the KubeStellar <code>bin</code> directory being on the <code>$PATH</code>.  Alternatively you could invoke them with explicit pathnames.  The kubectl plugin lines use fully specific executables (e.g., <code>kubectl kubestellar prep-for-syncer</code> corresponds to <code>bin/kubectl-kubestellar-prep_for_syncer</code>).</p> <pre><code>cd ../KubeStellar\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-kcp/#create-synctarget-and-location-objects-to-represent-the-florin-and-guilder-clusters","title":"Create SyncTarget and Location objects to represent the florin and guilder clusters","text":"<p>Use the following two commands. They label both florin and guilder with <code>env=prod</code>, and also label guilder with <code>extended=si</code>.</p> <pre><code>kubectl kubestellar ensure location florin  loc-name=florin  env=prod\nkubectl kubestellar ensure location guilder loc-name=guilder env=prod extended=si\n</code></pre> <p>Those two script invocations are equivalent to creating the following four objects.</p> <pre><code>apiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\nname: florin\nlabels:\nid: florin\nloc-name: florin\nenv: prod\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\nname: florin\nlabels:\nloc-name: florin\nenv: prod\nspec:\nresource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: florin}\n---\napiVersion: workload.kcp.io/v1alpha1\nkind: SyncTarget\nmetadata:\nname: guilder\nlabels:\nid: guilder\nloc-name: guilder\nenv: prod\nextended: si\n---\napiVersion: scheduling.kcp.io/v1alpha1\nkind: Location\nmetadata:\nname: guilder\nlabels:\nloc-name: guilder\nenv: prod\nextended: si\nspec:\nresource: {group: workload.kcp.io, version: v1alpha1, resource: synctargets}\ninstanceSelector:\nmatchLabels: {id: guilder}\n</code></pre> <p>That script also deletes the Location named <code>default</code>, which is not used in this PoC, if it shows up.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-post-kcp/#create-the-edge-service-provider-workspace","title":"Create the edge service provider workspace","text":"<p>Use the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create espw --enter\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-pre-kcp/","title":"Example1 pre kcp","text":"<p>Stage 1 creates the infrastructure and the edge service provider workspace (ESPW) and lets that react to the inventory.  Then the KubeStellar syncers are deployed, in the edge clusters and configured to work with the corresponding mailbox workspaces.  This stage has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-pre-kcp/#create-two-kind-clusters","title":"Create two kind clusters.","text":"<p>This example uses two kind clusters as edge clusters.  We will call them \"florin\" and \"guilder\".</p> <p>This example uses extremely simple workloads, which use <code>hostPort</code> networking in Kubernetes.  To make those ports easily reachable from your host, this example uses an explicit <code>kind</code> configuration for each edge cluster.</p> <p>For the florin cluster, which will get only one workload, create a file named <code>florin-config.yaml</code> with the following contents.  In a <code>kind</code> config file, <code>containerPort</code> is about the container that is also a host (a Kubernetes node), while the <code>hostPort</code> is about the host that hosts that container.</p> <pre><code>cat &gt; florin-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>For the guilder cluster, which will get two workloads, create a file named <code>guilder-config.yaml</code> with the following contents.  The workload that uses hostPort 8081 goes in both clusters, while the workload that uses hostPort 8082 goes only in the guilder cluster.</p> <pre><code>cat &gt; guilder-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Finally, create the two clusters with the following two commands, paying attention to <code>$KUBECONFIG</code> and, if that's empty, <code>~/.kube/config</code>: <code>kind create</code> will inject/replace the relevant \"context\" in your active kubeconfig.</p> <pre><code>kind create cluster --name florin --config florin-config.yaml\nkind create cluster --name guilder --config guilder-config.yaml\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1a/","title":"Example1 stage 1a","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1a/#populate-the-edge-service-provider-workspace","title":"Populate the edge service provider workspace","text":"<p>This puts the definition and export of the KubeStellar API in the edge service provider workspace.</p> <p>Use the following command.</p> <pre><code>kubectl create -f config/exports\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1a/#the-mailbox-controller","title":"The mailbox controller","text":"<p>Running the mailbox controller will be conveniently automated. Eventually.  In the meantime, you can use the KubeStellar command shown here.</p> <p><pre><code>go run ./cmd/mailbox-controller -v=2 &amp;\nsleep 45\n</code></pre> <pre><code>...\nI0423 01:09:37.991080   10624 main.go:196] \"Found APIExport view\" exportName=\"workload.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/root/workload.kcp.io\"\n...\nI0423 01:09:38.449395   10624 controller.go:299] \"Created APIBinding\" worker=1 mbwsName=\"apmziqj9p9fqlflm-mb-bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\" mbwsCluster=\"yk9a66vjms1pi8hu\" bindingName=\"bind-edge\" resourceVersion=\"914\"\n...\nI0423 01:09:38.842881   10624 controller.go:299] \"Created APIBinding\" worker=3 mbwsName=\"apmziqj9p9fqlflm-mb-b8c64c64-070c-435b-b3bd-9c0f0c040a54\" mbwsCluster=\"12299slctppnhjnn\" bindingName=\"bind-edge\" resourceVersion=\"968\"\n^C\n</code></pre></p> <p>You need a <code>-v</code> setting of 2 or numerically higher to get log messages about individual mailbox workspaces.</p> <p>This controller creates a mailbox workspace for each SyncTarget and puts an APIBinding to the edge API in each of those mailbox workspaces.  For this simple scenario, you do not need to keep this controller running after it does those things (hence the <code>^C</code> above); normally it would run continuously.</p> <p>You can get a listing of those mailbox workspaces as follows.</p> <p><pre><code>kubectl get Workspaces\n</code></pre> <pre><code>NAME                                                       TYPE        REGION   PHASE   URL                                                     AGE\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   universal            Ready   https://192.168.58.123:6443/clusters/1najcltzt2nqax47   50s\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   universal            Ready   https://192.168.58.123:6443/clusters/1y7wll1dz806h3sb   50s\n</code></pre></p> <p>More usefully, using custom columns you can get a listing that shows the name of the associated SyncTarget.</p> <p><pre><code>kubectl get Workspace -o \"custom-columns=NAME:.metadata.name,SYNCTARGET:.metadata.annotations['edge\\.kcp\\.io/sync-target-name'],CLUSTER:.spec.cluster\"\n</code></pre> <pre><code>NAME                                                       SYNCTARGET   CLUSTER\n1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1   florin       1najcltzt2nqax47\n1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c   guilder      1y7wll1dz806h3sb\n</code></pre></p> <p>Also: if you ever need to look up just one mailbox workspace by SyncTarget name, you could do it as follows.</p> <p><pre><code>GUILDER_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kcp.io/sync-target-name\"] == \"guilder\") | .name')\n</code></pre> <pre><code>1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\n</code></pre></p> <p><pre><code>FLORIN_WS=$(kubectl get Workspace -o json | jq -r '.items | .[] | .metadata | select(.annotations [\"edge.kcp.io/sync-target-name\"] == \"florin\") | .name')\n</code></pre> <pre><code>1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1b/","title":"Example1 stage 1b","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1b/#connect-guilder-edge-cluster-with-its-mailbox-workspace","title":"Connect guilder edge cluster with its mailbox workspace","text":"<p>The following command will (a) create, in the mailbox workspace for guilder, an identity and authorizations for the edge syncer and (b) write a file containing YAML for deploying the syncer in the guilder cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw-1 guilder\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote physical cluster manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>The file written was, as mentioned in the output, <code>guilder-syncer.yaml</code>.  Next <code>kubectl apply</code> that to the guilder cluster.  That will look something like the following; adjust as necessary to make kubectl manipulate your guilder cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>You might check that the syncer is running, as follows.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                        coredns                            2/2     2            2           35m\nlocal-path-storage                 local-path-provisioner             1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-1b/#connect-florin-edge-cluster-with-its-mailbox-workspace","title":"Connect florin edge cluster with its mailbox workspace","text":"<p>Do the analogous stuff for the florin cluster.</p> <p><pre><code>kubectl kubestellar prep-for-syncer --imw root:imw-1 florin\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote physical cluster manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>And deploy the syncer in the florin cluster.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml </code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/","title":"Example1 stage 2","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/#stage-2","title":"Stage 2","text":"<p>Stage 2 creates two workloads, called \"common\" and \"special\", and lets the scheduler react.  It has the following steps.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/#create-and-populate-the-workload-management-workspace-for-the-common-workload","title":"Create and populate the workload management workspace for the common workload","text":"<p>One of the workloads is called \"common\", because it will go to both edge clusters.  The other one is called \"special\".</p> <p>In this example, each workload description goes in its own workload management workspace (WMW).  Start by creating a common parent for those two workspaces, with the following commands.</p> <pre><code>kubectl ws root\nkubectl ws create my-org --enter\n</code></pre> <p>Next, create the WMW for the common workload.  The following command will do that, if issued while \"root:my-org\" is the current workspace.</p> <pre><code>kubectl kubestellar ensure wmw wmw-c\n</code></pre> <p>This is equivalent to creating that workspace and then entering it and creating the following two <code>APIBinding</code> objects.</p> <p><pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-espw\nspec:\nreference:\nexport:\npath: root:espw\nname: edge.kcp.io\n---\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\nname: bind-kube\nspec:\nreference:\nexport:\npath: \"root:compute\"\nname: kubernetes\n</code></pre> <pre><code>sleep 15\n</code></pre></p> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.  The workload in this example in an Apache httpd server that serves up a very simple web page, conveyed via a Kubernetes ConfigMap that is mounted as a volume for the httpd pod.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n  labels: {common: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kcp.io/v1alpha1\nkind: Customizer\nmetadata:\n  namespace: commonstuff\n  name: example-customizer\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"env is %(env)\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: commonstuff\n  name: commond\n  annotations:\n    edge.kcp.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches both Location objects created earlier, thus directing the common workload to both edge clusters.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kcp.io/v1alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  namespaceSelector:\n    matchLabels: {\"common\":\"si\"}\n  nonNamespacedObjects:\n  - apiGroup: apis.kcp.io\n    resources: [ \"apibindings\" ]\n    resourceNames: [ \"bind-kube\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/#create-and-populate-the-workload-management-workspace-for-the-special-workload","title":"Create and populate the workload management workspace for the special workload","text":"<p>Use the following <code>kubectl</code> commands to create the WMW for the special workload.</p> <pre><code>kubectl ws root:my-org\nkubectl kubestellar ensure wmw wmw-s\n</code></pre> <p>Next, use <code>kubectl</code> to create the following workload objects in that workspace.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: specialstuff\n  labels: {special: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: specialstuff\n  name: httpd-htdocs\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a special web site.\n        Running in %(loc-name).\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: edge.kcp.io/v1alpha1\nkind: Customizer\nmetadata:\n  namespace: specialstuff\n  name: example-customizer\n  annotations:\n    edge.kcp.io/expand-parameters: \"true\"\nreplacements:\n- path: \"$.spec.template.spec.containers.0.env.0.value\"\n  value: '\"in %(env) env\"'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: specialstuff\n  name: speciald\n  annotations:\n    edge.kcp.io/customizer: example-customizer\nspec:\n  selector: {matchLabels: {app: special} }\n  template:\n    metadata:\n      labels: {app: special}\n    spec:\n      containers:\n      - name: httpd\n        env:\n        - name: EXAMPLE_VAR\n          value: example value\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8082\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p> <p>Finally, use <code>kubectl</code> to create the following EdgePlacement object. Its \"where predicate\" (the <code>locationSelectors</code> array) has one label selector that matches only one of the Location objects created earlier, thus directing the special workload to just one edge cluster.</p> <p><pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kcp.io/v1alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-s\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\",\"extended\":\"si\"}\n  namespaceSelector: \n    matchLabels: {\"special\":\"si\"}\n  nonNamespacedObjects:\n  - apiGroup: apis.kcp.io\n    resources: [ \"apibindings\" ]\n    resourceNames: [ \"bind-kube\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group3.test\"\n    resources: [\"widgets\"]\n    names: [\"*\"]\nEOF\n</code></pre> <pre><code>sleep 10\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-2/#edge-scheduling","title":"Edge scheduling","text":"<p>In response to each EdgePlacement, the scheduler will create a corresponding SinglePlacementSlice object.  These will indicate the following resolutions of the \"where\" predicates.</p> EdgePlacement Resolved Where edge-placement-c florin, guilder edge-placement-s guilder <p>Eventually there will be automation that conveniently runs the scheduler.  In the meantime, you can run it by hand: switch to the ESPW and invoke the KubeStellar command that runs the scheduler.</p> <p><pre><code>kubectl ws root:espw\n</code></pre> <pre><code>Current workspace is \"root:espw\".\n</code></pre> <pre><code>go run ./cmd/kubestellar-scheduler &amp;\nsleep 45\n</code></pre> <pre><code>I0423 01:33:37.036752   11305 kubestellar-scheduler.go:212] \"Found APIExport view\" exportName=\"edge.kcp.io\" serverURL=\"https://192.168.58.123:6443/services/apiexport/7qkse309upzrv0fy/edge.kcp.io\"\n...\nI0423 01:33:37.320859   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-scheduler\" triggeringKind=Location key=\"apmziqj9p9fqlflm|florin\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"florin\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n...\nI0423 01:33:37.391772   11305 reconcile_on_location.go:192] \"updated SinglePlacementSlice\" controller=\"kubestellar-scheduler\" triggeringKind=Location key=\"apmziqj9p9fqlflm|guilder\" locationWorkspace=\"apmziqj9p9fqlflm\" location=\"guilder\" workloadWorkspace=\"10l175x6ejfjag3e\" singlePlacementSlice=\"edge-placement-c\"\n^C\n</code></pre></p> <p>In this simple scenario you do not need to keep the scheduler running after it gets its initial work done; normally it would run continually.</p> <p>Check out the SinglePlacementSlice objects as follows.</p> <p><pre><code>kubectl ws root:my-org:wmw-c\n</code></pre> <pre><code>Current workspace is \"root:my-org:wmw-c\".\n</code></pre></p> <p><pre><code>kubectl get SinglePlacementSlice -o yaml\n</code></pre> <pre><code>apiVersion: v1\nitems:\n- apiVersion: edge.kcp.io/v1alpha1\n  destinations:\n  - cluster: apmziqj9p9fqlflm\n    locationName: florin\n    syncTargetName: florin\n    syncTargetUID: b8c64c64-070c-435b-b3bd-9c0f0c040a54\n  - cluster: apmziqj9p9fqlflm\n    locationName: guilder\n    syncTargetName: guilder\n    syncTargetUID: bf452e1f-45a0-4d5d-b35c-ef1ece2879ba\n  kind: SinglePlacementSlice\n  metadata:\n    annotations:\n      kcp.io/cluster: 10l175x6ejfjag3e\n    creationTimestamp: \"2023-04-23T05:33:37Z\"\ngeneration: 4\nname: edge-placement-c\n    ownerReferences:\n    - apiVersion: edge.kcp.io/v1alpha1\n      kind: EdgePlacement\n      name: edge-placement-c\n      uid: 199cfe1e-48d9-4351-af5c-e66c83bf50dd\n    resourceVersion: \"1316\"\nuid: b5db1f9d-1aed-4a25-91da-26dfbb5d8879\nkind: List\nmetadata:\n  resourceVersion: \"\"\n</code></pre></p> <p>Also check out the SinglePlacementSlice objects in <code>root:my-org:wmw-s</code>.  It should go similarly, but the <code>destinations</code> should include only the entry for guilder.</p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-3/","title":"Example1 stage 3","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-3/#stage-3","title":"Stage 3","text":"<p>In Stage 3, in response to the EdgePlacement and SinglePlacementSlice objects, the placement translator will copy the workload prescriptions into the mailbox workspaces and create <code>SyncerConfig</code> objects there.</p> <p>Eventually there will be convenient automation running the placement translator.  In the meantime, you can run it manually: switch to the ESPW and use the KubeStellar command that runs the placement translator.</p> <p><pre><code>kubectl ws root:espw\n</code></pre> <pre><code>Current workspace is \"root:espw\".\n</code></pre> <pre><code>go run ./cmd/placement-translator &amp;\nsleep 120\n</code></pre> <pre><code>I0423 01:39:56.362722   11644 shared_informer.go:282] Waiting for caches to sync for placement-translator\n...\n</code></pre></p> <p>After it stops logging stuff, wait another minute and then you can ^C it or use another shell to continue exploring.</p> <p>The florin cluster gets only the common workload.  Examine florin's <code>SyncerConfig</code> as follows.  Utilize florin's name (which you stored in Stage 1) here.</p> <pre><code>kubectl ws $FLORIN_WS\n</code></pre> <pre><code>Current workspace is \"root:espw:1t82bk54r6gjnzsp-mb-1a045336-8178-4026-8a56-5cd5609c0ec1\" (type root:universal).\n</code></pre> <pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: 12299slctppnhjnn\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 3\nname: the-one\n  resourceVersion: \"1323\"\nuid: 8840fee6-37dc-407e-ad01-2ad59389d4ff\nspec:\n  namespaceScope:\n    namespaces:\n    - commonstuff\n    resources:\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n    - apiVersion: v1\n      group: \"\"\nresource: configmaps\n    - apiVersion: v1\n      group: \"\"\nresource: limitranges\n    - apiVersion: v1\n      group: \"\"\nresource: secrets\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\nresource: pods\n    - apiVersion: v1\n      group: \"\"\nresource: serviceaccounts\n    - apiVersion: v1\n      group: \"\"\nresource: services\n    - apiVersion: v1\n      group: \"\"\nresource: resourcequotas\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n  upsync:\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre> <p>You can check that the workload got there too.</p> <p><pre><code>kubectl get ns\n</code></pre> <pre><code>NAME          STATUS   AGE\ncommonstuff   Active   6m34s\ndefault       Active   32m\n</code></pre></p> <p><pre><code>kubectl get deployments -A\n</code></pre> <pre><code>NAMESPACE     NAME      READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff   commond   0/0     0            0           6m44s\n</code></pre></p> <p>The guilder cluster gets both the common and special workloads. Examine guilder's <code>SyncerConfig</code> object and workloads as follows, using the name that you stored in Stage 1.</p> <p><pre><code>kubectl ws root:espw\n</code></pre> <pre><code>Current workspace is \"root:espw\".\n</code></pre></p> <p><pre><code>kubectl ws $GUILDER_WS\n</code></pre> <pre><code>Current workspace is \"root:espw:1t82bk54r6gjnzsp-mb-f0a82ab1-63f4-49ea-954d-3a41a35a9f1c\" (type root:universal).\n</code></pre></p> <p><pre><code>kubectl get SyncerConfig the-one -o yaml\n</code></pre> <pre><code>apiVersion: edge.kcp.io/v1alpha1\nkind: SyncerConfig\nmetadata:\n  annotations:\n    kcp.io/cluster: yk9a66vjms1pi8hu\n  creationTimestamp: \"2023-04-23T05:39:56Z\"\ngeneration: 4\nname: the-one\n  resourceVersion: \"1325\"\nuid: 3da056c7-0d5c-45a3-9d91-d04f04415f30\nspec:\n  namespaceScope:\n    namespaces:\n    - commonstuff\n    - specialstuff\n    resources:\n    - apiVersion: v1\n      group: \"\"\nresource: services\n    - apiVersion: v1\n      group: apps\n      resource: deployments\n    - apiVersion: v1\n      group: \"\"\nresource: pods\n    - apiVersion: v1\n      group: coordination.k8s.io\n      resource: leases\n    - apiVersion: v1\n      group: networking.k8s.io\n      resource: ingresses\n    - apiVersion: v1\n      group: \"\"\nresource: limitranges\n    - apiVersion: v1\n      group: \"\"\nresource: serviceaccounts\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: rolebindings\n    - apiVersion: v1\n      group: \"\"\nresource: configmaps\n    - apiVersion: v1\n      group: \"\"\nresource: secrets\n    - apiVersion: v1\n      group: rbac.authorization.k8s.io\n      resource: roles\n    - apiVersion: v1\n      group: \"\"\nresource: resourcequotas\n  upsync:\n  - apiGroup: group3.test\n    names:\n    - '*'\nresources:\n    - widgets\n  - apiGroup: group1.test\n    names:\n    - george\n    - cosmo\n    namespaces:\n    - orbital\n    resources:\n    - sprockets\n    - flanges\n  - apiGroup: group2.test\n    names:\n    - william\n    resources:\n    - cogs\nstatus: {}\n</code></pre></p> <p><pre><code>kubectl get deployments -A\n</code></pre> <pre><code>NAMESPACE      NAME       READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff    commond    0/0     0            0           6m1s\nspecialstuff   speciald   0/0     0            0           5m58s\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-4/","title":"Example1 stage 4","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-stage-4/#stage-4","title":"Stage 4","text":"<p>In Stage 4, the edge syncer does its thing.  Actually, it should have done it as soon as the relevant inputs became available in stage 3. Now we examine what happened.</p> <p>You can check that the workloads are running in the edge clusters as they should be.</p> <p>The syncer does its thing between the florin cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>The syncer does its thing between the guilder cluster and its mailbox workspace.  This is driven by the <code>SyncerConfig</code> object named <code>the-one</code> in that mailbox workspace.</p> <p>Using the kubeconfig that <code>kind</code> modified, examine the florin cluster. Find just the <code>commonstuff</code> namespace and the <code>commond</code> Deployment.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get ns\n</code></pre> <pre><code>NAME                                 STATUS   AGE\ncommonstuff                          Active   6m51s\ndefault                              Active   57m\nkubestellar-syncer-florin-1t9zgidy   Active   17m\nkube-node-lease                      Active   57m\nkube-public                          Active   57m\nkube-system                          Active   57m\nlocal-path-storage                   Active   57m\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy -A | egrep 'NAME|stuff'\n</code></pre> <pre><code>NAMESPACE                         NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                       commond                           1/1     1            1           7m59s\n</code></pre></p> <p>Examine the guilder cluster.  Find both workload namespaces and both Deployments.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get ns | egrep NAME\\|stuff\n</code></pre> <pre><code>NAME                               STATUS   AGE\ncommonstuff                        Active   8m33s\nspecialstuff                       Active   8m33s\n</code></pre></p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A | egrep NAME\\|stuff\n</code></pre> <pre><code>NAMESPACE                          NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                        commond                            1/1     1            1           8m37s\nspecialstuff                       speciald                           1/1     1            1           8m55s\n</code></pre></p> <p>Examining the common workload in the guilder cluster, for example, will show that the replacement-style customization happened.</p> <p><pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -n commonstuff commond -o yaml\n</code></pre> <pre><code>...\n      containers:\n      - env:\n        - name: EXAMPLE_VAR\n          value: env is prod\n        image: library/httpd:2.4\n        imagePullPolicy: IfNotPresent\n        name: httpd\n...\n</code></pre></p> <p>Check that the common workload on the florin cluster is working.</p> <p><pre><code>sleep 10\n</code></pre> <pre><code>curl http://localhost:8094\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in florin.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the special workload on the guilder cluster is working. <pre><code>sleep 10\n</code></pre> <pre><code>curl http://localhost:8097\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a special web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Check that the common workload on the guilder cluster is working.</p> <p><pre><code>curl http://localhost:8096\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;body&gt;\n    This is a common web site.\n    Running in guilder.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-start-kcp/","title":"Example1 start kcp","text":""},{"location":"Coding%20Milestones/PoC2023q1/example1-subs/example1-start-kcp/#start-kcp","title":"Start kcp","text":"<p>Download and build or install kcp, according to your preference.</p> <p>In some shell that will be used only for this purpose, issue the <code>kcp start</code> command.  If you have junk from previous runs laying around, you should probably <code>rm -rf .kcp</code> first.</p> <p>In the shell commands in all the following steps it is assumed that <code>kcp</code> is running and <code>$KUBECONFIG</code> is set to the <code>.kcp/admin.kubeconfig</code> that <code>kcp</code> produces, except where explicitly noted that the florin or guilder cluster is being accessed.</p> <p>It is also assumed that you have the usual kcp kubectl plugins on your <code>$PATH</code>.</p> <pre><code>git clone https://github.com/kcp-dev/edge-mc KubeStellar\n</code></pre> <p>clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>cd kcp\nmake build\n</code></pre></p> <p>run kcp (kcp will spit out tons of information and stay running in this terminal window) <pre><code>export KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\nexport PATH=$(pwd)/bin:$PATH\nkcp start &amp;&gt; /dev/null &amp;\nsleep 30 </code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler-subs/kubestellar-scheduler-0-pull-kcp-and-kubestellar-source-and-start-kcp/","title":"Kubestellar scheduler 0 pull kcp and kubestellar source and start kcp","text":"<pre><code>git clone https://github.com/kcp-dev/edge-mc KubeStellar\n</code></pre> <p>Clone the v0.11.0 branch kcp source: <pre><code>git clone -b v0.11.0 https://github.com/kcp-dev/kcp kcp\n</code></pre> Build the kubectl-ws binary and include it in <code>$PATH</code> <pre><code>cd kcp\nmake build\nexport PATH=$(pwd)/bin:$PATH\n</code></pre></p> <p>Run kcp (kcp will spit out tons of information and stay running in this terminal window) <pre><code>kcp start &amp;&gt; /dev/null &amp;\nsleep 30\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler-subs/kubestellar-scheduler-1-export-kubeconfig-and-path-for-kcp/","title":"Kubestellar scheduler 1 export kubeconfig and path for kcp","text":"<pre><code>export KUBECONFIG=$(pwd)/.kcp/admin.kubeconfig\nexport PATH=$(pwd)/bin:$PATH\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler-subs/kubestellar-scheduler-2-ws-root-and-ws-create-edge/","title":"Kubestellar scheduler 2 ws root and ws create edge","text":"<p>Next, create the edge service provider workspace:</p> <p>Use workspace <code>root:espw</code> as the Edge Service Provider Workspace (ESPW). <pre><code>kubectl ws root\nkubectl ws create espw\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler-subs/kubestellar-scheduler-exports/","title":"Kubestellar scheduler exports","text":"<pre><code>kubectl ws root:espw\nkubectl apply -f ../KubeStellar/config/exports/\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler-subs/kubestellar-scheduler-imports/","title":"Kubestellar scheduler imports","text":"<p>Use the user home workspace (\\~) as the workload management workspace (WMW). <pre><code>kubectl ws \\~\n</code></pre></p> <p>Bind APIs. <pre><code>kubectl apply -f ../KubeStellar/config/imports/\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler-subs/kubestellar-scheduler-process-start-without-cd-kubestellar/","title":"Kubestellar scheduler process start without cd kubestellar","text":"<pre><code>kubectl ws root:espw\ngo run cmd/kubestellar-scheduler/main.go -v 2 &amp;\nsleep 45\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-scheduler-subs/kubestellar-scheduler-process-start/","title":"Kubestellar scheduler process start","text":"<pre><code>kubectl ws root:espw\ncd ../KubeStellar\ngo run cmd/kubestellar-scheduler/main.go -v 2 &amp;\nsleep 45\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer-subs/kubestellar-syncer-0-deploy-florin/","title":"Kubestellar syncer 0 deploy florin","text":"<p>Go to inventory management workspace and find the mailbox workspace name. <pre><code>kubectl ws root:imw-1\nmbws=`kubectl get SyncTarget florin -o jsonpath=\"{.metadata.annotations['kcp\\.io/cluster']}-mb-{.metadata.uid}\"`\necho \"mailbox workspace name = $mbws\"\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nmailbox workspace name = vosh9816n2xmpdwm-mb-bb47149d-52d3-4f14-84dd-7b64ac01c97f\n</code></pre></p> <p>Go to the mailbox workspace and run the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer. <pre><code>kubectl ws root:espw:$mbws\n./bin/kubectl-kubestellar-syncer_gen florin --syncer-image quay.io/kubestellar/syncer:v0.2.2 -o florin-syncer.yaml\n</code></pre> <pre><code>Current workspace is \"root:espw:vosh9816n2xmpdwm-mb-bb47149d-52d3-4f14-84dd-7b64ac01c97f\".\nCreating service account \"kubestellar-syncer-florin-32uaph9l\"\nCreating cluster role \"kubestellar-syncer-florin-32uaph9l\" to give service account \"kubestellar-syncer-florin-32uaph9l\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-32uaph9l\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-32uaph9l\" to bind service account \"kubestellar-syncer-florin-32uaph9l\" to cluster role \"kubestellar-syncer-florin-32uaph9l\".\n\nWrote physical cluster manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-32uaph9l\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-32uaph9l\" kubestellar-syncer-florin-32uaph9l\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Deploy the generated yaml manifest to the target cluster. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin apply -f florin-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-florin-32uaph9l created\nserviceaccount/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-32uaph9l created\nsecret/kubestellar-syncer-florin-32uaph9l created\ndeployment.apps/kubestellar-syncer-florin-32uaph9l created\n</code></pre></p> <p>Check that the syncer is running, as follows. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-florin get deploy -A\n</code></pre> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-florin-32uaph9l    kubestellar-syncer-florin-32uaph9l    1/1     1            1           42s\nkube-system                           coredns                               2/2     2            2           41m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           41m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer-subs/kubestellar-syncer-0-deploy-guilder/","title":"Kubestellar syncer 0 deploy guilder","text":"<p>Go to inventory management workspace and find the mailbox workspace name. <pre><code>kubectl ws root:imw-1\nmbws=`kubectl get SyncTarget guilder -o jsonpath=\"{.metadata.annotations['kcp\\.io/cluster']}-mb-{.metadata.uid}\"`\necho \"mailbox workspace name = $mbws\"\n</code></pre> <pre><code>Current workspace is \"root:imw-1\".\nmailbox workspace name = vosh9816n2xmpdwm-mb-bf1277df-0da9-4a26-b0fc-3318862b1a5e\n</code></pre></p> <p>Go to the mailbox workspace and run the following command to obtain yaml manifests to bootstrap KubeStellar-Syncer. <pre><code>kubectl ws root:espw:$mbws\n./bin/kubectl-kubestellar-syncer_gen guilder --syncer-image quay.io/kubestellar/syncer:v0.2.2 -o guilder-syncer.yaml\n</code></pre> <pre><code>Current workspace is \"root:espw:vosh9816n2xmpdwm-mb-bf1277df-0da9-4a26-b0fc-3318862b1a5e\".\nCreating service account \"kubestellar-syncer-guilder-wfeig2lv\"\nCreating cluster role \"kubestellar-syncer-guilder-wfeig2lv\" to give service account \"kubestellar-syncer-guilder-wfeig2lv\"\n1. write and sync access to the synctarget \"kubestellar-syncer-guilder-wfeig2lv\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-guilder-wfeig2lv\" to bind service account \"kubestellar-syncer-guilder-wfeig2lv\" to cluster role \"kubestellar-syncer-guilder-wfeig2lv\".\n\nWrote physical cluster manifest to guilder-syncer.yaml for namespace \"kubestellar-syncer-guilder-wfeig2lv\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"guilder-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-guilder-wfeig2lv\" kubestellar-syncer-guilder-wfeig2lv\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:espw\".\n</code></pre></p> <p>Deploy the generated yaml manifest to the target cluster. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre> <pre><code>namespace/kubestellar-syncer-guilder-wfeig2lv created\nserviceaccount/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-guilder-wfeig2lv created\nsecret/kubestellar-syncer-guilder-wfeig2lv created\ndeployment.apps/kubestellar-syncer-guilder-wfeig2lv created\n</code></pre></p> <p>Check that the syncer is running, as follows. <pre><code>KUBECONFIG=~/.kube/config kubectl --context kind-guilder get deploy -A\n</code></pre> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\nkubestellar-syncer-guilder-saaywsu5   kubestellar-syncer-guilder-saaywsu5   1/1     1            1           52s\nkube-system                           coredns                               2/2     2            2           35m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           35m\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/kubestellar-syncer-subs/kubestellar-syncer-1-syncer-gen-plugin/","title":"Kubestellar syncer 1 syncer gen plugin","text":"<p>Generate UUID for Syncer identification. <pre><code>syncer_id=\"syncer-\"`uuidgen | tr '[:upper:]' '[:lower:]'`\n</code></pre></p> <p>Go to a workspace. <pre><code>kubectl ws root\nkubectl ws create ws1 --enter\n</code></pre></p> <p>Create the following APIBinding in the workspace (Note that in the case of mailbox workspaces, it's done by mailbox controller at creating the mailbox workspace.) <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\n  name: bind-espw\nspec:\n  reference:\n    export:\n      path: root:espw\n      name: edge.kcp.io\nEOL\n</code></pre></p> <p>Create a serviceaccount in the workspace. <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: $syncer_id\nEOL\n</code></pre></p> <p>Create clusterrole and clusterrolebinding to bind the serviceaccount to the role. <pre><code>cat &lt;&lt; EOL | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: $syncer_id\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- nonResourceURLs: [\"/\"]\n  verbs: [\"access\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: $syncer_id\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: $syncer_id\nsubjects:\n- apiGroup: \"\"\n  kind: ServiceAccount\n  name: $syncer_id\n  namespace: default\nEOL\n</code></pre></p> <p>Get the serviceaccount token that will be set in the upstream kubeconfig manifest. <pre><code>secret_name=`kubectl get secret -o custom-columns=\":.metadata.name\"| grep $syncer_id`\ntoken=`kubectl get secret $secret_name -o jsonpath='{.data.token}' | base64 -d`\n</code></pre></p> <p>Get the certificates that will be set in the upstream kubeconfig manifest. <pre><code>cacrt=`kubectl config view --minify --raw | yq \".clusters[0].cluster.certificate-authority-data\"`\n</code></pre></p> <p>Get server_url that will be set in the upstream kubeconfig manifest. <pre><code>server_url=`kubectl config view --minify --raw | yq \".clusters[0].cluster.server\" | sed -e 's|https://\\(.*\\):\\([^/]*\\)/.*|https://\\1:\\2|g'`\n</code></pre></p> <p>Set some other parameters. a. downstream_namespace where Syncer Pod runs <pre><code>downstream_namespace=\"kubestellar-$syncer_id\"\n</code></pre> b. Syncer image <pre><code>image=\"quay.io/kubestellar/syncer:v0.2.2\"\n</code></pre> c. Logical cluster name <pre><code>cluster_name=`kubectl get logicalclusters.core.kcp.io cluster -o custom-columns=\":.metadata.annotations.kcp\\.io\\/cluster\" --no-headers`\n</code></pre></p> <p>Download manifest template. <pre><code>curl -LO https://raw.githubusercontent.com/kcp-dev/edge-mc/main/pkg/syncer/scripts/kubestellar-syncer-bootstrap.template.yaml\n</code></pre></p> <p>Generate manifests to bootstrap KubeStellar-Syncer. <pre><code>syncer_id=$syncer_id cacrt=$cacrt token=$token server_url=$server_url downstream_namespace=$downstream_namespace image=$image cluster_name=$cluster_name envsubst &lt; kubestellar-syncer-bootstrap.template.yaml\n</code></pre> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: kubestellar-syncer-9ee90de6-eb76-4ddb-9346-c4c8d92075e1\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n...\n</code></pre></p>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller-subs/mailbox-controller-process-start-without-cd-kubestellar/","title":"Mailbox controller process start without cd kubestellar","text":"<pre><code>kubectl ws root:espw\ngo run ./cmd/mailbox-controller -v=2 &amp;\nsleep 45\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/mailbox-controller-subs/mailbox-controller-process-start/","title":"Mailbox controller process start","text":"<pre><code>kubectl ws root:espw\ncd ../KubeStellar\ngo run ./cmd/mailbox-controller -v=2 &amp;\nsleep 45\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator-subs/placement-translator-process-start-without-cd-kubestellar/","title":"Placement translator process start without cd kubestellar","text":"<pre><code>kubectl ws root:espw\ngo run ./cmd/placement-translator &amp;\nsleep 120\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q1/placement-translator-subs/placement-translator-process-start/","title":"Placement translator process start","text":"<pre><code>kubectl ws root:espw\ncd ../KubeStellar\ngo run ./cmd/placement-translator &amp;\nsleep 120\n</code></pre>"},{"location":"Coding%20Milestones/PoC2023q3/outline/","title":"Coming Soon","text":""},{"location":"Community/_index/","title":"Join the KubeStellar community","text":""},{"location":"Community/_index/#kubestellar-is-an-open-source-project-that-anyone-in-the-community-can-use-improve-and-enjoy-join-us-heres-a-few-ways-to-find-out-whats-happening-and-get-involved","title":"KubeStellar is an open source project that anyone in the community can use, improve, and enjoy. Join us! Here's a few ways to find out what's happening and get involved","text":""},{"location":"Community/_index/#learn-and-connect","title":"Learn and Connect","text":""},{"location":"Community/_index/#using-or-want-to-use-kubestellar-find-out-more-here","title":"Using or want to use KubeStellar? Find out more here:","text":"<ul> <li>User mailing list: Discussion and help from your fellow users</li> <li>YouTube Channel: Follow us on YouTube to view recordings of past KubeStellar community meetings and demo days</li> <li>LinkedIn: See what others are saying about the community</li> <li>Medium Blog Series: Follow us on Medium to read about community developments</li> </ul>"},{"location":"Community/_index/#develop-and-contribute","title":"Develop and Contribute","text":""},{"location":"Community/_index/#if-you-want-to-get-more-involved-by-contributing-to-kubestellar-join-us-here","title":"If you want to get more involved by contributing to KubeStellar, join us here:","text":"<ul> <li>GitHub: Development takes place here!</li> <li>#kubestellar-dev Slack channel in the Kubernetes slack workspace: Chat with other project developers</li> <li>Developer mailing list: Discuss development issues around the project</li> <li>You can find out how to contribute to KubeStellar in our Contribution Guidelines</li> </ul>"},{"location":"Community/_index/#community-meetings","title":"Community Meetings","text":"<ol> <li>Join our Developer mailing list to get your community meeting invitation.</li> <li>You can also directly subscribe to the community calendar, or view our calendar</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Sign up to discuss a topic in the KubeStellar Community Meeting Agenda</li> </ol>"},{"location":"Community/_index/#other-resources","title":"Other Resources","text":"<ul> <li>Google Drive</li> </ul>"},{"location":"Community/partners/argocd/","title":"ArgoCD","text":"<p>This document explains how to add KubeStellar's 'workspaces' as Argo CD's 'clusters'.</p>"},{"location":"Community/partners/argocd/#add-kubestellars-workspaces-to-argo-cd-as-clusters","title":"Add KubeStellar's workspaces to Argo CD as clusters","text":"<p>As of today, the 'workspaces', aka 'logical clusters' used by KubeStellar are not identical with ordinary Kubernetes clusters. Thus, in order to add them as Argo CD's 'clusters', there are a few more steps to take.</p> <p>For KubeStellar's Inventory Management Workspace (IMW) and Workload Management Workspace (WMW). The steps are similar. Let's take WMW as an example:</p> <ol> <li>Create `kube-system` namespace in the workspace.</li> <li>Make sure necessary apibindings exist in the workspace.  For WMW, we need one for Kubernetes and one for KubeStellar's edge API.</li> <li>Exclude `ClusterWorkspace` from discovery and sync.  <pre><code>kubectl -n argocd edit cm argocd-cm\n</code></pre>  Make sure `resource.exclusions` exists in the `data` field of the `argocd-cm` configmap as follows: <pre><code>data:\nresource.exclusions: |\n- apiGroups:\n- \"tenancy.kcp.io\"\nkinds:\n- \"ClusterWorkspace\"\nclusters:\n- \"*\"\n</code></pre>  Restart the Argo CD server. <pre><code>kubectl -n argocd rollout restart deployment argocd-server\n</code></pre>  Argo CD's documentation mentions this feature as [Resource Exclusion/Inclusion](https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#resource-exclusioninclusion). </li> <li>Make sure the current context uses WMW, then identify the admin.kubeconfig. The command and output should be similar to <pre><code>$ argocd cluster add --name wmw --kubeconfig ./admin.kubeconfig workspace.kcp.io/current\nWARNING: This will create a service account `argocd-manager` on the cluster referenced by context `workspace.kcp.io/current` with full cluster level privileges. Do you want to continue [y/N]? y\nINFO[0001] ServiceAccount \"argocd-manager\" already exists in namespace \"kube-system\"\nINFO[0001] ClusterRole \"argocd-manager-role\" updated\nINFO[0001] ClusterRoleBinding \"argocd-manager-role-binding\" updated\nCluster 'https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo' added\n</code></pre>  ### Create Argo CD Applications Once KubeStellar's workspaces are added, Argo CD Applications can be created as normal. There are a few examples listed [here](https://github.com/edge-experiments/gitops-source/tree/main/edge-mc), and the commands to use the examples are listed as follows.  #### Create Argo CD Applications against KubeStellar's IMW Create two Locations. The command and output should be similar to <pre><code>$ argocd app create locations \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path edge-mc/locations/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:imw-turbo \\\n--sync-policy automated\napplication 'locations' created\n</code></pre>  Create two SyncTargets. The command and output should be similar to <pre><code>$ argocd app create synctargets \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path edge-mc/synctargets/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:imw-turbo \\\n--sync-policy automated\napplication 'synctargets' created\n</code></pre>  #### Create Argo CD Application against KubeStellar's WMW Create a Namespace. The command and output should be similar to <pre><code>$ argocd app create namespace \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path edge-mc/namespaces/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'namespace' created\n</code></pre>  Create a Deployment for 'cpumemload'. The command and output should be similar to <pre><code>$ argocd app create cpumemload \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path edge-mc/workloads/cpumemload/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'cpumemload' created\n</code></pre>  Create an EdgePlacement. The command and output should be similar to <pre><code>$ argocd app create edgeplacement \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path edge-mc/placements/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'edgeplacement' created\n</code></pre> </li> </ol>"},{"location":"Community/partners/argocd/#other-resources","title":"Other Resources","text":"<p>Medium - Sync 10,000 ArgoCD Applications in One Shot Medium - Sync 10,000 ArgoCD Applications in One Shot, by Yourself Medium - GitOpsCon - here we come</p>"},{"location":"Community/partners/argocd/#argocd-scale-experiment-kubestellar-community-demo-day","title":"ArgoCD Scale Experiment - KubeStellar Community Demo Day","text":""},{"location":"Community/partners/argocd/#gitopscon-2023-a-quantitative-study-on-argo-scalability-andrew-anderson-jun-duan-ibm","title":"GitOpsCon 2023 - A Quantitative Study on Argo Scalability - Andrew Anderson &amp; Jun Duan, IBM","text":""},{"location":"Community/partners/argocd/#argocd-and-kubestellar-in-the-news","title":"ArgoCD and KubeStellar in the news","text":""},{"location":"Community/partners/fluxcd/","title":"FluxCD","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/kyverno/","title":"Check out KubeStellar working with Kyverno:","text":"<p>Medium - Syncing Objects from one Kubernetes cluster to another Kubernetes cluster</p>"},{"location":"Community/partners/kyverno/#kyverno-and-kubestellar-demo-day","title":"Kyverno and KubeStellar Demo Day","text":""},{"location":"Community/partners/kyverno/#kyverno-and-kubestellar-in-the-news","title":"Kyverno and KubeStellar in the news","text":""},{"location":"Community/partners/kyverno/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/mvi/","title":"MVI","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/openziti/","title":"OpenZiti","text":""},{"location":"Community/partners/turbonomic/","title":"Check out KubeStellar working with Turbonomic:","text":"<p>Medium - Make Multi-Cluster Scheduling a No-Brainer</p>"},{"location":"Community/partners/turbonomic/#turbonomic-and-kubestellar-demo-day","title":"Turbonomic and KubeStellar Demo Day","text":""},{"location":"Community/partners/turbonomic/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/turbonomic/#turbonomic-and-kubestellar-in-the-news","title":"Turbonomic and KubeStellar in the news","text":""},{"location":"Contribution%20guidelines/CONTRIBUTING/","title":"Contributing to KubeStellar","text":"<p>Greetings! We are grateful for your interest in joining the KubeStellar community and making a positive impact. Whether you're raising issues, enhancing documentation, fixing bugs, or developing new features, your contributions are essential to our success.</p> <p>To get started, kindly read through this document and familiarize yourself with our code of conduct. If you have any inquiries, please feel free to reach out to us on the KubeStellar-dev Slack channel.</p> <p>We can't wait to collaborate with you!</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#contributing-code","title":"Contributing Code","text":""},{"location":"Contribution%20guidelines/CONTRIBUTING/#prerequisites","title":"Prerequisites","text":"<p>Install Go 1.19+.   Please note that the go language version numbers in these files must exactly agree:</p> <pre><code>Your local go/go.mod file, kcp/.ci-operator.yaml, kcp/Dockerfile, and in all the kcp/.github/workflows yaml files that specify go-version.\n\n- In ./ci-operator.yaml the go version is indicated by the \"tag\" attribute.\n- In ./Dockerfile it is indicated by the \"golang\" attribute\n- In go.mod it is indicated by the \"go\" directive.\n- In the .github/workflows yaml files it is indicated by \"go-version\"\n</code></pre> <p>Check out our QuickStart Guide</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#issues","title":"Issues","text":"<p>Prioritization for pull requests is given to those that address and resolve existing GitHub issues. Utilize the available issue labels to identify meaningful and relevant issues to work on.</p> <p>If you believe that there is a need for a fix and no existing issue covers it, feel free to create a new one.</p> <p>As a new contributor, we encourage you to start with issues labeled as good first issues.</p> <p>Your assistance in improving documentation is highly valued, regardless of your level of experience with the project.</p> <p>To claim an issue that you are interested in, kindly leave a comment on the issue and request the maintainers to assign it to you.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#committing","title":"Committing","text":"<p>We encourage all contributors to adopt best practices in git commit management to facilitate efficient reviews and retrospective analysis. Your git commits should provide ample context for reviewers and future codebase readers.</p> <p>A recommended format for final commit messages is as follows:</p> <pre><code>{Short Title}: {Problem this commit is solving and any important contextual information} {issue number if applicable}\n</code></pre>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>When submitting a pull request, clear communication is appreciated. This can be achieved by providing the following information:</p> <ul> <li>Detailed description of the problem you are trying to solve, along with links to related GitHub issues</li> <li>Explanation of your solution, including links to any design documentation and discussions</li> <li>Information on how you tested and validated your solution</li> <li>Updates to relevant documentation and examples, if applicable</li> </ul> <p>The pull request template has been designed to assist you in communicating this information effectively.</p> <p>Smaller pull requests are typically easier to review and merge than larger ones. If your pull request is big, it is always recommended to collaborate with the maintainers to find the best way to divide it.</p> <p>Approvers will review your PR within a business day. A PR requires both an /lgtm and then an /approve in order to get merged. You may /approve your own PR but you may not /lgtm it. Automation will add the PR it to the OpenShift PR merge queue. The OpenShift Tide bot will automatically merge your work when it is available.</p> <p>Congratulations! Your pull request has been successfully merged! \ud83d\udc4f</p> <p>If you have any questions about contributing, don't hesitate to reach out to us on the KubeStellar-dev Slack channel.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-locally","title":"Testing Locally","text":"<p>Our QuickStart  guide shows a user how to install a local KCP server and install the KubeStellar components and run an example.  As a contributor you will want a different setup flow, including <code>git clone</code> of this repo instead of fetching and unpacking a release archive.  The same example usage should work for you, and there is a larger example at this link.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-changes-to-the-bootstrap-script","title":"Testing changes to the bootstrap script","text":"<p>The quickstart says to fetch the bootstrap script from the main branch of the KubeStellar repo; if you want to contribute a change to that script then you will need to test your changed version.  Just run your local copy (perhaps in a special testing directory, just to be safe) and be sure to add the downloaded <code>bin</code> at the front of your <code>$PATH</code> (contrary to what the scripting currently tells you) so that your <code>git clone</code>'s <code>bin</code> does not shadow the one being tested.</p> <p>Note that changes to the bootstrap script start being used by users as soon as your PR merges.  Since this script can only fetch a released version of the executables, changes to this script can not rely on any behavior of those executables that is not in the currently latest release.  Also, a change that restricts the range of usable releases needs to add checking for use of incompatible releases.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#testing-the-bootstrap-script-against-an-upcoming-release","title":"Testing the bootstrap script against an upcoming release","text":"<p>Prior to making a new release, there needs to be testing that the current bootstrap script works with the executable behavior that will appear in the new release.  To support this we will add an option to the bootstrap script that enables it to use a local release archive instead of fetching an archive of an actual release from github.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#licensing","title":"Licensing","text":"<p>KubeStellar is Apache 2.0 licensed and we accept contributions via GitHub pull requests.</p> <p>Please read the following guide if you're interested in contributing to KubeStellar.</p>"},{"location":"Contribution%20guidelines/CONTRIBUTING/#certificate-of-origin","title":"Certificate of Origin","text":"<p>By contributing to this project you agree to the Developer Certificate of Origin (DCO). This document was created by the Linux Kernel community and is a simple statement that you, as a contributor, have the legal right to make the contribution. See the DCO file for details.</p>"},{"location":"Contribution%20guidelines/LICENSE/","title":"License","text":"<p>Apache License Version 2.0, January 2004 http://www.apache.org/licenses/  TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <p>To apply the Apache License to your work, attach the following    boilerplate notice, with the fields enclosed by brackets \"[]\"    replaced with your own identifying information. (Don't include    the brackets!)  The text should be enclosed in the appropriate    comment syntax for the file format. We also recommend that a    file or class name and description of purpose be included on the    same \"printed page\" as the copyright notice for easier    identification within third-party archives.</p> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"Contribution%20guidelines/coc/","title":"Code of Conduct","text":"<p>The Code of Conduct serves as a set of rules used by the KubeStellar community to establish a safe, respectful and inclusive environment.</p>"},{"location":"Contribution%20guidelines/coc/#kubestellar-community-code-of-conduct","title":"KubeStellar Community Code of Conduct","text":"<p>The KubeStellar Community abides by the CNCF Code of Conduct.</p> <p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the KubeStellar Code of Conduct Committee of Conduct Committee.</p>"},{"location":"Contribution%20guidelines/coc/#contributor-code-of-conduct","title":"Contributor Code of Conduct","text":"<p>As contributors and maintainers of this project, and in the interest of fostering an open and welcoming community, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities.</p> <p>We are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion, or nationality.</p> <p>Examples of unacceptable behavior by participants include:</p> <p>The use of sexualized language or imagery Personal attacks Trolling or insulting/derogatory comments Public or private harassment Publishing others\u2019 private information, such as physical or electronic addresses, without explicit permission Other unethical or unprofessional conduct. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. By adopting this Code of Conduct, project maintainers commit themselves to fairly and consistently applying these principles to every aspect of managing this project. Project maintainers who do not follow or enforce the Code of Conduct may be permanently removed from the project team.</p> <p>This code of conduct applies both within project spaces and in public spaces when an individual is representing the project or its community.</p> <p>Instances of abusive, harassing, or otherwise unacceptable behavior in KubeStellar may be reported by contacting the KubeStellar Code of Conduct Committee of Conduct Committee. For other projects, please contact a CNCF project maintainer or our mediator, Mishi Choudhary mishi@linux.com.</p> <p>This Code of Conduct is adapted from the Contributor Covenant (http://contributor-covenant.org), version 1.2.0, available at http://contributor-covenant.org/version/1/2/0/</p> <p>CNCF Events Code of Conduct</p> <p>CNCF events are governed by the Linux Foundation Code of Conduct available on the event page. This is designed to be compatible with the above policy and also includes more details on responding to incidents.</p>"},{"location":"Contribution%20guidelines/governance/","title":"Governance","text":""},{"location":"Contribution%20guidelines/governance/#kubestellar-project-governance","title":"KubeStellar Project Governance","text":"<p>The KubeStellar project is dedicated to solving challenges stemming from multicluster configuration management for edge, multi-cloud, and hybrid cloud.  This governance explains how the project is run.</p> <ul> <li>Manifesto</li> <li>Values</li> <li>Maintainers</li> <li>Code of Conduct Enforcement</li> <li>Security Response Team</li> <li>Voting</li> <li>Modifying this Charter</li> </ul>"},{"location":"Contribution%20guidelines/governance/#manifesto","title":"Manifesto","text":"<ul> <li>KubeStellar Maintainers strive to be good citizens in the Kubernetes project.</li> <li>KubeStellar Maintainers see KubeStellar always as part of the Kubernetes ecosystem and always     strive to keep that ecosystem united. In particular, this means:</li> <li>KubeStellar strives to not divert from Kubernetes, but strives to extend its       use-cases to non-container control planes while keeping the ecosystems of       libraries and tooling united.</li> <li>KubeStellar \u2013 as a consumer of Kubernetes API Machinery \u2013 will strive to stay 100%       compatible with the semantics of Kubernetes APIs, while removing container       orchestration specific functionality.</li> <li>KubeStellar strives to upstream changes to Kubernetes code as much as possible.</li> </ul>"},{"location":"Contribution%20guidelines/governance/#values","title":"Values","text":"<p>The KubeStellar and its leadership embrace the following values:</p> <ul> <li>Openness: Communication and decision-making happens in the open and is     discoverable for future reference. As much as possible, all discussions and     work take place in public forums and open repositories.</li> <li>Fairness: All stakeholders have the opportunity to provide feedback and     submit contributions, which will be considered on their merits.</li> <li>Community over Product or Company: Sustaining and growing our community     takes priority over shipping code or sponsors' organizational goals. Each     contributor participates in the project as an individual.</li> <li>Inclusivity: We innovate through different perspectives and skill sets,     which can only be accomplished in a welcoming and respectful environment.</li> <li>Participation: Responsibilities within the project are earned through     participation, and there is a clear path up the contributor ladder into     leadership positions.</li> </ul>"},{"location":"Contribution%20guidelines/governance/#maintainers","title":"Maintainers","text":"<p>KubeStellar Maintainers have write access to the project GitHub repository. They can merge their own patches or patches from others. The current maintainers can be found as top-level approvers in OWNERS.  Maintainers collectively  manage the project's resources and contributors.</p> <p>This privilege is granted with some expectation of responsibility: maintainers are people who care about the KubeStellar project and want to help it grow and improve. A maintainer is not just someone who can make changes, but someone who has demonstrated their ability to collaborate with the team, get the most knowledgeable people to review code and docs, contribute high-quality code, and follow through to fix issues (in code or tests).</p> <p>A maintainer is a contributor to the project's success and a citizen helping the project succeed.</p> <p>The collective team of all Maintainers is known as the Maintainer Council, which  is the governing body for the project.</p>"},{"location":"Contribution%20guidelines/governance/#becoming-a-maintainer","title":"Becoming a Maintainer","text":"<p>To become a Maintainer you need to demonstrate the following:</p> <ul> <li>commitment to the project:<ul> <li>participate in discussions, contributions, code and documentation reviews   for 3 months or more,</li> <li>perform reviews for 5 non-trivial pull requests,</li> <li>contribute 5 non-trivial pull requests and have them merged,</li> </ul> </li> <li>ability to write quality code and/or documentation,</li> <li>ability to collaborate with the team,</li> <li>understanding of how the team works (policies, processes for testing and code review, etc),</li> <li>understanding of the project's code base and coding and documentation style.    </li> </ul> <p>A new Maintainer must be proposed by an existing maintainer by sending a message to the developer mailing list. A simple majority  vote of existing Maintainers approves the application.</p> <p>Maintainers who are selected will be granted the necessary GitHub rights, and invited to the private maintainer mailing list.</p>"},{"location":"Contribution%20guidelines/governance/#bootstrapping-maintainers","title":"Bootstrapping Maintainers","text":"<p>To bootstrap the process, 3 maintainers are defined (in the initial PR adding  this to the repository) that do not necessarily follow the above rules. When a  new maintainer is added following the above rules, the existing maintainers  define one not following the rules to step down, until all of them follow the  rules.</p>"},{"location":"Contribution%20guidelines/governance/#removing-a-maintainer","title":"Removing a Maintainer","text":"<p>Maintainers may resign at any time if they feel that they will not be able to  continue fulfilling their project duties.</p> <p>Maintainers may also be removed after being inactive, failure to fulfill their  Maintainer responsibilities, violating the Code of Conduct, or other reasons.  Inactivity is defined as a period of very low or no activity in the project for  a year or more, with no definite schedule to return to full Maintainer activity.</p> <p>A Maintainer may be removed at any time by a 2/3 vote of the remaining maintainers.</p> <p>Depending on the reason for removal, a Maintainer may be converted to Emeritus  status. Emeritus Maintainers will still be consulted on some project matters,  and can be rapidly returned to Maintainer status if their availability changes.</p>"},{"location":"Contribution%20guidelines/governance/#meetings","title":"Meetings","text":"<p>Time zones permitting, Maintainers are expected to participate in the public  community call meeting. Maintainers will also have closed meetings in order to  discuss security reports or Code of Conduct violations. Such meetings should be  scheduled by any Maintainer on receipt of a security issue or CoC report.  All current Maintainers must be invited to such closed meetings, except for any  Maintainer who is accused of a CoC violation.</p>"},{"location":"Contribution%20guidelines/governance/#code-of-conduct","title":"Code of Conduct","text":"<p>Code of Conduct violations by community members will be discussed and resolved on the private Maintainer mailing list.</p>"},{"location":"Contribution%20guidelines/governance/#security-response-team","title":"Security Response Team","text":"<p>The Maintainers will appoint a Security Response Team to handle security reports. This committee may simply consist of the Maintainer Council themselves. If this  responsibility is delegated, the Maintainers will appoint a team of at least two  contributors to handle it. The Maintainers will review who is assigned to this  at least once a year.</p> <p>The Security Response Team is responsible for handling all reports of security  holes and breaches according to the security policy.</p>"},{"location":"Contribution%20guidelines/governance/#voting","title":"Voting","text":"<p>While most business in KubeStellar is conducted by \"lazy consensus\", periodically the Maintainers may need to vote on specific actions or changes. A vote can be taken on the developer mailing list or the private Maintainer mailing list for security or conduct matters.  Votes may also be taken at the community call  meeting. Any Maintainer may demand a vote be taken.</p> <p>Most votes require a simple majority of all Maintainers to succeed. Maintainers can be removed by a 2/3 majority vote of all Maintainers, and changes to this Governance require a 2/3 vote of all Maintainers.</p>"},{"location":"Contribution%20guidelines/governance/#modifying-this-charter","title":"Modifying this Charter","text":"<p>Changes to this Governance and its supporting documents may be approved by a  2/3 vote of the Maintainers.</p>"},{"location":"Contribution%20guidelines/onboarding/","title":"Onboarding","text":"<p>KubeStellar GitHub Organization On-boarding and Off-boarding Policy</p> <p>Effective Date: June 1st, 2023</p> <p>At KubeStellar we love our contributors.  Our contributors can make various valuable contributions to our project. They can actively engage in code development by submitting pull requests, implementing new features, or fixing bugs. Additionally, contributors can assist with testing, CICD, documentation, providing clear and comprehensive guides, tutorials, and examples. Moreover, they can contribute to the project by participating in discussions, offering feedback, and helping to improve overall community engagement and collaboration.</p> <ol> <li> <p>Introduction: The purpose of this policy is to ensure a smooth on-boarding and off-boarding process for members of the KubeStellar GitHub organization. This policy applies to all individuals joining or leaving the organization, including community contributors.</p> </li> <li> <p>On-boarding Process: 2.1. Access Request:</p> </li> <li>New members shall submit an access request, via a blank GitHub issue from the KubeStellar repository, mentioning all members of the OWNERS file.</li> <li>The access request should include the member's GitHub username and a brief description of their role and contributions to the KubeStellar project.</li> </ol> <p>2.2. Review and Approval: - The organization's maintainers or designated personnel will review the access request issue. - The maintainers will evaluate the request based on the member's role, contributions, and adherence to the organization's code of conduct. - Upon approval, the member will receive an invitation to join the KubeStellar GitHub organization.</p> <p>2.3. Getting Help: - The organization's maintainers are here to help contributors be efficient and confident in their collaboration effort. If you need help you can reach out to the maintainers on slack at the KubeStellar-dev channel. - Be sure to join the KubeStellar-dev Google Group to get access to important artifacts like proposals, diagrams, and meeting invitations.</p> <p>2.4. Orientation: - Newly on-boarded members will be provided with contribution guidelines. - The guide will include instructions on how to access relevant repositories, participate in discussions, and contribute to ongoing projects.</p> <ol> <li>Off-boarding Process: 3.1. Departure Notification:</li> <li>Members leaving the organization shall notify the maintainers or their respective team lead in advance of their departure date.</li> <li>The notification should include the member's departure date and any necessary transition information.</li> </ol> <p>3.2. Access Termination: - Upon receiving the departure notification, the maintainers or designated personnel will initiate the off-boarding process. - The member's access to the KubeStellar GitHub organization will be revoked promptly to ensure data security and prevent unauthorized access.</p> <p>3.3. Knowledge Transfer: - Departing members should facilitate the transfer of their ongoing projects, tasks, and knowledge to their respective replacements or relevant team members. - Documentation or guidelines related to ongoing projects should be updated and made available to the team for seamless continuity.</p> <ol> <li>Code of Conduct:</li> <li>All members of the KubeStellar GitHub organization are expected to adhere to the organization's code of conduct, promoting a respectful and inclusive environment.</li> <li> <p>Violations of the code of conduct will be addressed following the organization's established procedures for handling such incidents.</p> </li> <li> <p>Policy Compliance:</p> </li> <li>It is the responsibility of all members to comply with the on-boarding and off-boarding policy.</li> <li> <p>The organization's maintainers or designated personnel will oversee the implementation and enforcement of this policy.</p> </li> <li> <p>Policy Review:</p> </li> <li>This policy will be reviewed periodically to ensure its effectiveness and relevance.</li> <li>Any updates or revisions to the policy will be communicated to the organization's members in a timely manner.</li> </ol> <p>Please note that this policy is subject to change, and any modifications will be communicated to all members of the KubeStellar GitHub organization.</p> <p>By joining the organization, all members agree to abide by the terms and guidelines outlined in this policy.</p> <p>Andy Anderson (clubanderson) KubeStellar Maintainer June 1, 2023</p>"},{"location":"Contribution%20guidelines/operations/all-macros/","title":"All macros","text":"<p>All variables supported by this documentation implementation:</p>"},{"location":"Contribution%20guidelines/operations/all-macros/#macros-plugin-environment","title":"Macros Plugin Environment","text":""},{"location":"Contribution%20guidelines/operations/all-macros/#general-list","title":"General List","text":"<p>All available variables and filters within the macros plugin:</p> Variable Type Content extra dict version [dict], analytics [dict] config MkDocsConfig {'config_file_path': '/home/runner/work/edge-mc/edge-mc/docs/mike-mkdocsnvl5k_p1.yml', 'site_name': 'KubeStellar', 'nav': [{'Home': 'index.md'}, {'QuickStart': 'Getting-Started/quickstart.md'}, {'Contributing': [{'Guidelines': 'Contribution guidelines/CONTRIBUTING.md'}, {'Code of Conduct': 'Contribution guidelines/coc.md'}, {'License': 'Contribution guidelines/LICENSE.md'}, {'Governance': 'Contribution guidelines/governance.md'}, {'Onboarding': 'Contribution guidelines/onboarding.md'}, {'Security': [{'Policy': 'Contribution guidelines/security/security.md'}, {'Contacts': 'Contribution guidelines/security/security_contacts.md'}]}, {'Operations': [{'Code Management': 'Contribution guidelines/operations/code-management.md'}, {'Release Management': 'Contribution guidelines/operations/release-management.md'}, {'Document Management': 'Contribution guidelines/operations/document-management.md'}]}]}, {'Coding Milestones': [{'PoC2023q1': [{'Details': 'Coding Milestones/PoC2023q1/outline.md'}, {'Inivitation to Contribute': 'Coding Milestones/PoC2023q1/coding-milestone-invite-q1.md'}, {'KubeStellar-Syncer': 'Coding Milestones/PoC2023q1/kubestellar-syncer.md'}, {'Extended Example': 'Coding Milestones/PoC2023q1/example1.md'}, {'KubeStellar Processes': [{'KubeStellar Scheduler': 'Coding Milestones/PoC2023q1/kubestellar-scheduler.md'}, {'KubeStellar Mailbox Controller': 'Coding Milestones/PoC2023q1/mailbox-controller.md'}, {'KubeStellar Placement Translator': 'Coding Milestones/PoC2023q1/placement-translator.md'}]}, {'Roadmap': 'Coding Milestones/PoC2023q1/roadmap-uses.md'}, {'Environments': [{'Overview': 'Coding Milestones/PoC2023q1/environments/_index.md'}, {'Cloud Environment': 'Coding Milestones/PoC2023q1/environments/cloud-env.md'}, {'Developer Environment': 'Coding Milestones/PoC2023q1/environments/dev-env.md'}]}, {'Reference': [{'Commands': 'Coding Milestones/PoC2023q1/commands.md'}]}]}, {'PoC2023q3': [{'Details': 'Coding Milestones/PoC2023q3/outline.md'}]}]}, {'Community': 'Community/_index.md'}, {'Partners': [{'ArgoCD': 'Community/partners/argocd.md'}, {'Turbonomic': 'Community/partners/turbonomic.md'}, {'MVI': 'Community/partners/mvi.md'}, {'FluxCD': 'Community/partners/fluxcd.md'}, {'OpenZiti': 'Community/partners/openziti.md'}, {'Kyverno': 'Community/partners/kyverno.md'}]}, {'Getting Started': [{'QuickStart': 'Getting-Started/quickstart.md'}, {'Extended Example': 'Coding Milestones/PoC2023q1/example1.md'}, {'User Guide': 'Getting-Started/user-guide.md'}, {'KubeStellar - The Infomercial': 'Getting-Started/infomercial.md'}]}, {'Blog': 'https://medium.com/@kubestellar/list/predefined:e785a0675051:READING_LIST\" target=\"_blank'}], 'pages': None, 'site_url': 'https://docs.kubestellar.io/main', 'site_description': None, 'site_author': None, 'theme': Theme(name='material', dirs=['/home/runner/work/edge-mc/edge-mc/docs/venv/lib/python3.10/site-packages/mkdocs_static_i18n/custom_i18n_sitemap', '/home/runner/work/edge-mc/edge-mc/docs/overrides', '/home/runner/work/edge-mc/edge-mc/docs/venv/lib/python3.10/site-packages/material', '/home/runner/work/edge-mc/edge-mc/docs/venv/lib/python3.10/site-packages/mkdocs/templates'], static_templates=['404.html', 'sitemap.xml'], name='material', locale=Locale(language='en', territory=''), language='en', direction=None, features=['content.action.edit', 'content.action.view', 'navigation.indexes', 'navigation.tabs', 'navigation.path', 'navigation.footer', 'content.code.copy', 'content.code.annotate'], palette={'primary': None, 'accent': None}, font={'text': 'SapceMono', 'code': 'Roboto Mono'}, icon=None, favicon='favicons/favicon.ico', logo='logo.png'), 'docs_dir': '/home/runner/work/edge-mc/edge-mc/docs/content', 'site_dir': '/home/runner/work/edge-mc/edge-mc/docs/generated', 'copyright': None, 'google_analytics': None, 'dev_addr': _IpAddressValue(host='127.0.0.1', port=8000), 'use_directory_urls': True, 'repo_url': 'https://github.com/kcp-dev/edge-mc', 'repo_name': 'GitHub', 'edit_uri_template': None, 'edit_uri': 'edit/main/docs/content/', 'extra_css': ['stylesheets/kubestellar.css'], 'extra_javascript': ['js/open_in_new_tab.js'], 'extra_templates': [], 'markdown_extensions': ['toc', 'tables', 'fenced_code', 'pymdownx.superfences', 'attr_list', 'md_in_html', 'pymdownx.highlight', 'pymdownx.inlinehilite', 'pymdownx.snippets', 'pymdownx.tabbed', 'admonition'], 'mdx_configs': {'toc': {'permalink': '#'}, 'pymdownx.highlight': {'anchor_linenums': True}, 'pymdownx.tabbed': {'alternate_style': True}}, 'strict': False, 'remote_branch': 'gh-pages', 'remote_name': 'origin', 'extra': {'version': {'default': 'stable', 'provider': 'mike'}, 'analytics': {'provider': 'google', 'property': 'G-SR5TD1CXY7', 'feedback': {'title': 'Was this page helpful?', 'ratings': [{'icon': 'material/emoticon-happy-outline', 'name': 'This page was helpful', 'data': 1, 'note': 'Thanks for your feedback!'}, {'icon': 'material/emoticon-sad-outline', 'name': 'This page could be improved', 'data': 0, 'note': 'Thanks for your feedback! Help us improve this page by using our feedback form.'}]}}}, 'plugins': {'mike': , 'material/search': , 'open-in-new-tab': , 'include-markdown': , 'macros': , 'i18n': }, 'hooks': {}, 'watch': ['/home/runner/work/edge-mc/edge-mc/docs/mkdocs.yml', '/home/runner/work/edge-mc/edge-mc/docs/content', '/home/runner/work/edge-mc/edge-mc/docs/overrides'], 'repo_short_name': 'kcp-dev/edge-mc', 'repo_default_file_path': 'edge-mc', 'docs_url': 'https://docs.kubestellar.io', 'repo_raw_url': 'https://raw.githubusercontent.com/kcp-dev/edge-mc', 'ks_branch': 'main', 'ks_tag': 'latest'} environment dict system = 'Linux', system_version = '5.15.0-1038-azure', python_version = '3.10.11', mkdocs_version = '1.4.2', macros_plugin_version = '0.7.0', jinja2_version = '3.1.2' plugin LegacyConfig {'module_name': 'main', 'modules': [], 'include_dir': 'overrides', 'include_yaml': [], 'j2_block_start_string': '', 'j2_block_end_string': '', 'j2_variable_start_string': '', 'j2_variable_end_string': '', 'on_undefined': 'keep', 'on_error_fail': False, 'verbose': False} git dict status = True, date [datetime], short_commit = 'a380eaa7', commit = 'a380eaa78b9fc6259b6ab5c52d946a261d1d3aee', tag = '', author = 'kcp CI Bot', author_email = '134318005+kcp-ci-bot@users.noreply.github.com', committer = 'GitHub', committer_email = 'noreply@github.com', date_ISO = 'Sun Jun 11 15:09:50 2023 +0200', message = 'Merge pull request #590 from yana1205/rename-syncer-more\\n\\n:bug: Rename cliplugin directory from kcp-edge to kubestellar', raw = 'commit a380eaa78b9fc6259b6ab5c52d946a261d1d3aee\\nAuthor: kcp CI Bot &lt;134318005+kcp-ci-bot@users.noreply.github.com&gt;\\nDate:   Sun Jun 11 15:09:50 2023 +0200\\n\\n    Merge pull request #590 from yana1205/rename-syncer-more\\n    \\n    :bug: Rename cliplugin directory from kcp-edge to kubestellar', root_dir = '/home/runner/work/edge-mc/edge-mc' version dict default = 'stable', provider = 'mike' analytics dict provider = 'google', property = 'G-SR5TD1CXY7', feedback [dict] macros SuperDict context [function], macros_info [function], now [function], fix_url [function], section_items [function] filters dict pretty [function] filters_builtin dict abs [builtin_function_or_method], attr [function], batch [function], capitalize [function], center [function], count [builtin_function_or_method], d [function], default [function], dictsort [function], e [builtin_function_or_method], escape [builtin_function_or_method], filesizeformat [function], first [function], float [function], forceescape [function], format [function], groupby [function], indent [function], int [function], join [function], last [function], length [builtin_function_or_method], list [function], lower [function], items [function], map [function], min [function], max [function], pprint [function], random [function], reject [function], rejectattr [function], replace [function], reverse [function], round [function], safe [function], select [function], selectattr [function], slice [function], sort [function], string [builtin_function_or_method], striptags [function], sum [function], title [function], trim [function], truncate [function], unique [function], upper [function], urlencode [function], urlize [function], wordcount [function], wordwrap [function], xmlattr [function], tojson [function] navigation Navigation Page(title='Home', url='/main/')Page(title='QuickStart', url='/main/Getting-Started/quickstart/')Section(title='Contributing')    Page(title='Guidelines', url='/main/Contribution%20guidelines/CONTRIBUTING/')    Page(title='Code of Conduct', url='/main/Contribution%20guidelines/coc/')    Page(title='License', url='/main/Contribution%20guidelines/LICENSE/')    Page(title='Governance', url='/main/Contribution%20guidelines/governance/')    Page(title='Onboarding', url='/main/Contribution%20guidelines/onboarding/')    Section(title='Security')        Page(title='Policy', url='/main/Contribution%20guidelines/security/security/')        Page(title='Contacts', url='/main/Contribution%20guidelines/security/security_contacts/')    Section(title='Operations')        Page(title='Code Management', url='/main/Contribution%20guidelines/operations/code-management/')        Page(title='Release Management', url='/main/Contribution%20guidelines/operations/release-management/')        Page(title='Document Management', url='/main/Contribution%20guidelines/operations/document-management/')Section(title='Coding Milestones')    Section(title='PoC2023q1')        Page(title='Details', url='/main/Coding%20Milestones/PoC2023q1/outline/')        Page(title='Inivitation to Contribute', url='/main/Coding%20Milestones/PoC2023q1/coding-milestone-invite-q1/')        Page(title='KubeStellar-Syncer', url='/main/Coding%20Milestones/PoC2023q1/kubestellar-syncer/')        Page(title='Extended Example', url='/main/Coding%20Milestones/PoC2023q1/example1/')        Section(title='KubeStellar Processes')            Page(title='KubeStellar Scheduler', url='/main/Coding%20Milestones/PoC2023q1/kubestellar-scheduler/')            Page(title='KubeStellar Mailbox Controller', url='/main/Coding%20Milestones/PoC2023q1/mailbox-controller/')            Page(title='KubeStellar Placement Translator', url='/main/Coding%20Milestones/PoC2023q1/placement-translator/')        Page(title='Roadmap', url='/main/Coding%20Milestones/PoC2023q1/roadmap-uses/')        Section(title='Environments')            Page(title='Overview', url='/main/Coding%20Milestones/PoC2023q1/environments/_index/')            Page(title='Cloud Environment', url='/main/Coding%20Milestones/PoC2023q1/environments/cloud-env/')            Page(title='Developer Environment', url='/main/Coding%20Milestones/PoC2023q1/environments/dev-env/')        Section(title='Reference')            Page(title='Commands', url='/main/Coding%20Milestones/PoC2023q1/commands/')    Section(title='PoC2023q3')        Page(title='Details', url='/main/Coding%20Milestones/PoC2023q3/outline/')Page(title='Community', url='/main/Community/_index/')Section(title='Partners')    Page(title='ArgoCD', url='/main/Community/partners/argocd/')    Page(title='Turbonomic', url='/main/Community/partners/turbonomic/')    Page(title='MVI', url='/main/Community/partners/mvi/')    Page(title='FluxCD', url='/main/Community/partners/fluxcd/')    Page(title='OpenZiti', url='/main/Community/partners/openziti/')    Page(title='Kyverno', url='/main/Community/partners/kyverno/')Section(title='Getting Started')    Page(title='QuickStart', url='/main/Getting-Started/quickstart/')    Page(title='Extended Example', url='/main/Coding%20Milestones/PoC2023q1/example1/')    Page(title='User Guide', url='/main/Getting-Started/user-guide/')    Page(title='KubeStellar - The Infomercial', url='/main/Getting-Started/infomercial/')Link(title='Blog', url='https://medium.com/@kubestellar/list/predefined:e785a0675051:READING_LIST\" target=\"_blank') files I18nFiles page Page Page(title='All macros', url='/main/Contribution%20guidelines/operations/all-macros/')"},{"location":"Contribution%20guidelines/operations/all-macros/#config-information","title":"Config Information","text":"<p>Standard MkDocs configuration information. Do not try to modify.</p> <p>e.g. <code>{{ config.docs_dir }}</code></p> <p>See also the MkDocs documentation on the config object.</p> Variable Type Content config_file_path str '/home/runner/work/edge-mc/edge-mc/docs/mike-mkdocsnvl5k_p1.yml' site_name str 'KubeStellar' nav list [{'Home': 'index.md'}, {'QuickStart': 'Getting-Started/quickstart.md'}, {'Contributing': [{'Guidelines': 'Contribution guidelines/CONTRIBUTING.md'}, {'Code of Conduct': 'Contribution guidelines/coc.md'}, {'License': 'Contribution guidelines/LICENSE.md'}, {'Governance': 'Contribution guidelines/governance.md'}, {'Onboarding': 'Contribution guidelines/onboarding.md'}, {'Security': [{'Policy': 'Contribution guidelines/security/security.md'}, {'Contacts': 'Contribution guidelines/security/security_contacts.md'}]}, {'Operations': [{'Code Management': 'Contribution guidelines/operations/code-management.md'}, {'Release Management': 'Contribution guidelines/operations/release-management.md'}, {'Document Management': 'Contribution guidelines/operations/document-management.md'}]}]}, {'Coding Milestones': [{'PoC2023q1': [{'Details': 'Coding Milestones/PoC2023q1/outline.md'}, {'Inivitation to Contribute': 'Coding Milestones/PoC2023q1/coding-milestone-invite-q1.md'}, {'KubeStellar-Syncer': 'Coding Milestones/PoC2023q1/kubestellar-syncer.md'}, {'Extended Example': 'Coding Milestones/PoC2023q1/example1.md'}, {'KubeStellar Processes': [{'KubeStellar Scheduler': 'Coding Milestones/PoC2023q1/kubestellar-scheduler.md'}, {'KubeStellar Mailbox Controller': 'Coding Milestones/PoC2023q1/mailbox-controller.md'}, {'KubeStellar Placement Translator': 'Coding Milestones/PoC2023q1/placement-translator.md'}]}, {'Roadmap': 'Coding Milestones/PoC2023q1/roadmap-uses.md'}, {'Environments': [{'Overview': 'Coding Milestones/PoC2023q1/environments/_index.md'}, {'Cloud Environment': 'Coding Milestones/PoC2023q1/environments/cloud-env.md'}, {'Developer Environment': 'Coding Milestones/PoC2023q1/environments/dev-env.md'}]}, {'Reference': [{'Commands': 'Coding Milestones/PoC2023q1/commands.md'}]}]}, {'PoC2023q3': [{'Details': 'Coding Milestones/PoC2023q3/outline.md'}]}]}, {'Community': 'Community/_index.md'}, {'Partners': [{'ArgoCD': 'Community/partners/argocd.md'}, {'Turbonomic': 'Community/partners/turbonomic.md'}, {'MVI': 'Community/partners/mvi.md'}, {'FluxCD': 'Community/partners/fluxcd.md'}, {'OpenZiti': 'Community/partners/openziti.md'}, {'Kyverno': 'Community/partners/kyverno.md'}]}, {'Getting Started': [{'QuickStart': 'Getting-Started/quickstart.md'}, {'Extended Example': 'Coding Milestones/PoC2023q1/example1.md'}, {'User Guide': 'Getting-Started/user-guide.md'}, {'KubeStellar - The Infomercial': 'Getting-Started/infomercial.md'}]}, {'Blog': 'https://medium.com/@kubestellar/list/predefined:e785a0675051:READING_LIST\" target=\"_blank'}] pages NoneType None site_url str 'https://docs.kubestellar.io/main' site_description NoneType None site_author NoneType None theme Theme Theme(name='material', dirs=['/home/runner/work/edge-mc/edge-mc/docs/venv/lib/python3.10/site-packages/mkdocs_static_i18n/custom_i18n_sitemap', '/home/runner/work/edge-mc/edge-mc/docs/overrides', '/home/runner/work/edge-mc/edge-mc/docs/venv/lib/python3.10/site-packages/material', '/home/runner/work/edge-mc/edge-mc/docs/venv/lib/python3.10/site-packages/mkdocs/templates'], static_templates=['404.html', 'sitemap.xml'], name='material', locale=Locale(language='en', territory=''), language='en', direction=None, features=['content.action.edit', 'content.action.view', 'navigation.indexes', 'navigation.tabs', 'navigation.path', 'navigation.footer', 'content.code.copy', 'content.code.annotate'], palette={'primary': None, 'accent': None}, font={'text': 'SapceMono', 'code': 'Roboto Mono'}, icon=None, favicon='favicons/favicon.ico', logo='logo.png') docs_dir str '/home/runner/work/edge-mc/edge-mc/docs/content' site_dir str '/home/runner/work/edge-mc/edge-mc/docs/generated' copyright NoneType None google_analytics NoneType None dev_addr _IpAddressValue _IpAddressValue(host='127.0.0.1', port=8000) use_directory_urls bool True repo_url str 'https://github.com/kcp-dev/edge-mc' repo_name str 'GitHub' edit_uri_template NoneType None edit_uri str 'edit/main/docs/content/' extra_css list ['stylesheets/kubestellar.css'] extra_javascript list ['js/open_in_new_tab.js'] extra_templates list [] markdown_extensions list ['toc', 'tables', 'fenced_code', 'pymdownx.superfences', 'attr_list', 'md_in_html', 'pymdownx.highlight', 'pymdownx.inlinehilite', 'pymdownx.snippets', 'pymdownx.tabbed', 'admonition'] mdx_configs dict toc [dict], pymdownx.highlight [dict], pymdownx.tabbed [dict] strict bool False remote_branch str 'gh-pages' remote_name str 'origin' extra LegacyConfig {'version': {'default': 'stable', 'provider': 'mike'}, 'analytics': {'provider': 'google', 'property': 'G-SR5TD1CXY7', 'feedback': {'title': 'Was this page helpful?', 'ratings': [{'icon': 'material/emoticon-happy-outline', 'name': 'This page was helpful', 'data': 1, 'note': 'Thanks for your feedback!'}, {'icon': 'material/emoticon-sad-outline', 'name': 'This page could be improved', 'data': 0, 'note': 'Thanks for your feedback! Help us improve this page by using our feedback form.'}]}}} plugins PluginCollection mike [MikePlugin], material/search [SearchPlugin], open-in-new-tab [OpenInNewTabPlugin], include-markdown [IncludeMarkdownPlugin], macros [MacrosPlugin], i18n [I18n] hooks dict watch list ['/home/runner/work/edge-mc/edge-mc/docs/mkdocs.yml', '/home/runner/work/edge-mc/edge-mc/docs/content', '/home/runner/work/edge-mc/edge-mc/docs/overrides'] repo_short_name str 'kcp-dev/edge-mc' repo_default_file_path str 'edge-mc' docs_url str 'https://docs.kubestellar.io' repo_raw_url str 'https://raw.githubusercontent.com/kcp-dev/edge-mc' ks_branch str 'main' ks_tag str 'latest'"},{"location":"Contribution%20guidelines/operations/all-macros/#macros","title":"Macros","text":"<p>These macros have been defined programmatically for this environment (module or pluglets). </p> Variable Type Content context function (obj, e) <p>Default mkdocs_macro List the defined variables</p> macros_info function () <p>Test/debug function:         list useful documentation on the mkdocs_macro environment.</p> now function () <p>Get the current time (returns a datetime object).          Used alone, it provides a timestamp.         To get the year use <code>now().year</code>, for the month number          <code>now().month</code>, etc.</p> fix_url function (url, r) <p>If url is relative, fix it so that it points to the docs diretory.     This is necessary because relative links in markdown must be adapted     in html ('img/foo.png' =&gt; '../img/img.png').</p> section_items function (page, nav, config, children, siblings, child) <p>Returns a list of all pages that are siblings to page.</p>"},{"location":"Contribution%20guidelines/operations/all-macros/#git-information","title":"Git Information","text":"<p>Information available on the last commit and the git repository containing the documentation project:</p> <p>e.g. <code>{{ git.message }}</code></p> Variable Type Content status bool True date datetime datetime.datetime(2023, 6, 11, 15, 9, 50, tzinfo=tzoffset(None, 7200)) short_commit str 'a380eaa7' commit str 'a380eaa78b9fc6259b6ab5c52d946a261d1d3aee' tag str '' author str 'kcp CI Bot' author_email str '134318005+kcp-ci-bot@users.noreply.github.com' committer str 'GitHub' committer_email str 'noreply@github.com' date_ISO str 'Sun Jun 11 15:09:50 2023 +0200' message str 'Merge pull request #590 from yana1205/rename-syncer-more\\n\\n:bug: Rename cliplugin directory from kcp-edge to kubestellar' raw str 'commit a380eaa78b9fc6259b6ab5c52d946a261d1d3aee\\nAuthor: kcp CI Bot &lt;134318005+kcp-ci-bot@users.noreply.github.com&gt;\\nDate:   Sun Jun 11 15:09:50 2023 +0200\\n\\n    Merge pull request #590 from yana1205/rename-syncer-more\\n    \\n    :bug: Rename cliplugin directory from kcp-edge to kubestellar' root_dir str '/home/runner/work/edge-mc/edge-mc'"},{"location":"Contribution%20guidelines/operations/all-macros/#page-attributes","title":"Page Attributes","text":"<p>Provided by MkDocs. These attributes change for every page (the attributes shown are for this page).</p> <p>e.g. <code>{{ page.title }}</code></p> <p>See also the MkDocs documentation on the page object.</p> Variable Type Content file I18nFile I18nFile(src_path='Contribution guidelines/operations/all-macros.md', abs_src_path='/home/runner/work/edge-mc/edge-mc/docs/content/Contribution guidelines/operations/all-macros.md', dest_path='Contribution guidelines/operations/all-macros/index.html', abs_dest_path='/home/runner/work/edge-mc/edge-mc/docs/generated/Contribution guidelines/operations/all-macros/index.html', name='all-macros', locale_suffix='None', dest_language='', dest_name='all-macros.md', url='Contribution%20guidelines/operations/all-macros/') title str 'All macros' parent NoneType None children NoneType None previous_page NoneType None next_page NoneType None _Page__active bool False update_date str '2023-06-11' canonical_url str 'https://docs.kubestellar.io/main/Contribution%20guidelines/operations/all-macros/' abs_url str '/main/Contribution%20guidelines/operations/all-macros/' edit_url str 'https://github.com/kcp-dev/edge-mc/edit/main/docs/content/Contribution guidelines/operations/all-macros.md' markdown str 'All variables supported by this documentation implementation:\\n\\n{{ macros_info() }}' content NoneType None toc list [] meta dict <p>To have all titles of all pages, use:</p> <pre><code>{% for page in navigation.pages %}\n- {{ page.title }}\n{% endfor %}\n</code></pre>"},{"location":"Contribution%20guidelines/operations/all-macros/#plugin-filters","title":"Plugin Filters","text":"<p>These filters are provided as a standard by the macros plugin.</p> Variable Type Content pretty function (var_list, rows, header, e) <p>Default mkdocs_macro Prettify a dictionary or object          (used for environment documentation, or debugging).</p>"},{"location":"Contribution%20guidelines/operations/all-macros/#builtin-jinja2-filters","title":"Builtin Jinja2 Filters","text":"<p>These filters are provided by Jinja2 as a standard.</p> <p>See also the Jinja2 documentation on builtin filters).</p> Variable Type Content abs builtin_function_or_method <p>Return the absolute value of the argument.</p> attr function (environment, obj, name, value) <p>Get an attribute of an object.  <code>foo|attr(\"bar\")</code> works like     <code>foo.bar</code> just that always an attribute is returned and items are not     looked up.</p> batch function (value, linecount, fill_with, tmp, item) <p>A filter that batches items. It works pretty much like <code>slice</code>     just the other way round. It returns a list of lists with the     given number of items. If you provide a second parameter this     is used to fill up missing items. See this example.</p> capitalize function (s) <p>Capitalize a value. The first character will be uppercase, all others     lowercase.</p> center function (value, width) <p>Centers the value in a field of a given width.</p> count builtin_function_or_method <p>Return the number of items in a container.</p> d function (value, default_value, boolean) <p>If the value is undefined it will return the passed default value,     otherwise the value of the variable.</p> default function (value, default_value, boolean) <p>If the value is undefined it will return the passed default value,     otherwise the value of the variable.</p> dictsort function (value, case_sensitive, by, reverse, sort_func) <p>Sort a dict and yield (key, value) pairs. Python dicts may not     be in the order you want to display them in, so sort them first.</p> e builtin_function_or_method <p>Replace the characters <code>&amp;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>'</code>, and <code>\"</code> in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML.</p> escape builtin_function_or_method <p>Replace the characters <code>&amp;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>'</code>, and <code>\"</code> in the string with HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML.</p> filesizeformat function (value, binary, bytes, base, prefixes, i, prefix, unit) <p>Format the value like a 'human-readable' file size (i.e. 13 kB,     4.1 MB, 102 Bytes, etc).  Per default decimal prefixes are used (Mega,     Giga, etc.), if the second parameter is set to <code>True</code> the binary     prefixes are used (Mebi, Gibi).</p> first function (args, kwargs, b) <p>Return the first item of a sequence.</p> float function (value, default) <p>Convert the value into a floating point number. If the     conversion doesn't work it will return <code>0.0</code>. You can     override this default using the first parameter.</p> forceescape function (value) <p>Enforce HTML escaping.  This will probably double escape variables.</p> format function (value, args, kwargs) <p>Apply the given values to a <code>printf-style</code>_ format string, like     <code>string % values</code>.</p> groupby function (args, kwargs, b) <p>Group a sequence of objects by an attribute using Python's     :func:<code>itertools.groupby</code>. The attribute can use dot notation for     nested access, like <code>\"address.city\"</code>. Unlike Python's <code>groupby</code>,     the values are sorted first so only one group is returned for each     unique value.</p> indent function (s, width, first, blank, newline, rv, lines) <p>Return a copy of the string with each line indented by 4 spaces. The     first line and blank lines are not indented by default.</p> int function (value, default, base) <p>Convert the value into an integer. If the     conversion doesn't work it will return <code>0</code>. You can     override this default using the first parameter. You     can also override the default base (10) in the second     parameter, which handles input with prefixes such as     0b, 0o and 0x for bases 2, 8 and 16 respectively.     The base is ignored for decimal numbers and non-string values.</p> join function (args, kwargs, b) <p>Return a string which is the concatenation of the strings in the     sequence. The separator between elements is an empty string per     default, you can define it with the optional parameter.</p> last function (environment, seq) <p>Return the last item of a sequence.</p> length builtin_function_or_method <p>Return the number of items in a container.</p> list function (args, kwargs, b) <p>Convert the value into a list.  If it was a string the returned list     will be a list of characters.</p> lower function (s) <p>Convert a value to lowercase.</p> items function (value) <p>Return an iterator over the <code>(key, value)</code> items of a mapping.</p> map function (args, kwargs, b) <p>Applies a filter on a sequence of objects or looks up an attribute.     This is useful when dealing with lists of objects but you are really     only interested in a certain value of it.</p> min function (environment, value, case_sensitive, attribute) <p>Return the smallest item from the sequence.</p> max function (environment, value, case_sensitive, attribute) <p>Return the largest item from the sequence.</p> pprint function (value) <p>Pretty print a variable. Useful for debugging.</p> random function (context, seq) <p>Return a random item from the sequence.</p> reject function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to each object,     and rejecting the objects with the test succeeding.</p> rejectattr function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to the specified     attribute of each object, and rejecting the objects with the test     succeeding.</p> replace function (eval_ctx, s, old, new, count) <p>Return a copy of the value with all occurrences of a substring     replaced with a new one. The first argument is the substring     that should be replaced, the second is the replacement string.     If the optional third argument <code>count</code> is given, only the first     <code>count</code> occurrences are replaced.</p> reverse function (value, rv, e) <p>Reverse the object or return an iterator that iterates over it the other     way round.</p> round function (value, precision, method, func) <p>Round the number to a given precision. The first     parameter specifies the precision (default is <code>0</code>), the     second the rounding method.</p> safe function (value) <p>Mark the value as safe which means that in an environment with automatic     escaping enabled this variable will not be escaped.</p> select function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to each object,     and only selecting the objects with the test succeeding.</p> selectattr function (args, kwargs, b) <p>Filters a sequence of objects by applying a test to the specified     attribute of each object, and only selecting the objects with the     test succeeding.</p> slice function (args, kwargs, b) <p>Slice an iterator and return a list of lists containing     those items. Useful if you want to create a div containing     three ul tags that represent columns.</p> sort function (environment, value, reverse, case_sensitive, attribute, key_func) <p>Sort an iterable using Python's :func:<code>sorted</code>.</p> string builtin_function_or_method <p>Convert an object to a string if it isn't already. This preserves a :class:<code>Markup</code> string rather than converting it back to a basic string, so it will still be marked as safe and won't be escaped again.</p> striptags function (value) <p>Strip SGML/XML tags and replace adjacent whitespace by one space.</p> sum function (args, kwargs, b) <p>Returns the sum of a sequence of numbers plus the value of parameter     'start' (which defaults to 0).  When the sequence is empty it returns     start.</p> title function (s) <p>Return a titlecased version of the value. I.e. words will start with     uppercase letters, all remaining characters are lowercase.</p> trim function (value, chars) <p>Strip leading and trailing characters, by default whitespace.</p> truncate function (env, s, length, killwords, end, leeway, result) <p>Return a truncated copy of the string. The length is specified     with the first parameter which defaults to <code>255</code>. If the second     parameter is <code>true</code> the filter will cut the text at length. Otherwise     it will discard the last word. If the text was in fact     truncated it will append an ellipsis sign (<code>\"...\"</code>). If you want a     different ellipsis sign than <code>\"...\"</code> you can specify it using the     third parameter. Strings that only exceed the length by the tolerance     margin given in the fourth parameter will not be truncated.</p> unique function (environment, value, case_sensitive, attribute, getter, seen, item, key) <p>Returns a list of unique items from the given iterable.</p> upper function (s) <p>Convert a value to uppercase.</p> urlencode function (value, items) <p>Quote data for use in a URL path or query using UTF-8.</p> urlize function (eval_ctx, value, trim_url_limit, nofollow, target, rel, extra_schemes, policies, rel_parts, scheme, rv) <p>Convert URLs in text into clickable links.</p> wordcount function (s) <p>Count the words in that string.</p> wordwrap function (environment, s, width, break_long_words, wrapstring, break_on_hyphens) <p>Wrap a string to the given width. Existing newlines are treated     as paragraphs to be wrapped separately.</p> xmlattr function (eval_ctx, d, autospace, rv) <p>Create an SGML/XML attribute string based on the items in a dict.     All values that are neither <code>none</code> nor <code>undefined</code> are automatically     escaped.</p> tojson function (eval_ctx, value, indent, policies, dumps, kwargs) <p>Serialize an object to a string of JSON, and mark it safe to     render in HTML. This filter is only for use in HTML documents.</p>"},{"location":"Contribution%20guidelines/operations/code-management/","title":"Code Management","text":""},{"location":"Contribution%20guidelines/operations/document-management/","title":"Document Management","text":""},{"location":"Contribution%20guidelines/operations/document-management/#overview","title":"Overview","text":"<p>Our documentation is powered by Material for MkDocs with some  additional plugins and tools:</p> <ul> <li>awesome-pages plugin</li> <li>macros plugin</li> <li>mike for multiple version support</li> </ul> <p>We have support in place for multiple languages (i18n), although we currently only have documentation in English. If  you're interested in contributing translations, please let us know!</p>"},{"location":"Contribution%20guidelines/operations/document-management/#file-structure","title":"File structure","text":"<p>All documentation-related items live in <code>docs</code> (with the small exception of various <code>make</code> targets and some helper  scripts in <code>hack</code>).</p> <p>The structure of <code>docs</code> is as follows:</p> Path Description config/$language/mkdocs.yml Language-specific <code>mkdocs</code> configuration. content/$language Language-specific website content. generated/branch All generated content for all languages for the current version. generated/branch/$language Generated content for a single language. Never added to git. generated/branch/index.html Minimal index for the current version that redirects to the default language (en) overrides Global (not language-specific) content. Dockerfile Builds the kubestellar-docs image containing mkdocs + associated tooling. mkdocs.yml Minimal <code>mkdocs</code> configuration for <code>mike</code> for multi-version support. requirements.txt List of Python modules used to build the site."},{"location":"Contribution%20guidelines/operations/document-management/#global-variables","title":"Global Variables","text":"<p>There are many global variables defined in the docs/mkdocs.yml.  The following are some very common variables you are encouraged to use in our documentation.  Use of these variables/macros allows our documentation to have github branch context and take advantage of our evolution without breaking</p> <pre><code>- site_name: KubeStellar\n- repo_url: https://github.com/kcp-dev/edge-mc\n- site_url: https://docs.kubestellar.io/main\n- repo_default_file_path: {{ no such element: mkdocs.config.defaults.MkDocsConfig object['repo_default_path'] }}\n- repo_short_name: kcp-dev/edge-mc\n- docs_url: https://docs.kubestellar.io\n- repo_raw_url: https://raw.githubusercontent.com/kcp-dev/edge-mc\n- edit_uri: edit/main/docs/content/\n- ks_branch: main\n- ks_tag: latest\n</code></pre> <p>to use a variables/macro in your documentation reference like this:</p> <p>{{ config.&lt;var_name&gt; }}</p> <p>and in context that can look something like this:</p> <p>bash &lt;(curl -s {{ config.repo_raw_url }}/{{ config.ks_branch }}/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version {{ config.ks_tag }}</p> <p>note:  \u00a0\u00a0\u00a0\u00a0- A more extensive and detailed list is located at mkdocs information  \u00a0\u00a0\u00a0\u00a0- We also check for broken links as part of our PR pipeline.  For more information check out our Broken Links Crawler</p>"},{"location":"Contribution%20guidelines/operations/document-management/#including-external-markdown","title":"Including external markdown","text":"<p>We make extensive use of 'include-markdown' to help us keep our documentation modular and up-to-date.  To use 'include-markdown' you must add a block in your document that refers to a block in your external document content:</p> <p>In your original markdown document, add a block that refers to the external markdown you want to include: </p> <p>In the document you want to include, add the start and end tags you configured in the include-markdown block in your original document: </p> <p>for more information on the 'include-markdown' plugin for mkdocs look here</p>"},{"location":"Contribution%20guidelines/operations/document-management/#serving-up-documents-locally","title":"Serving up documents locally","text":"<p>You can view and modify our documentation in your local development environment.  Simply checkout one of our branches.</p> <pre><code>git clone git@github.com:kcp-dev/edge-mc.git\ncd {{ no such element: mkdocs.config.defaults.MkDocsConfig object['repo_default_path'] }}/docs\ngit checkout main\n</code></pre> <p>You can view and modify our documentation in the branch you have checked out by using <code>mkdocs serve</code> from mkdocs:</p> <p><pre><code>pip install -r requirements.txt\nmkdocs serve\n</code></pre> Then open a browser to <code>http://localhost:8000/</code></p> <p>Another way to view (not modify - this method reflects what has been deployed to the <code>gh-pages</code> branch of our repo) all branches/versions of our documentation locally using 'mike' mike for mkdocs:</p> <p><pre><code>git clone git@github.com:kcp-dev/edge-mc.git\ncd {{ no such element: mkdocs.config.defaults.MkDocsConfig object['repo_default_path'] }}\ngit checkout main\nmake serve-docs\n</code></pre> Then open a browser to <code>http://localhost:8000/</code></p>"},{"location":"Contribution%20guidelines/operations/document-management/#supported-aliases-for-our-documentation","title":"Supported aliases for our documentation","text":"<p>We currently support 3 aliases for our documentation:</p> <pre><code>- from the release major.minor branch:\n    - [https://docs.kubestellar.io/stable](../../../https://docs.kubestellar.io/stable)\n- from the main branch:\n    - [https://docs.kubestellar.io/unstable](../../../https://docs.kubestellar.io/unstable)\n    - [https://docs.kubestellar.io/latest](../../../https://docs.kubestellar.io/latest)\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#shortcut-urls","title":"Shortcut URLs","text":"<p>We have a few shortcut urls that come in handy when referring others to our project:</p> <p>note: You need to join our mailing list first to get access to some of the links that follow (https://docs.kubestellar.io/joinus)</p> <ul> <li>https://kubestellar.io/agenda - our community meeting agenda google doc</li> <li>https://kubestellar.io/blog - our medium reading list</li> <li>https://kubestellar.io/code - our current GitHub repo (wherever that is)</li> <li>https://kubestellar.io/community - our stable docs community page</li> <li>https://kubestellar.io/drive - our google drive</li> <li>https://kubestellar.io/joinus - our dev mailing list where you join and get our invites</li> <li>https://kubestellar.io/join_us - also, our dev mailing list</li> <li>https://kubestellar.io/linkedin - our linkedin filter (soon, our page)</li> <li>https://kubestellar.io/tv - our youtube channel</li> <li>https://kubestellar.io/youtube - also, our youtube channel</li> <li>https://kubestellar.io/infomercial - our infomercial that premieres on June 12th at 9am</li> </ul> <p>and.. the very important\u2026 - https://kubestellar.io/quickstart - our 'stable' quickstart</p>"},{"location":"Contribution%20guidelines/operations/document-management/#codeblocks","title":"Codeblocks","text":"<p>mkdocs has some very helpful ways to include blocks of code in a style that makes it clear to our readers that console interaction is necessary in the documentation.  There are options to include a plain codeblock (```), shell (shell), console (console - no used in our documentation), language or format-specific (yaml, etc.), and others.  For more detailed information, checkout the mkdocs information on codeblocks.</p> <p>Here are some examples of how we use codeblocks:</p> <ul> <li> <p>For a codeblock that can be 'tested' (and seen by the reader) as part of our CI, use the <code>shell</code> block: codeblock: <pre><code>```shell\nmkdocs serve\n```\n</code></pre> as seen by reader: <pre><code>mkdocs serve\n</code></pre> </p> </li> <li> <p>For a codeblock that should be 'tested', BUT not seen by the reader, use the <code>.bash</code> with the plain codeblock, and the '.hide-me' style (great for hiding a sleep command that user does not need to run, but CI does): codeblock: <pre><code>``` {.bash .hide-me}\nsleep 10\n```\n</code></pre> as seen by reader: <pre><code>\n</code></pre> </p> </li> <li> <p>For a codeblock that should not be 'tested' as part of our CI, use the <code>.bash</code> with the plain codeblock, and without the '.hide-me' style: codeblock: <pre><code>``` {.bash}\nmkdocs server\n```\n</code></pre> as seen by reader: <pre><code>mkdocs server\n</code></pre> <li> <p>For a codeblock that should not be 'tested', be seen by the reader, and not include a 'copy' icon (great for output-only instances), use the <code>.bash</code> codeblock without the '.no-copy' style: codeblock: <pre><code>``` {.bash .no-copy}\nI0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\n```\n</code></pre> as seen by reader: <pre><code>I0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\n</code></pre> <li> <p>For language-specific highlighting (yaml, etc.), use the yaml codeblock codeblock: <pre><code>```yaml\nnav:\n  - Home: index.md\n  - QuickStart: Getting-Started/quickstart.md\n  - Contributing: \n      - Guidelines: Contribution guidelines/CONTRIBUTING.md\n```\n</code></pre> as seen by reader: <pre><code>nav:\n- Home: index.md\n- QuickStart: Getting-Started/quickstart.md\n- Contributing: - Guidelines: Contribution guidelines/CONTRIBUTING.md\n</code></pre> </p> </li> <li> <p>For a codeblock that has a title, and will not be tested, use the 'title' parameter in conjunction with the plain codeblock (greater for showing or prescribing contents of files): codeblock: <pre><code>``` title=\"testing.sh\"\n#!/bin/sh\necho hello KubeStellar\n```\n</code></pre> as seen by reader: testing.sh<pre><code>#!/bin/sh\necho hello KubeStellar\n</code></pre> </p> </li> <p>(other variations are possible, PR an update to the kubestellar.css file and, once approved, use the style on the plain codeblock in your documentation.)</p>"},{"location":"Contribution%20guidelines/operations/document-management/#testingrunning-docs","title":"Testing/Running Docs","text":"<p>How do we ensure that our documented examples work?  Simple, we 'execute' our documentation in our CI.  We built automation called 'docs-ecutable' which can be invoked to test any markdown (.md) file in our repository. You could use it in your project as well - afterall it is opensource.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#the-way-it-works","title":"The way it works:","text":"<ul> <li>create your .md file as you normally would</li> <li>add codeblocks that can be tested, tested but hidden, or not tested at all:<ul> <li>use 'shell' to indicate code you want to be tested</li> <li>use '.bash' with the plain codeblock, and the '.hide-md' style for code you want to be tested, but hidden from the reader (some like this, but its not cool if you want others to run your instructions without hiccups)</li> <li>use plain codeblock (```) if you want to show sample output that is not to be tested</li> </ul> </li> <li>you can use 'include-markdown' blocks, and they will also be executed (or not), depending on the codeblock style you use in the included markdown files.</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#the-github-workflow","title":"The GitHub Workflow:","text":"<ul> <li>One example of the GitHub Workflow is located in our kcp-dev/edge-mc at https://github.com/kcp-dev/edge-mc/blob/main/.github/workflows/docs-ecutable-scheduler.yml</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#the-secret-sauce","title":"The secret sauce:","text":"<ul> <li>The code that makes all this possible is at https://github.com/kcp-dev/edge-mc/blob/main/docs/scripts/docs-ecutable.sh<ul> <li>This code parses the .md file you give it to pull out all the 'shell' and '.bash .hide-me' blocks</li> <li>The code is smart enough to traverse the include-markdown blocks and include the 'shell' and '.bash .hide-me' blocks in them</li> <li>It then creates a file called 'generate_script.sh' which is then run at the end of the docs-ecutable execution.</li> </ul> </li> </ul> <p>All of this is invoke in a target in our Makefile <pre><code>.PHONY: docs-ecutable\ndocs-ecutable: MANIFEST=$(MANIFEST) docs/scripts/docs-ecutable.sh\n</code></pre></p> <p>You give the path from that follows the 'https://github.com/kcp-dev/edge-mc/docs' path, and name of the .md file you want to 'execute'/'test' as the value for the MANIFEST variable:</p> How to 'make' our docs-ecutable target<pre><code>make MANIFEST=\"'docs/content/Getting-Started/quickstart.md'\" docs-ecutable\n</code></pre> <p>note: there are single and double-quotes used here to avoid issues with 'spaces' used in files names or directories.  Use the single and double-quotes as specified in the quickstart example here.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#important-files-in-our-gh-pages-branch","title":"Important files in our gh-pages branch","text":""},{"location":"Contribution%20guidelines/operations/document-management/#indexhtml-and-homehtml","title":"index.html and home.html","text":"<p>In the 'gh-pages' branch there are two(2) important files that redirect the github docs url to our KubeStellar doc site hosted with GoDaddy.com.</p> <p>https://github.com/kcp-dev/edge-mc/blob/gh-pages/home.html https://github.com/kcp-dev/edge-mc/blob/gh-pages/index.html</p> <p>both files have content similar to: index.html and home.html<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;KubeStellar&lt;/title&gt;\n&lt;meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" &gt;\n&lt;meta http-equiv=\"refresh\" content=\"0; URL=https://docs.kubestellar.io/stable\" /&gt;\n&lt;/head&gt;\n</code></pre></p> <p>Do not remove these files!</p>"},{"location":"Contribution%20guidelines/operations/document-management/#cname","title":"CNAME","text":"<p>The CNAME file has to be in the gh-pages root to allow github to recognize the url tls cert served by our hosting provider.  Do not remove this file!</p> <p>the CNAME file must have the following content in it: CNAME<pre><code>docs.kubestellar.io\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/document-management/#versionsjson","title":"versions.json","text":"<p>The versions.json file contains the version and alias information required by 'mike' to properly serve our doc site.  This file is maintained by the 'mike' environment and should not be edited by hand.</p> <pre><code>[{\"version\": \"release-0.2\", \"title\": \"release-0.2\", \"aliases\": [\"stable\"]}, {\"version\": \"main\", \"title\": \"main\", \"aliases\": [\"latest\", \"unstable\"]}]\n</code></pre>"},{"location":"Contribution%20guidelines/operations/document-management/#in-case-of-emergency","title":"In case of emergency","text":"<p>If you find yourself in a jam and the pages are not showing up at kubestellar.io or docs.kubestellar.io, check the following 1) Is the index.html, home.html, CNAME, and versions.json file in the gh-pages branch alongside the folders for the compiled documents?  If not, then recreate those files as indicated above (except for versions.json which is programmatically created by 'mike'). 2) Is GitHub settings for 'Pages' for the domain pointing at the https://docs.kubestellar.io url?  If not, paste it in and check off 'enforce https'.  This can happen if the CNAME file goes missing from the gh-pages branch.</p>"},{"location":"Contribution%20guidelines/operations/document-management/#how-to-recreate-the-gh-pages-branch","title":"How to recreate the gh-pages branch","text":"<p>To recreate the gh-pages branch, do the following: - checkout the gh-pages branch to your local system <pre><code>git clone -b gh-pages https://github.com/kcp-dev/edge-mc KubeStellar\ncd KubeStellar\n</code></pre> - delete all files in the branch and push it to GitHub <pre><code>rm -rf *\ngit add; git commit -m \"removing all gh-pages files\"; git push -u origin gh-pages\n</code></pre> -- switch to the 'main' branch <pre><code>git checkout main\ngit pull\n</code></pre> - switch to /docs and run 'mike deploy' for the main branch for alias 'unstable' and 'latest' <pre><code>cd docs\nmike deploy --push --rebase --update-aliases main unstable\nmike deploy --push --rebase --update-aliases main latest\n</code></pre> - switch to the 'release' branch and 'mike deploy' for the release branch for alias 'stable' (your release name will vary) <pre><code>git checkout release-0.2\ngit pull\nmike deploy --push --rebase --update-aliases release-0.2 stable\n</code></pre> - switch back to the gh-pages branch and recreate the home.html, index.html, and CNAME files as needed (make sure you back out of the docs path first before switching to gh-pages because that path does not exist in that branch) <pre><code>cd ..\ngit checkout gh-pages\ngit pull\nvi index.html\nvi home.html\nvi CNAME\n</code></pre> - push the new files into gh-pages <pre><code>git add .;git commit -m \"add index, home, and CNAME files\";git push -u origin gh-pages\n</code></pre> - go into the GitHub UI and go to the settings for the project and click on 'Pages' to add https://docs.kubestellar.io as the domain and check the box to enforce https.</p> <ul> <li>if the above did not work, then you might have an issue with the GoDaddy domain (expired, files missing, etc.)</li> </ul>"},{"location":"Contribution%20guidelines/operations/document-management/#publishing-workflow","title":"Publishing Workflow","text":"<p>All documentation building and publishing is done using GitHub Actions in docs-gen-and-push.yaml. The overall sequence is:</p>"},{"location":"Contribution%20guidelines/operations/release-management/","title":"Release Management","text":""},{"location":"Contribution%20guidelines/operations/release-management/#publishing-a-new-kubestellar-release","title":"Publishing a new KubeStellar release","text":""},{"location":"Contribution%20guidelines/operations/release-management/#prerequisite-make-sure-you-have-a-gpg-signing-key","title":"Prerequisite - make sure you have a GPG signing key","text":"<ol> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/generating-a-new-gpg-key</li> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/adding-a-gpg-key-to-your-github-account</li> <li>https://docs.github.com/en/authentication/managing-commit-signature-verification/telling-git-about-your-signing-key</li> </ol>"},{"location":"Contribution%20guidelines/operations/release-management/#create-the-tags","title":"Create the tags","text":""},{"location":"Contribution%20guidelines/operations/release-management/#note","title":"Note:","text":"<p>You currently need write access to the https://github.com/kcp-dev/edge-mc repository to perform these tasks.</p> <p>You also need an available team member with approval permissions from https://github.com/openshift/release/blob/master/ci-operator/config/kcp-dev/edge-mc/OWNERS.</p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-release-majorminor-branch","title":"Create a release-major.minor branch","text":"<p>To create a release branch, identify the current 'release' branches' name (e.g. release-0.2).  Increment the  or  segment as part of the 'release' branches' name.  For instance, the 'release' branch is 'release-0.2', you might name the new release branch 'release-0.3'. <pre><code>git clone git@github.com:kcp-dev/edge-mc.git\ncd {{ no such element: mkdocs.config.defaults.MkDocsConfig object['repo_default_path'] }}/docs\ngit checkout main\ngit checkout -b release-&lt;major&gt;.&lt;minor&gt; # replace &lt;major&gt;.&lt;minor&gt; with your incremented &lt;major&gt;.&lt;minor&gt; pair\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-mkdocsyml-file","title":"Update the mkdocs.yml file","text":"<p>The mkdocs.yml file points to the branch and tag associated with the branch you have checked out.  Update the ks_branch and ks_tag key/value pairs at the top of the file</p> <pre><code>vi docs/mkdocs.yml\n</code></pre> <p>before: mkdocs.yml<pre><code>...\nks_branch: 'release-0.2'\nks_tag: 'v0.2.2'\n...\n</code></pre></p> <p>after: mkdocs.yml<pre><code>...\nks_branch: 'release-0.2'\nks_tag: 'v0.3.0'\n...\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#remove-the-current-stable-alias-using-mike-danger","title":"Remove the current 'stable' alias using 'mike' (DANGER!)","text":"<p>Be careful, this will cause links to the 'stable' docs, which is the default for our community, to become unavailable.  For now, point 'stable' at 'main' <pre><code>mike delete stable # remove the 'stable' alias from the current 'release-&lt;major&gt;.&lt;minor&gt;' branches' doc set\nmike deploy --push --rebase --update-aliases main stable # this generates the 'main' branches' docs set and points 'stable' at it temporarily\n</code></pre></p>"},{"location":"Contribution%20guidelines/operations/release-management/#push-the-new-release-branch","title":"Push the new release branch","text":"<pre><code>git add .\ngit commit -m \"new release version &lt;major&gt;.&lt;minor&gt;\"\ngit push -u origin release-&lt;major&gt;.&lt;minor&gt; replace &lt;major&gt;.&lt;minor&gt; with your incremented &lt;major&gt;.&lt;minor&gt; pair\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#update-the-stable-alias-using-mike","title":"Update the 'stable' alias using 'mike'","text":"<pre><code>mike delete stable # remove the 'stable' alias from the 'main' branches' doc set\nmike deploy --push --rebase --update-aliases releae-0.3 stable  # this generates the new 'release-&lt;major&gt;.&lt;minor&gt;' branches' doc set and points 'stable' at it\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#test-your-doc-site","title":"Test your doc site","text":"<p>Open a Chrome Incognito browser to https://kubestellar.io and look for the version drop down to be updated to the new release you just pushed with 'git' and deployed with 'mike'</p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-build","title":"Create a build","text":"<pre><code>./hack/make-release-full.sh v0.3.0\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-tagged-release","title":"Create a tagged release","text":"<p>View the existing tags you have for the repo</p> <pre><code>git fetch --tags\ngit tag\n</code></pre> <p>create a tag that follows ...  For this example we will increment tag 'v0.2.2' to 'v0.3.0' <pre><code>TAG=v0.3.0\nREF=release-0.3\ngit tag --sign --message \"$TAG\" \"$TAG\" \"$REF\"\ngit push ??? \"$TAG\"  #TODO - not sure if this is right\n</code></pre>"},{"location":"Contribution%20guidelines/operations/release-management/#create-a-release-in-gh-ui","title":"Create a release in GH UI","text":"<ul> <li>Navigate to the KubeStellar GitHub Source Repository Releases section at https://github.com/kcp-dev/edge-mc/releases</li> <li> <p>Click 'Draft a new release' and create a new tag ('v0.3.0' in our example)</p> <ul> <li>Select the new release branch you just created (release-0.3)</li> <li>Add a release title (v.0.3.0)</li> <li>Add some release notes</li> <li>Attach the binaries that were created in the 'make build-all' process above<ul> <li>You add the KubeStellar-specific '*.tar.gz' and the 'checksum256.txt' files</li> <li>GitHub will automatically add the 'Source Code (zip)' and 'Source Code (tar.gz)'</li> </ul> </li> </ul> <p></p> </li> </ul>"},{"location":"Contribution%20guidelines/operations/release-management/#check-that-gh-workflows-for-docs-are-working","title":"Check that GH Workflows for docs are working","text":"<p>Check to make sure the GitHub workflows for doc generation, doc push, and broken links is working and passing https://github.com/kcp-dev/edge-mc/actions/workflows/docs-gen-and-push.yml https://github.com/kcp-dev/edge-mc/actions/workflows/broken-links-crawler.yml</p>"},{"location":"Contribution%20guidelines/operations/release-management/#create-an-email-addressed-to-kubestellar-devgooglegroupscom-and-kubestellar-usersgooglegroupscom","title":"Create an email addressed to kubestellar-dev@googlegroups.com and kubestellar-users@googlegroups.com","text":"<p><pre><code>Subject: [release] &lt;major&gt;&lt;minor&gt;\n</code></pre>     - In the body, include noteworthy changes     - Provide a link to the release in GitHub for the full release notes     - Post a message in the #kubestellar Slack channel</p>"},{"location":"Contribution%20guidelines/security/security/","title":"Policy","text":""},{"location":"Contribution%20guidelines/security/security/#security-announcements","title":"Security Announcements","text":"<p>Join the kubestellar-security-announce group for emails about security and major API announcements.</p>"},{"location":"Contribution%20guidelines/security/security/#report-a-vulnerability","title":"Report a Vulnerability","text":"<p>We're extremely grateful for security researchers and users that report vulnerabilities to the KubeStellar Open Source Community. All reports are thoroughly investigated by a set of community volunteers.</p> <p>You can also email the private kubestellar-security-announce@googlegroups.com list with the security details and the details expected for all KubeStellar bug reports.</p>"},{"location":"Contribution%20guidelines/security/security/#when-should-i-report-a-vulnerability","title":"When Should I Report a Vulnerability?","text":"<ul> <li>You think you discovered a potential security vulnerability in KubeStellar</li> <li>You are unsure how a vulnerability affects KubeStellar</li> <li>You think you discovered a vulnerability in another project that KubeStellar depends on</li> <li>For projects with their own vulnerability reporting and disclosure process, please report it directly there</li> </ul>"},{"location":"Contribution%20guidelines/security/security/#when-should-i-not-report-a-vulnerability","title":"When Should I NOT Report a Vulnerability?","text":"<ul> <li>You need help tuning KubeStellar components for security</li> <li>You need help applying security related updates</li> <li>Your issue is not security related</li> </ul>"},{"location":"Contribution%20guidelines/security/security/#security-vulnerability-response","title":"Security Vulnerability Response","text":"<p>Each report is acknowledged and analyzed by the maintainers of KubeStellar within 3 working days.</p> <p>Any vulnerability information shared with Security Response Committee stays within KubeStellar project and will not be disseminated to other projects unless it is necessary to get the issue fixed.</p> <p>As the security issue moves from triage, to identified fix, to release planning we will keep the reporter updated.</p>"},{"location":"Contribution%20guidelines/security/security/#public-disclosure-timing","title":"Public Disclosure Timing","text":"<p>A public disclosure date is negotiated by the KubeStellar Security Response Committee and the bug submitter. We prefer to fully disclose the bug as soon as possible once a user mitigation is available. It is reasonable to delay disclosure when the bug or the fix is not yet fully understood, the solution is not well-tested, or for vendor coordination. The timeframe for disclosure is from immediate (especially if it's already publicly known) to a few weeks. For a vulnerability with a straightforward mitigation, we expect report date to disclosure date to be on the order of 7 days. The KubeStellar maintainers hold the final say when setting a disclosure date.</p>"},{"location":"Contribution%20guidelines/security/security_contacts/","title":"Contacts","text":"<p>Defined below are the security contacts for this repo.</p> <p>They are the contact point for the Product Security Committee to reach out to for triaging and handling of incoming issues.</p> <p>The below names agree to address security concerns if and when they arise.</p> <p>DO NOT REPORT SECURITY VULNERABILITIES DIRECTLY TO THESE NAMES, SEND INFORMATION TO kubestellar-security-announce@googlegroups.com</p> <p>clubanderson MikeSpreitzer ezrasilvera pdettori</p>"},{"location":"Getting-Started/infomercial/","title":"KubeStellar - The Infomercial","text":""},{"location":"Getting-Started/quickstart/","title":"QuickStart","text":""},{"location":"Getting-Started/quickstart/#demo-video","title":"Demo Video","text":"<p>Watch this video to see a step-by-step demo of KubeStellar running and then follow the instructions below to get your own KubeStellar started quickly.</p> <p> </p> <p>Estimated time to complete this example:</p> <p>~4 minutes</p> <p>Required Packages:</p> MacUbuntuRHELWSL <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19<pre><code>curl -L \"https://go.dev/dl/go1.19.5.linux-$(dpkg --print-architecture).tar.gz\" -o go.tar.gz\ntar -C /usr/local -xzf go.tar.gz\nrm go.tar.gz\necho 'export PATH=$PATH:/usr/local/go/bin' &gt;&gt; /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64 &amp;&amp; chmod +x ./kind &amp;&amp; mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\nchoco install curl -y\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p>"},{"location":"Getting-Started/quickstart/#setup-instructions","title":"Setup Instructions","text":"<p>Table of contents:</p> <ol> <li>Install and run KubeStellar</li> <li>Example deployment of Apache HTTP Server workload into two local kind clusters<ol> <li>Stand up two kind clusters: florin and guilder</li> <li>Create a KubeStellar Inventory Management Workspace (IMW) and Workload Management Workspace (WMW)</li> <li>Onboarding the clusters</li> <li>Create and deploy the Apache Server workload into florin and guilder clusters</li> </ol> </li> <li>Teardown the environment</li> <li>Next Steps</li> </ol> <p>This guide is intended to show how to (1) quickly bring up a KubeStellar environment with its dependencies from a binary release and then (2) run through a simple example usage.</p>"},{"location":"Getting-Started/quickstart/#1-install-and-run-kubestellar","title":"1. Install and run KubeStellar","text":"<p>KubeStellar works in the context of kcp, so to use KubeStellar you also need kcp. Download the kcp and KubeStellar binaries and scripts into a <code>kubestellar</code> subfolder in your current working directory using the following command:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kcp-dev/edge-mc/main/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version latest\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\nexport KUBECONFIG=\"$(pwd)/.kcp/admin.kubeconfig\"\n</code></pre> <p>Check that <code>KubeStellar</code> is running:</p> <p>First, check that controllers are running with the following command:</p> <pre><code>ps aux | grep -e mailbox-controller -e placement-translator -e kubestellar-scheduler\n</code></pre> <p>which should yield something like:</p> <pre><code>user     1892  0.0  0.3 747644 29628 pts/1    Sl   10:51   0:00 mailbox-controller -v=2\nuser     1902  0.3  0.3 743652 27504 pts/1    Sl   10:51   0:02 scheduler -v 2 user     1912  0.3  0.5 760428 41660 pts/1    Sl   10:51   0:02 placement-translator -v=2\n</code></pre> <p>Second, check that the Edge Service Provider Workspace (<code>espw</code>) is created with the following command:</p> <pre><code>kubectl ws tree\n</code></pre> <p>which should yield:</p> <pre><code>kubectl ws tree\n.\n\u2514\u2500\u2500 root\n    \u251c\u2500\u2500 compute\n    \u2514\u2500\u2500 espw\n</code></pre>"},{"location":"Getting-Started/quickstart/#2-example-deployment-of-apache-http-server-workload-into-two-local-kind-clusters","title":"2. Example deployment of Apache HTTP Server workload into two local kind clusters","text":"<p>In this example you will create two edge clusters and define one workload that will be distributed from the center to those edge clusters.  This example is similar to the one described more expansively on the website, but with the some steps reorganized and combined and the special workload and summarization aspirations removed.</p>"},{"location":"Getting-Started/quickstart/#a-stand-up-two-kind-clusters-florin-and-guilder","title":"a. Stand up two kind clusters: florin and guilder","text":"<p>Create the first edge cluster:</p> <pre><code>kind create cluster --name florin --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>Note: if you already have a cluster named 'florin' from a previous exercise of KubeStellar, please delete the florin cluster ('kind delete cluster --name florin') and create it using the instruction above.</p> <p>Create the second edge cluster:</p> <pre><code>kind create cluster --name guilder --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Note: if you already have a cluster named 'guilder' from a previous exercise of KubeStellar, please delete the guilder cluster ('kind delete cluster --name guilder') and create it using the instruction above.</p>"},{"location":"Getting-Started/quickstart/#b-create-a-kubestellar-inventory-management-workspace-imw-and-workload-management-workspace-wmw","title":"b. Create a KubeStellar Inventory Management Workspace (IMW) and Workload Management Workspace (WMW)","text":"<p>IMWs are used by KubeStellar to store inventory objects (<code>SyncTargets</code> and <code>Locations</code>). Create an IMW named <code>example-imw</code> with the following command:</p> <pre><code>kubectl config use-context root\nkubectl ws root\nkubectl ws create example-imw\n</code></pre> <p>WMWs are used by KubeStellar to store workload descriptions and <code>EdgePlacement</code> objects. Create an WMW named <code>example-wmw</code> in a <code>my-org</code> workspace with the following commands:</p> <pre><code>kubectl ws root\nkubectl ws create my-org --enter\nkubectl kubestellar ensure wmw example-wmw\n</code></pre> <p>A WMW does not have to be created before the edge cluster is on-boarded; the WMW only needs to be created before content is put in it.</p>"},{"location":"Getting-Started/quickstart/#c-onboarding-the-clusters","title":"c. Onboarding the clusters","text":"<p>Let's begin by onboarding the <code>florin</code> cluster:</p> <pre><code>kubectl ws root\nkubectl kubestellar prep-for-cluster --imw root:example-imw florin env=prod\n</code></pre> <p>which should yield something like:</p> <pre><code>Current workspace is \"root:example-imw\".\nsynctarget.workload.kcp.io/florin created\nlocation.scheduling.kcp.io/florin created\nsynctarget.workload.kcp.io/florin labeled\nlocation.scheduling.kcp.io/florin labeled\nCurrent workspace is \"root:example-imw\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:9nemli4rpx83ahnz-mb-c44d04db-ae85-422c-9e12-c5e7865bf37a\" (type root:universal).\nCreating service account \"kubestellar-syncer-florin-1yi5q9c4\"\nCreating cluster role \"kubestellar-syncer-florin-1yi5q9c4\" to give service account \"kubestellar-syncer-florin-1yi5q9c4\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-1yi5q9c4\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-1yi5q9c4\" to bind service account \"kubestellar-syncer-florin-1yi5q9c4\" to cluster role \"kubestellar-syncer-florin-1yi5q9c4\".\n\nWrote physical cluster manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-1yi5q9c4\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-1yi5q9c4\" kubestellar-syncer-florin-1yi5q9c4\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:example-imw\".\nCurrent workspace is \"root\".\n</code></pre> <p>An edge syncer manifest yaml file was created in your current director: <code>florin-syncer.yaml</code>. The default for the output file is the name of the SyncTarget object with \u201c-syncer.yaml\u201d appended.</p> <p>Now let's deploy the edge syncer to the <code>florin</code> edge cluster:</p> <pre><code>kubectl --context kind-florin apply -f florin-syncer.yaml\n</code></pre> <p>which should yield something like:</p> <pre><code>namespace/kubestellar-syncer-florin-1yi5q9c4 created\nserviceaccount/kubestellar-syncer-florin-1yi5q9c4 created\nsecret/kubestellar-syncer-florin-1yi5q9c4-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-1yi5q9c4 created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-1yi5q9c4 created\nsecret/kubestellar-syncer-florin-1yi5q9c4 created\ndeployment.apps/kubestellar-syncer-florin-1yi5q9c4 created\n</code></pre> <p>Optionally, check that the edge syncer pod is running:</p> <pre><code>kubectl --context kind-florin get pods -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                            NAME                                                  READY   STATUS    RESTARTS   AGE\nkubestellar-syncer-florin-1yi5q9c4   kubestellar-syncer-florin-1yi5q9c4-77cb588c89-xc5qr   1/1     Running   0          12m\nkube-system                          coredns-565d847f94-92f4k                              1/1     Running   0          58m\nkube-system                          coredns-565d847f94-kgddm                              1/1     Running   0          58m\nkube-system                          etcd-florin-control-plane                             1/1     Running   0          58m\nkube-system                          kindnet-p8vkv                                         1/1     Running   0          58m\nkube-system                          kube-apiserver-florin-control-plane                   1/1     Running   0          58m\nkube-system                          kube-controller-manager-florin-control-plane          1/1     Running   0          58m\nkube-system                          kube-proxy-jmxsg                                      1/1     Running   0          58m\nkube-system                          kube-scheduler-florin-control-plane                   1/1     Running   0          58m\nlocal-path-storage                   local-path-provisioner-684f458cdd-kw2xz               1/1     Running   0          58m\n</code></pre> <p>Now, let's onboard the <code>guilder</code> cluster:</p> <pre><code>kubectl ws root\nkubectl kubestellar prep-for-cluster --imw root:example-imw guilder env=prod extended=si\n</code></pre> <p>Apply the created edge syncer manifest:</p> <pre><code>kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre>"},{"location":"Getting-Started/quickstart/#d-create-and-deploy-the-apache-server-workload-into-florin-and-guilder-clusters","title":"d. Create and deploy the Apache Server workload into florin and guilder clusters","text":"<p>Create the <code>EdgePlacement</code> object for your workload. Its \u201cwhere predicate\u201d (the locationSelectors array) has one label selector that matches the Location objects (<code>florin</code> and <code>guilder</code>) created earlier, thus directing the workload to both edge clusters.</p> <p>In the <code>example-wmw</code> workspace create the following <code>EdgePlacement</code> object: </p> <pre><code>kubectl ws root:my-org:example-wmw\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kcp.io/v1alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  namespaceSelector:\n    matchLabels: {\"common\":\"si\"}\n  nonNamespacedObjects:\n  - apiGroup: apis.kcp.io\n    resources: [ \"apibindings\" ]\n    resourceNames: [ \"bind-kube\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <p>Put the prescription of the HTTP server workload into the WMW. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>edge-placement-c</code>) object created above. </p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n  labels: {common: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: commonstuff\n  name: commond\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Now, let's check that the deployment was created in the <code>florin</code> edge cluster - it may take a few 10s of seconds to appear:</p> <pre><code>kubectl --context kind-florin get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                            NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                          commond                              1/1     1            1           6m48s\nkubestellar-syncer-florin-2upj1awn   kubestellar-syncer-florin-2upj1awn   1/1     1            1           16m\nkube-system                          coredns                              2/2     2            2           28m\nlocal-path-storage                   local-path-provisioner               1/1     1            1           28m\n</code></pre> <p>Also, let's check that the deployment was created in the <code>guilder</code> edge cluster:</p> <pre><code>kubectl --context kind-guilder get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                           commond                               1/1     1            1           7m54s\nkubestellar-syncer-guilder-6tuay5d6   kubestellar-syncer-guilder-6tuay5d6   1/1     1            1           12m\nkube-system                           coredns                               2/2     2            2           27m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           27m\n</code></pre> <p>Lastly, let's check that the workload is working in both clusters:</p> <p>For <code>florin</code>:</p> <pre><code>while [[ $(kubectl --context kind-florin get pod -l \"app=common\" -n commonstuff -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;curl http://localhost:8094\n</code></pre> <p>which should eventually yield:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>For <code>guilder</code>:</p> <p><pre><code>while [[ $(kubectl --context kind-guilder get pod -l \"app=common\" -n commonstuff -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;curl http://localhost:8096\n</code></pre> which should eventually yield:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Congratulations, you\u2019ve just deployed a workload to two edge clusters using kubestellar! To learn more about kubestellar please visit our User Guide</p>"},{"location":"Getting-Started/quickstart/#3-teardown-the-environment","title":"3. Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl ws root:my-org\nkubectl kubestellar remove wmw example-wmw\nkubectl ws root\nkubectl delete workspace my-org\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Stop and uninstall KubeStellar use the following command:</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"},{"location":"Getting-Started/quickstart/#4-next-steps","title":"4. Next Steps","text":"<p>What you just did is a shortened version of the  more detailed example on the website, but with the some steps reorganized and combined and the special workload and summarization aspiration removed.  You can continue from here, learning more details about what you did in the QuickStart, and adding on some more steps for the special workload.</p>"},{"location":"Getting-Started/user-guide/","title":"User Guide","text":""},{"location":"Getting-Started/quickstart-subs/quickstart-0-demo/","title":"Quickstart 0 demo","text":""},{"location":"Getting-Started/quickstart-subs/quickstart-0-demo/#demo-video","title":"Demo Video","text":"<p>Watch this video to see a step-by-step demo of KubeStellar running and then follow the instructions below to get your own KubeStellar started quickly.</p> <p> </p>"},{"location":"Getting-Started/quickstart-subs/quickstart-1-install-and-run-kubestellar/","title":"Quickstart 1 install and run kubestellar","text":""},{"location":"Getting-Started/quickstart-subs/quickstart-1-install-and-run-kubestellar/#1-install-and-run-kubestellar","title":"1. Install and run KubeStellar","text":"<p>KubeStellar works in the context of kcp, so to use KubeStellar you also need kcp. Download the kcp and KubeStellar binaries and scripts into a <code>kubestellar</code> subfolder in your current working directory using the following command:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kcp-dev/edge-mc/main/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version latest\nexport PATH=\"$PATH:$(pwd)/kcp/bin:$(pwd)/kubestellar/bin\"\nexport KUBECONFIG=\"$(pwd)/.kcp/admin.kubeconfig\"\n</code></pre> <p>Check that <code>KubeStellar</code> is running:</p> <p>First, check that controllers are running with the following command:</p> <pre><code>ps aux | grep -e mailbox-controller -e placement-translator -e kubestellar-scheduler\n</code></pre> <p>which should yield something like:</p> <pre><code>user     1892  0.0  0.3 747644 29628 pts/1    Sl   10:51   0:00 mailbox-controller -v=2\nuser     1902  0.3  0.3 743652 27504 pts/1    Sl   10:51   0:02 scheduler -v 2 user     1912  0.3  0.5 760428 41660 pts/1    Sl   10:51   0:02 placement-translator -v=2\n</code></pre> <p>Second, check that the Edge Service Provider Workspace (<code>espw</code>) is created with the following command:</p> <pre><code>kubectl ws tree\n</code></pre> <p>which should yield:</p> <pre><code>kubectl ws tree\n.\n\u2514\u2500\u2500 root\n    \u251c\u2500\u2500 compute\n    \u2514\u2500\u2500 espw\n</code></pre>"},{"location":"Getting-Started/quickstart-subs/quickstart-2-apache-example-deployment-a-kind-clusters-florin/","title":"Quickstart 2 apache example deployment a kind clusters florin","text":"<p>Create the first edge cluster:</p> <pre><code>kind create cluster --name florin --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8094\nEOF\n</code></pre> <p>Note: if you already have a cluster named 'florin' from a previous exercise of KubeStellar, please delete the florin cluster ('kind delete cluster --name florin') and create it using the instruction above.</p>"},{"location":"Getting-Started/quickstart-subs/quickstart-2-apache-example-deployment-a-kind-clusters-guilder/","title":"Quickstart 2 apache example deployment a kind clusters guilder","text":"<p>Create the second edge cluster:</p> <pre><code>kind create cluster --name guilder --config - &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 8081\n    hostPort: 8096\n  - containerPort: 8082\n    hostPort: 8097\nEOF\n</code></pre> <p>Note: if you already have a cluster named 'guilder' from a previous exercise of KubeStellar, please delete the guilder cluster ('kind delete cluster --name guilder') and create it using the instruction above.</p>"},{"location":"Getting-Started/quickstart-subs/quickstart-2-apache-example-deployment-b-create-imw-and-wmw/","title":"Quickstart 2 apache example deployment b create imw and wmw","text":"<p>IMWs are used by KubeStellar to store inventory objects (<code>SyncTargets</code> and <code>Locations</code>). Create an IMW named <code>example-imw</code> with the following command:</p> <pre><code>kubectl config use-context root\nkubectl ws root\nkubectl ws create example-imw\n</code></pre> <p>WMWs are used by KubeStellar to store workload descriptions and <code>EdgePlacement</code> objects. Create an WMW named <code>example-wmw</code> in a <code>my-org</code> workspace with the following commands:</p> <pre><code>kubectl ws root\nkubectl ws create my-org --enter\nkubectl kubestellar ensure wmw example-wmw\n</code></pre> <p>A WMW does not have to be created before the edge cluster is on-boarded; the WMW only needs to be created before content is put in it.</p>"},{"location":"Getting-Started/quickstart-subs/quickstart-2-apache-example-deployment-c-onboarding-clusters/","title":"Quickstart 2 apache example deployment c onboarding clusters","text":"<p>Let's begin by onboarding the <code>florin</code> cluster:</p> <pre><code>kubectl ws root\nkubectl kubestellar prep-for-cluster --imw root:example-imw florin env=prod\n</code></pre> <p>which should yield something like:</p> <pre><code>Current workspace is \"root:example-imw\".\nsynctarget.workload.kcp.io/florin created\nlocation.scheduling.kcp.io/florin created\nsynctarget.workload.kcp.io/florin labeled\nlocation.scheduling.kcp.io/florin labeled\nCurrent workspace is \"root:example-imw\".\nCurrent workspace is \"root:espw\".\nCurrent workspace is \"root:espw:9nemli4rpx83ahnz-mb-c44d04db-ae85-422c-9e12-c5e7865bf37a\" (type root:universal).\nCreating service account \"kubestellar-syncer-florin-1yi5q9c4\"\nCreating cluster role \"kubestellar-syncer-florin-1yi5q9c4\" to give service account \"kubestellar-syncer-florin-1yi5q9c4\"\n1. write and sync access to the synctarget \"kubestellar-syncer-florin-1yi5q9c4\"\n2. write access to apiresourceimports.\n\nCreating or updating cluster role binding \"kubestellar-syncer-florin-1yi5q9c4\" to bind service account \"kubestellar-syncer-florin-1yi5q9c4\" to cluster role \"kubestellar-syncer-florin-1yi5q9c4\".\n\nWrote physical cluster manifest to florin-syncer.yaml for namespace \"kubestellar-syncer-florin-1yi5q9c4\". Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl apply -f \"florin-syncer.yaml\"\nto apply it. Use\n\nKUBECONFIG=&lt;pcluster-config&gt; kubectl get deployment -n \"kubestellar-syncer-florin-1yi5q9c4\" kubestellar-syncer-florin-1yi5q9c4\n\nto verify the syncer pod is running.\nCurrent workspace is \"root:example-imw\".\nCurrent workspace is \"root\".\n</code></pre> <p>An edge syncer manifest yaml file was created in your current director: <code>florin-syncer.yaml</code>. The default for the output file is the name of the SyncTarget object with \u201c-syncer.yaml\u201d appended.</p> <p>Now let's deploy the edge syncer to the <code>florin</code> edge cluster:</p> <pre><code>kubectl --context kind-florin apply -f florin-syncer.yaml\n</code></pre> <p>which should yield something like:</p> <pre><code>namespace/kubestellar-syncer-florin-1yi5q9c4 created\nserviceaccount/kubestellar-syncer-florin-1yi5q9c4 created\nsecret/kubestellar-syncer-florin-1yi5q9c4-token created\nclusterrole.rbac.authorization.k8s.io/kubestellar-syncer-florin-1yi5q9c4 created\nclusterrolebinding.rbac.authorization.k8s.io/kubestellar-syncer-florin-1yi5q9c4 created\nsecret/kubestellar-syncer-florin-1yi5q9c4 created\ndeployment.apps/kubestellar-syncer-florin-1yi5q9c4 created\n</code></pre> <p>Optionally, check that the edge syncer pod is running:</p> <pre><code>kubectl --context kind-florin get pods -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                            NAME                                                  READY   STATUS    RESTARTS   AGE\nkubestellar-syncer-florin-1yi5q9c4   kubestellar-syncer-florin-1yi5q9c4-77cb588c89-xc5qr   1/1     Running   0          12m\nkube-system                          coredns-565d847f94-92f4k                              1/1     Running   0          58m\nkube-system                          coredns-565d847f94-kgddm                              1/1     Running   0          58m\nkube-system                          etcd-florin-control-plane                             1/1     Running   0          58m\nkube-system                          kindnet-p8vkv                                         1/1     Running   0          58m\nkube-system                          kube-apiserver-florin-control-plane                   1/1     Running   0          58m\nkube-system                          kube-controller-manager-florin-control-plane          1/1     Running   0          58m\nkube-system                          kube-proxy-jmxsg                                      1/1     Running   0          58m\nkube-system                          kube-scheduler-florin-control-plane                   1/1     Running   0          58m\nlocal-path-storage                   local-path-provisioner-684f458cdd-kw2xz               1/1     Running   0          58m\n</code></pre> <p>Now, let's onboard the <code>guilder</code> cluster:</p> <pre><code>kubectl ws root\nkubectl kubestellar prep-for-cluster --imw root:example-imw guilder env=prod extended=si\n</code></pre> <p>Apply the created edge syncer manifest:</p> <pre><code>kubectl --context kind-guilder apply -f guilder-syncer.yaml\n</code></pre>"},{"location":"Getting-Started/quickstart-subs/quickstart-2-apache-example-deployment-d-create-and-deploy-apache-into-clusters/","title":"Quickstart 2 apache example deployment d create and deploy apache into clusters","text":"<p>Create the <code>EdgePlacement</code> object for your workload. Its \u201cwhere predicate\u201d (the locationSelectors array) has one label selector that matches the Location objects (<code>florin</code> and <code>guilder</code>) created earlier, thus directing the workload to both edge clusters.</p> <p>In the <code>example-wmw</code> workspace create the following <code>EdgePlacement</code> object: </p> <pre><code>kubectl ws root:my-org:example-wmw\n\nkubectl apply -f - &lt;&lt;EOF\napiVersion: edge.kcp.io/v1alpha1\nkind: EdgePlacement\nmetadata:\n  name: edge-placement-c\nspec:\n  locationSelectors:\n  - matchLabels: {\"env\":\"prod\"}\n  namespaceSelector:\n    matchLabels: {\"common\":\"si\"}\n  nonNamespacedObjects:\n  - apiGroup: apis.kcp.io\n    resources: [ \"apibindings\" ]\n    resourceNames: [ \"bind-kube\" ]\n  upsync:\n  - apiGroup: \"group1.test\"\n    resources: [\"sprockets\", \"flanges\"]\n    namespaces: [\"orbital\"]\n    names: [\"george\", \"cosmo\"]\n  - apiGroup: \"group2.test\"\n    resources: [\"cogs\"]\n    names: [\"william\"]\nEOF\n</code></pre> <p>Put the prescription of the HTTP server workload into the WMW. Note the namespace label matches the label in the namespaceSelector for the EdgePlacement (<code>edge-placement-c</code>) object created above. </p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: commonstuff\n  labels: {common: \"si\"}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: commonstuff\n  name: httpd-htdocs\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n      &lt;body&gt;\n        This is a common web site.\n      &lt;/body&gt;\n    &lt;/html&gt;\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: commonstuff\n  name: commond\nspec:\n  selector: {matchLabels: {app: common} }\n  template:\n    metadata:\n      labels: {app: common}\n    spec:\n      containers:\n      - name: httpd\n        image: library/httpd:2.4\n        ports:\n        - name: http\n          containerPort: 80\n          hostPort: 8081\n          protocol: TCP\n        volumeMounts:\n        - name: htdocs\n          readOnly: true\n          mountPath: /usr/local/apache2/htdocs\n      volumes:\n      - name: htdocs\n        configMap:\n          name: httpd-htdocs\n          optional: false\nEOF\n</code></pre> <p>Now, let's check that the deployment was created in the <code>florin</code> edge cluster - it may take a few 10s of seconds to appear:</p> <pre><code>kubectl --context kind-florin get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                            NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                          commond                              1/1     1            1           6m48s\nkubestellar-syncer-florin-2upj1awn   kubestellar-syncer-florin-2upj1awn   1/1     1            1           16m\nkube-system                          coredns                              2/2     2            2           28m\nlocal-path-storage                   local-path-provisioner               1/1     1            1           28m\n</code></pre> <p>Also, let's check that the deployment was created in the <code>guilder</code> edge cluster:</p> <pre><code>kubectl --context kind-guilder get deployments -A\n</code></pre> <p>which should yield something like:</p> <pre><code>NAMESPACE                             NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE\ncommonstuff                           commond                               1/1     1            1           7m54s\nkubestellar-syncer-guilder-6tuay5d6   kubestellar-syncer-guilder-6tuay5d6   1/1     1            1           12m\nkube-system                           coredns                               2/2     2            2           27m\nlocal-path-storage                    local-path-provisioner                1/1     1            1           27m\n</code></pre> <p>Lastly, let's check that the workload is working in both clusters:</p> <p>For <code>florin</code>:</p> <pre><code>while [[ $(kubectl --context kind-florin get pod -l \"app=common\" -n commonstuff -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;curl http://localhost:8094\n</code></pre> <p>which should eventually yield:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>For <code>guilder</code>:</p> <p><pre><code>while [[ $(kubectl --context kind-guilder get pod -l \"app=common\" -n commonstuff -o jsonpath='{.items[0].status.phase}') != \"Running\" ]]; do sleep 5; done;curl http://localhost:8096\n</code></pre> which should eventually yield:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n    This is a common web site.\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Congratulations, you\u2019ve just deployed a workload to two edge clusters using kubestellar! To learn more about kubestellar please visit our User Guide</p>"},{"location":"common-subs/check-command-syncerconfig-the-one/","title":"Check command syncerconfig the one","text":"<pre><code>let increment=10\nlet slept=1\nwhile ! kubectl get SyncerConfig the-one -o yaml; do\nif (( slept &gt;= 300 )); then\necho \"FAILURE to run command 'kubectl get SyncerConfig the-one -o yaml' (slept $slept)\" &gt;&amp;2\nexit 86\nfi\nsleep $increment\nlet slept=slept+increment\n    echo slept\ndone\n</code></pre>"},{"location":"common-subs/coming-soon/","title":"Coming soon","text":""},{"location":"common-subs/pre-req/","title":"Pre req","text":"<pre><code>os_type=\"\"\narch_type=\"\"\nfolder=\"\"\nget_os_type() {\ncase \"$OSTYPE\" in\nlinux*)   echo \"linux\" ;;\ndarwin*)  echo \"darwin\" ;;\n*)        echo \"Unsupported operating system type: $OSTYPE\" &gt;&amp;2 ; exit 1 ;;\nesac\n}\nget_arch_type() {\ncase \"$HOSTTYPE\" in\nx86_64*)  echo \"amd64\" ;;\naarch64*) echo \"arm64\" ;;\narm64*)   echo \"arm64\" ;;\n*)        echo \"Unsupported architecture type: $HOSTTYPE\" &gt;&amp;2 ; exit 1 ;;\nesac\n}\nget_os_type() {\ncase \"$OSTYPE\" in\nlinux*)   echo \"linux\" ;;\ndarwin*)  echo \"darwin\" ;;\n*)        echo \"Unsupported operating system type: $OSTYPE\" &gt;&amp;2 ; exit 1 ;;\nesac\n}\nget_arch_type() {\ncase \"$HOSTTYPE\" in\nx86_64*)  echo \"amd64\" ;;\naarch64*) echo \"arm64\" ;;\narm64*)   echo \"arm64\" ;;\n*)        echo \"Unsupported architecture type: $HOSTTYPE\" &gt;&amp;2 ; exit 1 ;;\nesac\n}\nif [ \"$os_type\" == \"\" ]; then\nos_type=$(get_os_type)\nfi\nif [ \"$arch_type\" == \"\" ]; then\narch_type=$(get_arch_type)\nfi\nif [ \"$folder\" == \"\" ]; then\nfolder=\"$PWD\"\nfi\necho $os_type\necho $arch_type\necho $folder\nif command -v docker &gt;/dev/null 2&gt;&amp;1; then\necho \"Docker is installed\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install docker\n    fi\nfi\nif docker info &gt;/dev/null 2&gt;&amp;1; then\necho \"Docker is started\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nopen --background -a Docker\n      sleep 30\nfi\nfi\nif command -v go &gt;/dev/null 2&gt;&amp;1; then\necho \"GO is installed\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install go@1.19\n    fi\nfi\nif command -v kubectl &gt;/dev/null 2&gt;&amp;1; then\necho \"kubectl is installed\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install kubectl\n    fi\nfi\nif command -v jq &gt;/dev/null 2&gt;&amp;1; then\necho \"jq is installed\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install jq\n    fi\nfi\nif command -v kind &gt;/dev/null 2&gt;&amp;1; then\necho \"kind is installed\"\nelse\nif [ \"$os_type\" == \"darwin\" ]; then\nbrew install kind\n    fi\nfi\nps -ef | grep mailbox-controller | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep kubestellar-scheduler | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep placement-translator | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep kcp | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep 'exe/main -v 2' | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nkind delete cluster --name florin 2&gt;&amp;1\nkind delete cluster --name guilder 2&gt;&amp;1\n</code></pre>"},{"location":"common-subs/remove-all/","title":"Remove all","text":"<pre><code>ps -ef | grep mailbox-controller | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep kubestellar-scheduler | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep placement-translator | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep kcp | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nps -ef | grep 'exe/main -v 2' | grep -v grep | awk '{print $2}' | xargs kill &gt;/dev/null 2&gt;&amp;1 || true\nkind delete cluster --name florin 2&gt;&amp;1\nkind delete cluster --name guilder 2&gt;&amp;1\n</code></pre>"},{"location":"common-subs/required-packages/","title":"Required packages","text":"<p>Required Packages:</p> MacUbuntuRHELWSL <p>jq - https://stedolan.github.io/jq/download/<pre><code>brew install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>brew install docker\nopen -a Docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>brew install kind\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>brew install kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>sudo apt-get install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>sudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-$(dpkg --print-architecture) &amp;&amp; chmod +x ./kind &amp;&amp; sudo mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/$(dpkg --print-architecture)/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19<pre><code>curl -L \"https://go.dev/dl/go1.19.5.linux-$(dpkg --print-architecture).tar.gz\" -o go.tar.gz\ntar -C /usr/local -xzf go.tar.gz\nrm go.tar.gz\necho 'export PATH=$PATH:/usr/local/go/bin' &gt;&gt; /etc/profile\nsource /etc/profile\ngo version\n</code></pre></p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>yum -y install jq\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>yum -y install epel-release &amp;&amp; yum -y install docker &amp;&amp; systemctl enable --now docker &amp;&amp; systemctl status docker\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64 &amp;&amp; chmod +x ./kind &amp;&amp; mv ./kind /usr/local/bin\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/ (version range expected: 1.23-1.25)<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; mv ./kubectl /usr/local/bin/kubectl\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p> <p>jq - https://stedolan.github.io/jq/download/<pre><code>choco install jq -y\nchoco install curl -y\n</code></pre> docker - https://docs.docker.com/engine/install/<pre><code>choco install docker -y\n</code></pre> kind - https://kind.sigs.k8s.io/docs/user/quick-start/<pre><code>curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64\n</code></pre> kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ (version range expected: 1.23-1.25)<pre><code>curl.exe -LO \"https://dl.k8s.io/release/v1.27.2/bin/windows/amd64/kubectl.exe\"\n</code></pre> GO v1.19 - You will need GO to compile and run kcp and the KubeStellar scheduler.  Currently kcp requires go version 1.19.</p>"},{"location":"common-subs/save-some-time/","title":"Save some time","text":"<p>This document is 'docs-ecutable' - you can 'run' this document, just like we do in our testing, on your local environment</p> <pre><code>git clone -n -b main https://github.com/kcp-dev/edge-mc --depth 1 KubeStellar-{{ no such element: dict object['short_name'] }}\ncd KubeStellar-{{ no such element: dict object['short_name'] }}\ngit restore --staged Makefile Makefile.venv go.mod docs/mkdocs.yml docs/content docs/scripts/docs-ecutable.sh\ngit checkout Makefile Makefile.venv go.mod docs/mkdocs.yml docs/content docs/scripts/docs-ecutable.sh\nmake MANIFEST=\"'{{ no such element: dict object['pre_req_name'] }}','{{ no such element: dict object['manifest_name'] }}'\" docs-ecutable\n</code></pre> <pre><code># done? remove everything\nmake MANIFEST=\"docs/content/common-subs/remove-all.md\" docs-ecutable\ncd ../\nrm -rf KubeStellar-{{ no such element: dict object['short_name'] }}\n</code></pre>"},{"location":"common-subs/teardown-the-environment/","title":"Teardown the environment","text":"<p>To remove the example usage, delete the IMW and WMW and kind clusters run the following commands:</p> <pre><code>rm florin-syncer.yaml guilder-syncer.yaml || true\nkubectl ws root\nkubectl delete workspace example-imw\nkubectl ws root:my-org\nkubectl kubestellar remove wmw example-wmw\nkubectl ws root\nkubectl delete workspace my-org\nkind delete cluster --name florin\nkind delete cluster --name guilder\n</code></pre> <p>Stop and uninstall KubeStellar use the following command:</p> <pre><code>kubestellar stop\n</code></pre> <p>Stop and uninstall KubeStellar and kcp with the following command:</p> <pre><code>remove-kubestellar\n</code></pre>"}]}