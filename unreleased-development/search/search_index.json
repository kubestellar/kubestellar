{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"readme/","title":"Overview","text":""},{"location":"readme/#multi-cluster-configuration-management-for-edge-multi-cloud-and-hybrid-cloud","title":"Multi-cluster Configuration Management for Edge, Multi-Cloud, and Hybrid Cloud","text":"<p>KubeStellar is a Cloud Native Computing Foundation (CNCF) Sandbox project that simplifies the deployment and configuration of applications across multiple Kubernetes clusters. It provides a seamless experience akin to using a single cluster, and it integrates with the tools you're already familiar with, eliminating the need to modify existing resources.</p> <p>KubeStellar is particularly beneficial if you're currently deploying in a single cluster and are looking to expand to multiple clusters, or if you're already using multiple clusters and are seeking a more streamlined developer experience.</p> <p> KubeStellar High Level View </p> <p>The use of multiple clusters offers several advantages, including:</p> <ul> <li>Separation of environments (e.g., development, testing, staging)</li> <li>Isolation of groups, teams, or departments</li> <li>Compliance with enterprise security or data governance requirements</li> <li>Enhanced resiliency, including across different clouds</li> <li>Improved resource availability</li> <li>Access to heterogeneous resources</li> <li>Capability to run applications on the edge, including in disconnected environments</li> </ul> <p>In a single-cluster setup, developers typically access the cluster and deploy Kubernetes objects directly. Without KubeStellar, multiple clusters are usually deployed and configured individually, which can be time-consuming and complex.</p> <p>KubeStellar simplifies this process by allowing developers to define a binding policy between clusters and Kubernetes objects. It then uses your regular single-cluster tooling to deploy and configure each cluster based on these binding policies, making multi-cluster operations as straightforward as managing a single cluster. This approach enhances productivity and efficiency, making KubeStellar a valuable tool in a multi-cluster Kubernetes environment.</p>"},{"location":"readme/#getting-started","title":"Getting Started","text":"<p>See the Getting Started setup guide for getting started with kicking the tires.</p>"},{"location":"readme/#contributing","title":"Contributing","text":"<p>We \u2764\ufe0f our contributors! If you're interested in helping us out, please head over to our Contributing guide.</p>"},{"location":"readme/#getting-in-touch","title":"Getting in touch","text":"<p>There are several ways to communicate with us:</p> <p>Instantly get access to our documents and meeting invites http://kubestellar.io/joinus</p> <ul> <li>The <code>#kubestellar-dev</code> channel in the Kubernetes Slack workspace</li> <li>Our mailing lists:<ul> <li>kubestellar-dev for development discussions</li> <li>kubestellar-users for discussions among users and potential users</li> </ul> </li> <li>Subscribe to the community calendar for community meetings and events<ul> <li>The kubestellar-dev mailing list is subscribed to this calendar</li> </ul> </li> <li>See recordings of past KubeStellar community meetings on YouTube</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Browse the shared Google Drive to share design docs, notes, etc.<ul> <li>Members of the kubestellar-dev mailing list can view this drive</li> </ul> </li> <li>Read our documentation</li> <li>Follow us on:</li> <li>LinkedIn - #kubestellar</li> <li>Medium - kubestellar.medium.com</li> </ul>"},{"location":"readme/#contributors","title":"\u2764\ufe0f Contributors","text":"<p>Thanks go to these wonderful people:</p> <sub>Jun Duan</sub>\ud83d\udc40 <sub>Braulio Dumba</sub>\ud83d\udc40 <sub>Mike Spreitzer</sub>\ud83d\udc40 <sub>Paolo Dettori</sub>\ud83d\udc40 <sub>Andy Anderson</sub>\ud83d\udc40 <sub>Franco Stellari</sub>\ud83d\udc40 <sub>Ezra Silvera</sub>\ud83d\udc40 <sub>Bob Filepp</sub>\ud83d\udc40 <sub>Effi Ofer</sub>\ud83d\udc40 <sub>Maria Camila Ruiz Cardenas</sub>\ud83d\udc40 <sub>Andrey Odarenko</sub>\ud83d\udc40 <sub>Aashni Manroa</sub>\ud83d\udc40 <sub>Kevin Roche</sub>\ud83d\udc40 <sub>Nick Masluk</sub>\ud83d\udc40 <sub>Francois Abel</sub>\ud83d\udc40 <sub>Nir Rozenbaum</sub>\ud83d\udc40 <sub>Maroon Ayoub</sub>\ud83d\udc40 <sub>Graham White</sub>\ud83d\udc40"},{"location":"Community/_index/","title":"Join the KubeStellar community","text":""},{"location":"Community/_index/#kubestellar-is-an-open-source-project-that-anyone-in-the-community-can-use-improve-and-enjoy-join-us-heres-a-few-ways-to-find-out-whats-happening-and-get-involved","title":"KubeStellar is an open source project that anyone in the community can use, improve, and enjoy. Join us! Here's a few ways to find out what's happening and get involved","text":""},{"location":"Community/_index/#learn-and-connect","title":"Learn and Connect","text":""},{"location":"Community/_index/#using-or-want-to-use-kubestellar-find-out-more-here","title":"Using or want to use KubeStellar? Find out more here:","text":"<ul> <li>User mailing list: Discussion and help from your fellow users</li> <li>YouTube Channel: Follow us on YouTube to view recordings of past KubeStellar community meetings and demo days</li> <li>LinkedIn: See what others are saying about the community</li> <li>Medium Blog Series: Follow us on Medium to read about community developments</li> </ul>"},{"location":"Community/_index/#develop-and-contribute","title":"Develop and Contribute","text":""},{"location":"Community/_index/#if-you-want-to-get-more-involved-by-contributing-to-kubestellar-join-us-here","title":"If you want to get more involved by contributing to KubeStellar, join us here:","text":"<ul> <li>GitHub: Development takes place here!</li> <li>#kubestellar-dev Slack channel in the Kubernetes slack workspace: Chat with other project developers</li> <li>Developer mailing list: Discuss development issues around the project</li> <li>You can find out how to contribute to KubeStellar in our Contribution Guidelines</li> </ul>"},{"location":"Community/_index/#community-meetings","title":"Community Meetings","text":"<ol> <li>Join our Developer mailing list to get your community meeting invitation.</li> <li>You can also directly subscribe to the community calendar, or view our calendar</li> <li>See upcoming and past community meeting agendas and notes</li> <li>Sign up to discuss a topic in the KubeStellar Community Meeting Agenda</li> </ol>"},{"location":"Community/_index/#other-resources","title":"Other Resources","text":"<ul> <li>Google Drive</li> </ul>"},{"location":"Community/gsoc/2025/ideas/","title":"Potential GSoC Project Ideas for AI/ML in Disconnected Environments with KubeStellar","text":""},{"location":"Community/gsoc/2025/ideas/#1-intelligent-model-deployment-synchronization-in-air-gapped-clusters","title":"1. Intelligent Model Deployment &amp; Synchronization in Air-Gapped Clusters","text":""},{"location":"Community/gsoc/2025/ideas/#goal-enable-efficient-deployment-and-synchronization-of-aiml-models-across-disconnected-or-air-gapped-kubernetes-clusters-using-kubestellar","title":"Goal: Enable efficient deployment and synchronization of AI/ML models across disconnected or air-gapped Kubernetes clusters using KubeStellar.","text":""},{"location":"Community/gsoc/2025/ideas/#features","title":"Features:","text":"<ul> <li>Automate model distribution from a central hub to edge clusters when connectivity is available.</li> <li>Implement version control for models, ensuring outdated versions do not overwrite newer ones.</li> <li>Introduce a caching mechanism for model artifacts and inference pipelines for offline operation.</li> </ul>"},{"location":"Community/gsoc/2025/ideas/#challenges-handling-large-model-sizes-ensuring-integrity-and-consistency-across-clusters","title":"Challenges: Handling large model sizes, ensuring integrity and consistency across clusters.","text":""},{"location":"Community/gsoc/2025/ideas/#2-aiml-pipeline-orchestration-for-edge-devices","title":"2. AI/ML Pipeline Orchestration for Edge Devices","text":""},{"location":"Community/gsoc/2025/ideas/#goal-build-a-lightweight-aiml-pipeline-manager-for-disconnected-environments-using-kubestellars-workload-synchronization-capabilities","title":"Goal: Build a lightweight AI/ML pipeline manager for disconnected environments using KubeStellar\u2019s workload synchronization capabilities.","text":""},{"location":"Community/gsoc/2025/ideas/#features_1","title":"Features:","text":"<ul> <li>Define, deploy, and update ML pipelines using a declarative approach.</li> <li>Implement scheduling policies for AI jobs that adjust based on resource constraints (e.g., CPU, memory).</li> <li>Introduce offline-first strategies where models can be trained or fine-tuned locally and synchronized when reconnected.</li> </ul>"},{"location":"Community/gsoc/2025/ideas/#challenges-efficient-resource-allocation-on-constrained-devices-handling-intermittent-connectivity","title":"Challenges: Efficient resource allocation on constrained devices, handling intermittent connectivity.","text":""},{"location":"Community/gsoc/2025/ideas/#3-federated-learning-support-with-kubestellar","title":"3. Federated Learning Support with KubeStellar","text":""},{"location":"Community/gsoc/2025/ideas/#goal-enable-federated-learning-training-ml-models-across-multiple-disconnected-clusters-without-sharing-raw-data-using-kubestellars-workload-propagation","title":"Goal: Enable federated learning (training ML models across multiple disconnected clusters without sharing raw data) using KubeStellar\u2019s workload propagation.","text":""},{"location":"Community/gsoc/2025/ideas/#features_2","title":"Features:","text":"<ul> <li>Implement mechanisms for sharing only model updates (gradients, weights) between clusters.</li> <li>Ensure secure aggregation of models when connectivity is restored.</li> <li>Optimize update frequency based on available bandwidth and compute power.</li> </ul>"},{"location":"Community/gsoc/2025/ideas/#challenges-privacy-and-security-of-model-updates-ensuring-consistency-across-federated-learning-nodes","title":"Challenges: Privacy and security of model updates, ensuring consistency across federated learning nodes.","text":""},{"location":"Community/gsoc/2025/ideas/#4-aiml-model-monitoring-and-drift-detection-in-disconnected-clusters","title":"4. AI/ML Model Monitoring and Drift Detection in Disconnected Clusters","text":""},{"location":"Community/gsoc/2025/ideas/#goal-build-a-monitoring-system-that-detects-model-drift-in-disconnected-environments-and-triggers-alerts-or-automatic-retraining-using-kubestellar","title":"Goal: Build a monitoring system that detects model drift in disconnected environments and triggers alerts or automatic retraining using KubeStellar.","text":""},{"location":"Community/gsoc/2025/ideas/#features_3","title":"Features:","text":"<ul> <li>Deploy AI models with embedded monitoring hooks that capture drift signals (e.g., statistical changes in input distributions).</li> <li>Store and sync monitoring metrics when connectivity is restored.</li> <li>Provide a mechanism for automatic model retraining and redeployment.</li> </ul>"},{"location":"Community/gsoc/2025/ideas/#challenges-efficiently-storing-and-analyzing-monitoring-data-locally-reducing-unnecessary-sync-traffic","title":"Challenges: Efficiently storing and analyzing monitoring data locally, reducing unnecessary sync traffic.","text":""},{"location":"Community/gsoc/2025/ideas/#5-optimized-model-compression-and-deployment-for-edge-devices","title":"5. Optimized Model Compression and Deployment for Edge Devices","text":""},{"location":"Community/gsoc/2025/ideas/#goal-integrate-automatic-model-compression-quantization-pruning-distillation-into-kubestellar-to-optimize-ai-deployments-in-disconnected-clusters","title":"Goal: Integrate automatic model compression (quantization, pruning, distillation) into KubeStellar to optimize AI deployments in disconnected clusters.","text":""},{"location":"Community/gsoc/2025/ideas/#features_4","title":"Features:","text":"<ul> <li>Implement policies that choose between full, quantized, or pruned models based on available resources.</li> <li>Automate model format conversion for optimized inference (e.g., TensorFlow Lite, ONNX).</li> <li>Sync only compressed versions when bandwidth is limited.</li> </ul>"},{"location":"Community/gsoc/2025/ideas/#challenges-ensuring-compressed-models-maintain-acceptable-accuracy-managing-multiple-model-versions","title":"Challenges: Ensuring compressed models maintain acceptable accuracy, managing multiple model versions.","text":""},{"location":"Community/kubecon/eu2025/contribfest/","title":"Join us for KubeStellar's Contribfest at KubeCon EU 2025 in London, UK!","text":"<p> KubeStellar Contribfest KubeCon EU 2025 London, UK </p> <p>We\u2019re thrilled to be hosting a Contribfest session at KubeCon + CloudNativeCon Europe 2025 in London, UK! Join us on Thursday, April 3, 2025, from 4:00pm to 5:15pm BST in ExceL London, Level 3 | ICC Capital Suite 1.</p> <p>As a Sandbox CNCF project, KubeStellar is at an exciting stage of growth, and there\u2019s never been a better time to get involved as an open-source contributor. In this post, we\u2019ll explain what Contribfest is, highlight some beginner-friendly issues, and show you how you can join the fun, whether you're attending in London or contributing remotely.</p>"},{"location":"Community/kubecon/eu2025/contribfest/#what-is-contribfest","title":"What is Contribfest?","text":"<p>Contribfest is an interactive session where attendees can collaborate with project maintainers and community contributors to tackle beginner-friendly issues, hunt bugs, discuss feature ideas, and even pair program to contribute directly to CNCF projects.</p> <p>At this year\u2019s KubeStellar Contribfest, project maintainers will guide contributors of all experience levels through the KubeStellar codebase, share insights into its architecture, and help attendees make their first contributions to this groundbreaking project for multicluster application orchestration.</p> <p>Whether you're new to KubeStellar or ready to dive deep, you'll have the chance to get familiar with the KubeStellar developer experience, work side-by-side with maintainers, and become part of the community driving innovation in the cloud-native ecosystem.</p>"},{"location":"Community/kubecon/eu2025/contribfest/#good-first-issues","title":"Good first issues","text":"<p>Want to get started ahead of Contribfest? Here are some open issues to consider bringing to the session\u2014or to start working on right now:</p> <ul> <li>[FEATURE] - kubestellar#2158 - Prometheus metrics</li> <li>[FEATURE] - kubestellar#2094 - enable pprof to all KubeStellar Controllers</li> <li>[DOCUMENTATION] - kubestellar#1915 - document KubeStellar tear-down procedure</li> <li>[FEATURE] - kubeflex#307 - enable me to work with more than one system</li> <li>[BUG] - kubeflex#300 - wait for condition not working as expected with control plane conditions</li> <li>[FEATURE] - kubeflex#271 - command is called 'kflex' but brew formula is called 'kubeflex' - pick one please</li> <li>[DOCUMENTATION] - kubeflex#270 - Clarity on installation instructions</li> <li>[FEATURE] - kubeflex#159 - List clusters</li> <li>[FEATURE] - kubeflex#165 - Add kflex command/flag to get the current context</li> </ul> <p>Explore the complete list of issues on KubeStellar's GitHub https://github.com/kubestellar/kubestellar/issues.</p>"},{"location":"Community/kubecon/eu2025/contribfest/#help-wanted","title":"Help wanted","text":"<p>Help us improve KubeStellar by contribution to upstream projects:</p> <ul> <li>[FEATURE] - HELM: Introduce an annotation that would make Helm wait for user-specified resources</li> <li>[FEATURE] - HELM: Introduce priority/coordination of Helm resource creation</li> <li>[FEATURE] - HELM: Introduce an alternative path for dependence chart override values)</li> <li>[FEATURE] - HELM: Introduce a mechanism for a chart to require a specified Helm min version or range)</li> </ul>"},{"location":"Community/kubecon/eu2025/contribfest/#join-us","title":"Join us!","text":"<p>If you\u2019re in London, join us for Contribfest on Thursday, April 3, 2025, from 4:00pm to 5:15pm BST in ExCeL London, Level 3 | ICC Capital Suite 1.</p> <p>No matter where you are, you can connect with us anytime on the KubeStellar Slack or during our weekly community meetings every Wednesday at 4:00pm UTC. Come hang out, learn about multicluster orchestration, and discover how you can contribute to KubeStellar!</p>"},{"location":"Community/kubecon/eu2025/contribfest/#we-cant-wait-to-see-you-at-kubecon-eu-2025","title":"We can\u2019t wait to see you at KubeCon EU 2025!","text":""},{"location":"Community/partners/argocd/","title":"ArgoCD","text":"<p>This document explains how to add KubeStellar's 'workspaces' as Argo CD's 'clusters'.</p>"},{"location":"Community/partners/argocd/#add-kubestellars-workspaces-to-argo-cd-as-clusters","title":"Add KubeStellar's workspaces to Argo CD as clusters","text":"<p>As of today, the 'workspaces', aka 'logical clusters' used by KubeStellar are not identical with ordinary Kubernetes clusters. Thus, in order to add them as Argo CD's 'clusters', there are a few more steps to take.</p> <p>For KubeStellar's Inventory Management Workspace (IMW) and Workload Management Workspace (WMW). The steps are similar. Let's take WMW as an example:</p> <ol> <li>Create `kube-system` namespace in the workspace.</li> <li>Make sure necessary apibindings exist in the workspace.  For WMW, we need one for Kubernetes and one for KubeStellar's edge API.</li> <li>Exclude `ClusterWorkspace` from discovery and sync.  <pre><code>kubectl -n argocd edit cm argocd-cm\n</code></pre>  Make sure `resource.exclusions` exists in the `data` field of the `argocd-cm` configmap as follows: <pre><code>data:\n  resource.exclusions: |\n    - apiGroups:\n      - \"tenancy.kcp.io\"\n      kinds:\n      - \"ClusterWorkspace\"\n      clusters:\n      - \"*\"\n</code></pre>  Restart the Argo CD server. <pre><code>kubectl -n argocd rollout restart deployment argocd-server\n</code></pre>  Argo CD's documentation mentions this feature as [Resource Exclusion/Inclusion](https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#resource-exclusioninclusion). </li> <li>Make sure the current context uses WMW, then identify the admin.kubeconfig. The command and output should be similar to <pre><code>$ argocd cluster add --name wmw --kubeconfig ./admin.kubeconfig workspace.kcp.io/current\nWARNING: This will create a service account `argocd-manager` on the cluster referenced by context `workspace.kcp.io/current` with full cluster level privileges. Do you want to continue [y/N]? y\nINFO[0001] ServiceAccount \"argocd-manager\" already exists in namespace \"kube-system\"\nINFO[0001] ClusterRole \"argocd-manager-role\" updated\nINFO[0001] ClusterRoleBinding \"argocd-manager-role-binding\" updated\nCluster 'https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo' added\n</code></pre>  ### Create Argo CD Applications Once KubeStellar's workspaces are added, Argo CD Applications can be created as normal. There are a few examples listed [here](https://github.com/edge-experiments/gitops-source/tree/main/kubestellar), and the commands to use the examples are listed as follows.  #### Create Argo CD Applications against KubeStellar's IMW Create two Locations. The command and output should be similar to <pre><code>$ argocd app create locations \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/locations/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:imw-turbo \\\n--sync-policy automated\napplication 'locations' created\n</code></pre>  Create two SyncTargets. The command and output should be similar to <pre><code>$ argocd app create synctargets \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/synctargets/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:imw-turbo \\\n--sync-policy automated\napplication 'synctargets' created\n</code></pre>  #### Create Argo CD Application against KubeStellar's WMW Create a Namespace. The command and output should be similar to <pre><code>$ argocd app create namespace \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/namespaces/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'namespace' created\n</code></pre>  Create a Deployment for 'cpumemload'. The command and output should be similar to <pre><code>$ argocd app create cpumemload \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/workloads/cpumemload/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'cpumemload' created\n</code></pre>  Create an EdgePlacement. The command and output should be similar to <pre><code>$ argocd app create edgeplacement \\\n--repo https://github.com/edge-experiments/gitops-source.git \\\n--path kubestellar/placements/ \\\n--dest-server https://172.31.31.125:6443/clusters/root:my-org:wmw-turbo \\\n--sync-policy automated\napplication 'edgeplacement' created\n</code></pre> </li> </ol>"},{"location":"Community/partners/argocd/#other-resources","title":"Other Resources","text":"<p>Medium - Sync 10,000 ArgoCD Applications in One Shot Medium - Sync 10,000 ArgoCD Applications in One Shot, by Yourself Medium - GitOpsCon - here we come</p>"},{"location":"Community/partners/argocd/#argocd-scale-experiment-kubestellar-community-demo-day","title":"ArgoCD Scale Experiment - KubeStellar Community Demo Day","text":""},{"location":"Community/partners/argocd/#gitopscon-2023-a-quantitative-study-on-argo-scalability-andrew-anderson-jun-duan-ibm","title":"GitOpsCon 2023 - A Quantitative Study on Argo Scalability - Andrew Anderson &amp; Jun Duan, IBM","text":""},{"location":"Community/partners/argocd/#argocd-and-kubestellar-in-the-news","title":"ArgoCD and KubeStellar in the news","text":""},{"location":"Community/partners/fluxcd/","title":"FluxCD","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/kyverno/","title":"Check out KubeStellar working with Kyverno:","text":"<p>Medium - Syncing Objects from one Kubernetes cluster to another Kubernetes cluster</p>"},{"location":"Community/partners/kyverno/#kyverno-and-kubestellar-demo-day","title":"Kyverno and KubeStellar Demo Day","text":""},{"location":"Community/partners/kyverno/#kyverno-and-kubestellar-in-the-news","title":"Kyverno and KubeStellar in the news","text":""},{"location":"Community/partners/kyverno/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/mvi/","title":"Check out KubeStellar working with IBM's Maximo Visual Inspection (MVI):","text":"<p>Medium - Deployment and configuration of MVI-Edge using KubeStellar</p>"},{"location":"Community/partners/mvi/#mvi-and-kubestellar-demo-day","title":"MVI and KubeStellar Demo Day","text":""},{"location":"Community/partners/mvi/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>Work with us to create this document</p>"},{"location":"Community/partners/mvi/#mvi-and-kubestellar-in-the-news","title":"MVI and KubeStellar in the news","text":""},{"location":"Community/partners/openziti/","title":"OpenZiti","text":""},{"location":"Community/partners/turbonomic/","title":"Check out KubeStellar working with Turbonomic:","text":"<p>Medium - Make Multi-Cluster Scheduling a No-Brainer</p>"},{"location":"Community/partners/turbonomic/#turbonomic-and-kubestellar-demo-day","title":"Turbonomic and KubeStellar Demo Day","text":""},{"location":"Community/partners/turbonomic/#how-do-i-get-this-working-with-my-kubestellar-instance","title":"How do I get this working with my KubeStellar instance?","text":"<p>As we can see from the blog and the demo, Turbonomic talks to KubeStellar via GitOps. The scheduling decisions are passed from Turbonomic to KubeStellar in two steps: 1. Turbo -&gt; GitHub repository. 2. GitHub repository -&gt; KubeStellar.</p> <p>For the first step (Turbonomic -&gt; GitHub repository), a controller named \"change reconciler\" creates PRs against the GitHub repository, where the PRs contains changes to scheduling decisions.</p> <p>There's also a piece of code which intercepts Turbonomic actions and creates CRs for the above change reconciler.</p> <p>For the second step (GitHub repository-&gt; KubeStellar), we can use Argo CD. The detailed procedure to integrate Argo CD with KubeStellar is documented here.</p> <p>As we can see from the blog and the demo, Turbonomic collects data from edge clusters. This is made possible by installing kubeturbo into each of the edge clusters.</p>"},{"location":"Community/partners/turbonomic/#turbonomic-and-kubestellar-in-the-news","title":"Turbonomic and KubeStellar in the news","text":""},{"location":"common-subs/coming-soon/","title":"Coming soon","text":""},{"location":"common-subs/placeholder/","title":"Placeholder","text":"<ul> <li>This is a placeholder file to allow  building the navigation</li> </ul>"},{"location":"contribution-guidelines/coc-inc/","title":"Code of Conduct","text":"<p>This project is following the CNCF Code of Conduct. </p>"},{"location":"contribution-guidelines/coc-inc/#kubestellar-community-code-of-conduct","title":"KubeStellar Community Code of Conduct","text":"<p>As contributors, maintainers, and participants in the CNCF community, and in the interest of fostering an open and welcoming community, we pledge to respect all people who participate or contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, attending conferences or events, or engaging in other community or project activities.</p> <p>We are committed to making participation in the CNCF community a harassment-free experience for everyone, regardless of age, body size, caste, disability, ethnicity, level of experience, family status, gender, gender identity and expression, marital status, military or veteran status, nationality, personal appearance, race, religion, sexual orientation, socioeconomic status, tribe, or any other dimension of diversity.</p>"},{"location":"contribution-guidelines/coc-inc/#scope","title":"Scope","text":"<p>This code of conduct applies: * within project and community spaces, * in other spaces when an individual CNCF community participant's words or actions are directed at or are about a CNCF project, the CNCF community, or another CNCF community participant.</p>"},{"location":"contribution-guidelines/coc-inc/#cncf-events","title":"CNCF Events","text":"<p>CNCF events that are produced by the Linux Foundation with professional events staff are governed by the Linux Foundation Events Code of Conduct available on the event page. This is designed to be used in conjunction with the CNCF Code of Conduct.</p>"},{"location":"contribution-guidelines/coc-inc/#our-standards","title":"Our Standards","text":"<p>The CNCF Community is open, inclusive and respectful. Every member of our community has the right to have their identity respected.</p> <p>Examples of behavior that contributes to a positive environment include but are not limited to:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> <li>Using welcoming and inclusive language</li> </ul> <p>Examples of unacceptable behavior include but are not limited to:</p> <ul> <li>The use of sexualized language or imagery</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment in any form</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Violence, threatening violence, or encouraging others to engage in violent behavior</li> <li>Stalking or following someone without their consent</li> <li>Unwelcome physical contact</li> <li>Unwelcome sexual or romantic attention or advances</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul> <p>The following behaviors are also prohibited: * Providing knowingly false or misleading information in connection with a Code of Conduct investigation or otherwise intentionally tampering with an investigation. * Retaliating against a person because they reported an incident or provided information about an incident as a witness.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct.  By adopting this Code of Conduct, project maintainers commit themselves to fairly and consistently applying these principles to every aspect of managing a CNCF project.  Project maintainers who do not follow or enforce the Code of Conduct may be temporarily or permanently removed from the project team.</p>"},{"location":"contribution-guidelines/coc-inc/#reporting","title":"Reporting","text":"<p>For incidents occurring in the KubeStellar community, contact the KubeStellar Code of Conduct Committee of Conduct Committee. You can expect a response within three business days.</p> <p>For other projects, or for incidents that are project-agnostic or impact multiple CNCF projects, please contact the CNCF Code of Conduct Committee via <code>conduct@cncf.io</code>.  Alternatively, you can contact any of the individual members of the CNCF Code of Conduct Committee to submit your report. For more detailed instructions on how to submit a report, including how to submit a report anonymously, please see our Incident Resolution Procedures. You can expect a response within three business days.</p> <p>For incidents occurring at CNCF event that is produced by the Linux Foundation, please contact <code>eventconduct@cncf.io</code>.</p>"},{"location":"contribution-guidelines/coc-inc/#enforcement","title":"Enforcement","text":"<p>Upon review and investigation of a reported incident, the CoC response team that has jurisdiction will determine what action is appropriate based on this Code of Conduct and its related documentation. </p> <p>For information about which Code of Conduct incidents are handled by project leadership, which incidents are handled by the CNCF Code of Conduct Committee, and which incidents are handled by the Linux Foundation (including its events team), see our Jurisdiction Policy.</p>"},{"location":"contribution-guidelines/coc-inc/#amendments","title":"Amendments","text":"<p>Consistent with the CNCF Charter, any substantive changes to this Code of Conduct must be approved by the Technical Oversight Committee.</p>"},{"location":"contribution-guidelines/coc-inc/#acknowledgements","title":"Acknowledgements","text":"<p>This Code of Conduct is adapted from the Contributor Covenant (http://contributor-covenant.org), version 2.0 available at http://contributor-covenant.org/version/2/0/code_of_conduct/</p>"},{"location":"contribution-guidelines/contributing-inc/","title":"Contributing to KubeStellar","text":"<p>Greetings! We are grateful for your interest in joining the KubeStellar community and making a positive impact. Whether you're raising issues, enhancing documentation, fixing bugs, or developing new features, your contributions are essential to our success.</p> <p>To get started, kindly read through this document and familiarize yourself with our code of conduct. If you have any inquiries, please feel free to reach out to us on Slack.</p> <p>We can't wait to collaborate with you!</p> <p>This document describes our policies, procedures and best practices for working on KubeStellar via the project and repository on GitHub. Much of this interaction (issues, pull requests, discussions) is meant to be viewed directly at the KubeStellar repository webpage on GitHub. Other community discussions and questions are available via our slack channel. If you have any inquiries, please feel free to reach out to us on the KubeStellar-dev Slack channel.</p> <p>Please read the following guidelines if you're interested in contributing to KubeStellar.</p>"},{"location":"contribution-guidelines/contributing-inc/#general-practices-in-the-kubestellar-github-project","title":"General practices in the KubeStellar GitHub Project","text":""},{"location":"contribution-guidelines/contributing-inc/#contributing-code-prerequisites","title":"Contributing Code -- Prerequisites","text":"<p>Please make sure that your environment has all the necessary versions as spelled out in the prerequisites section of our user guide</p>"},{"location":"contribution-guidelines/contributing-inc/#issues","title":"Issues","text":"<p>View active issues on GitHub</p> <p>Prioritization for pull requests is given to those that address and resolve existing GitHub issues. Utilize the available issue labels to identify meaningful and relevant issues to work on.</p> <p>If you believe that there is a need for a fix and no existing issue covers it, feel free to create a new one.</p> <p>As a new contributor, we encourage you to start with issues labeled as good first issue.</p> <p>We also have a subset of issues we've labeled help wanted!</p> <p>Your assistance in improving documentation is highly valued, regardless of your level of experience with the project.</p> <p>To claim an issue that you are interested in, kindly leave a comment on the issue and request the maintainers to assign it to you.</p>"},{"location":"contribution-guidelines/contributing-inc/#committing","title":"Committing","text":"<p>We encourage all contributors to adopt best practices in git commit management to facilitate efficient reviews and retrospective analysis. Note: that document was written for projects where some of the contributors are doing merges into the main branch, but in KubeStellar we have GitHub doing that for us. For the kubestellar repository, this is controlled by Prow; for the other repositories in the kubestellar organization we use the GitHub mechanisms directly.</p> <p>Your git commits should provide ample context for reviewers and future codebase readers.</p> <p>A recommended format for final commit messages is as follows:</p> <p><pre><code>{Short Title}: {Problem this commit is solving and any important contextual information} {issue number if applicable}\n</code></pre> In conformance with CNCF expectations, we will only merge commits that indicate your agreement with the Developer Certificate of Origin. The CNCF defines how to do this, and there are two cases: one for developers working for an organization that is a CNCF member, and one for contributors acting as individuals. For the latter, assent is indicated by doing a Git \"sign-off\" on the commit. </p> <p>See Git Commit Signoff and Signing for more information on how to do that.</p>"},{"location":"contribution-guidelines/contributing-inc/#pull-requests","title":"Pull Requests","text":"<p>View active Pull Requests on GitHub</p> <p>When submitting a pull request, clear communication is appreciated. This can be achieved by providing the following information:</p> <ul> <li>Detailed description of the problem you are trying to solve, along with links to related GitHub issues</li> <li>Explanation of your solution, including links to any design documentation and discussions</li> <li>Information on how you tested and validated your solution</li> <li>Updates to relevant documentation and examples, if applicable</li> </ul> <p>Following are a few more things to keep in mind when making a pull request.</p> <ul> <li>Smaller pull requests are typically easier to review and merge than larger ones. If your pull request is big, it is always recommended to collaborate with the maintainers to find the best way to divide it.</li> <li>Do not make a PR from your <code>main</code> branch. Your life will be much easier if the <code>main</code> branch in your fork tracks the <code>main</code> branch in the shared repository.</li> <li>Learn to use <code>git rebase</code>. It is your friend. It is one of your most helpful friends. It is how you can cope when other changes merge while you are in the midst of working on your PR.</li> <li>There are, broadly speaking, two styles of using Git history: keeping an accurate record of your development process, or producing a simple explanation of the end result. We aim for the latter. Squash out uninteresting intermediate commits.</li> <li>Do not merge from <code>main</code> into your PR's branch. That makes a tangled Git history, and we prefer to keep it simple. Instead, rebase your PR's branch onto the latest edition of <code>main</code>.</li> <li>When adding/updating a GitHub Actions workflow, be aware of the action reference discipline.</li> <li>For a PR that modifies the website, include a preview. That gets much easier if you follow the documentation about setting up for that (i.e., properly create your <code>gh-pages</code> branch, enabling its use in your fork's settings) and make the name of your PR's branch start with \"doc-\". If you already have a PR with a different sort of name, you can explicitly invoke the rendering workflow --- unless your branch name has a slash or other exotic character in it; stick to alphanumerics plus dash and dot. You can not change the name of the branch in a PR, but you can close a PR and open an equivalent one using a branch with a good name.</li> <li>For a PR that modifies the website, remember that the doc source files are viewed two ways (see the website documentation); make them work in both views.</li> <li>If you mix pervasive changes to whitespace with substantial changes, you risk GitHub's display of the diff becoming confused. DO check that. If the diff display is confused, it makes reviewing much harder. Have mercy on your reviewers; skip the pervasive whitespace changes if they confuse GitHub's diff. BTW, did you really intend to make all those whitespace changes, or are they an unintended gift from your IDE? Don't make changes that you do not really intend.</li> </ul>"},{"location":"contribution-guidelines/contributing-inc/#titling-pull-requests","title":"Titling Pull Requests","text":"<p>We require that the title of each pull request start with a special nickname character (emoji) that classifies the request into one of the following categories. </p> <p>The nickname characters to use for different PRs are as follows</p> <ul> <li>\u2728 (nickname <code>:sparkles:</code>) feature</li> <li>\ud83d\udc1b (nickname <code>:bug:</code>) bug fix</li> <li>\ud83d\udcd6 (nickname <code>:book:</code>) docs</li> <li>\ud83d\udcdd (nickname <code>:memo:</code>)  proposal</li> <li>\u26a0\ufe0f (nickname <code>:warning:</code>) breaking change</li> <li>\ud83c\udf31 (nickname <code>:seedling:</code>) other/misc</li> <li>\u2753 (nickname <code>:question:</code>) requires manual review/categorization</li> </ul> <p>Note: The GitHub web interface will assist you with adding the character; while editing the title of your pull request:</p> <ul> <li>type a colon (':')</li> <li>begin typing the character nickname (_e.g. sparkles)_</li> <li>the web interface should offer you a pick-list of corresponding characters.</li> <li>Just click on the correct one to insert it in the title</li> <li>Add at least one space after the special character.</li> </ul>"},{"location":"contribution-guidelines/contributing-inc/#continuous-integration","title":"Continuous Integration","text":"<p>Pull requests are subjected to checking by a collection of GitHub Actions workflows and Prow jobs. The infra repo defines the Prow instance used for KubeStellar. The GitHub Actions workflows are found in the .github/workflows directory.</p>"},{"location":"contribution-guidelines/contributing-inc/#github-action-reference-discipline","title":"GitHub Action reference discipline","text":"<p>For the sake of supply chain security, every reference from a workflow to an action identifies the action's version by a commit hash. In particular, there is a file that lists the approved commit hash for each action. The file should be updated/extended only when you have confidence in the new/added version. There is a script for updating and checking this stuff. There is a workflow that checks that every workflow follows the discipline here.</p>"},{"location":"contribution-guidelines/contributing-inc/#review-and-approval-process","title":"Review and Approval Process","text":"<p>Reviewers will review your PR within a business day. A PR requires both an <code>/lgtm</code> and then an <code>/approve</code> in order to get merged. These are commands to Prow, each appearing alone on a line in a comment of the PR. You may <code>/approve</code> your own PR but you may not <code>/lgtm</code> it. Once both forms of assent have been given and the other gating checks have passed, the PR will go into the Prow merge queue and eventually be merged. Once that happens, you will be notified:</p> <p>Congratulations! Your pull request has been successfully merged! \ud83d\udc4f</p> <p>If you have any questions about contributing, don't hesitate to reach out to us on the KubeStellar-dev Slack channel.</p>"},{"location":"contribution-guidelines/contributing-inc/#testing-locally","title":"Testing Locally","text":"<p>Our Getting Started guide shows a user how to install a simple \"kick the tires\" instance of KubeStellar using a helm chart and kind.</p> <p>To set up and test a development system, please refer to the test/e2e/README.md file in the GitHub repository. After running any of those e2e (end to end) tests you will be left with a running system that can be exercised further.</p>"},{"location":"contribution-guidelines/contributing-inc/#testing-changes-to-the-helm-chart","title":"Testing changes to the helm chart","text":"<p>If you are interested in modifying the Helm chart itself, look at the User Guide page on the Core Helm chart for more information on its many options before you begin, notably on how to specify using a local version of the script.</p>"},{"location":"contribution-guidelines/contributing-inc/#testing-the-script-against-an-upcoming-release","title":"Testing the script against an upcoming release","text":"<p>Prior to making a new release, there needs to be testing that the current Helm chart works with the executable behavior that will appear in the new release.  </p>"},{"location":"contribution-guidelines/contributing-inc/#licensing","title":"Licensing","text":"<p>KubeStellar is Apache 2.0 licensed and we accept contributions via GitHub pull requests.</p>"},{"location":"contribution-guidelines/contributing-inc/#certificate-of-origin","title":"Certificate of Origin","text":"<p>By contributing to this project you agree to the Developer Certificate of Origin (DCO). This document was created by the Linux Kernel community and is a simple statement that you, as a contributor, have the legal right to make the contribution. See the DCO file for details.</p> <p>Note: this file \"contribute-github.md\" is a wrapper that includes the CONTRIBUTING.md file found in the root of the KubeStellar repository, adding website relative links while eliding relative navigation links that are specific to the repository folder structure</p>"},{"location":"contribution-guidelines/governance-inc/","title":"Governance","text":""},{"location":"contribution-guidelines/governance-inc/#kubestellar-project-governance","title":"KubeStellar Project Governance","text":"<p>The KubeStellar project is dedicated to solving challenges stemming from multi-cluster configuration management for edge, multi-cloud, and hybrid cloud.  This governance explains how the project is run.</p> <ul> <li>Manifesto</li> <li>Values</li> <li>Maintainers</li> <li>Code of Conduct Enforcement</li> <li>Security Response Team</li> <li>Voting</li> <li>Modifying this Charter</li> </ul>"},{"location":"contribution-guidelines/governance-inc/#manifesto","title":"Manifesto","text":"<ul> <li>KubeStellar Maintainers strive to be good citizens in the Kubernetes project.</li> <li>KubeStellar Maintainers see KubeStellar always as part of the Kubernetes ecosystem and always     strive to keep that ecosystem united. In particular, this means:</li> <li>KubeStellar strives to not divert from Kubernetes, but strives to extend its       use-cases to non-container control planes while keeping the ecosystems of       libraries and tooling united.</li> <li>KubeStellar \u2013 as a consumer of Kubernetes API Machinery \u2013 will strive to stay 100%       compatible with the semantics of Kubernetes APIs, while removing container       orchestration specific functionality.</li> <li>KubeStellar strives to upstream changes to Kubernetes code as much as possible.</li> </ul>"},{"location":"contribution-guidelines/governance-inc/#values","title":"Values","text":"<p>The KubeStellar and its leadership embrace the following values:</p> <ul> <li>Openness: Communication and decision-making happens in the open and is     discoverable for future reference. As much as possible, all discussions and     work take place in public forums and open repositories.</li> <li>Fairness: All stakeholders have the opportunity to provide feedback and     submit contributions, which will be considered on their merits.</li> <li>Community over Product or Company: Sustaining and growing our community     takes priority over shipping code or sponsors' organizational goals. Each     contributor participates in the project as an individual.</li> <li>Inclusivity: We innovate through different perspectives and skill sets,     which can only be accomplished in a welcoming and respectful environment.</li> <li>Participation: Responsibilities within the project are earned through     participation, and there is a clear path up the contributor ladder into     leadership positions.</li> </ul>"},{"location":"contribution-guidelines/governance-inc/#maintainers","title":"Maintainers","text":"<p>KubeStellar Maintainers have write access to the project GitHub repository. They can merge their own patches or patches from others. The current maintainers can be found as top-level approvers in OWNERS.  Maintainers collectively  manage the project's resources and contributors.</p> <p>This privilege is granted with some expectation of responsibility: maintainers are people who care about the KubeStellar project and want to help it grow and improve. A maintainer is not just someone who can make changes, but someone who has demonstrated their ability to collaborate with the team, get the most knowledgeable people to review code and docs, contribute high-quality code, and follow through to fix issues (in code or tests).</p> <p>A maintainer is a contributor to the project's success and a citizen helping the project succeed.</p> <p>The collective team of all Maintainers is known as the Maintainer Council, which  is the governing body for the project.</p>"},{"location":"contribution-guidelines/governance-inc/#becoming-a-maintainer","title":"Becoming a Maintainer","text":"<p>To become a Maintainer you need to demonstrate the following:</p> <ul> <li>commitment to the project:<ul> <li>participate in discussions, contributions, code and documentation reviews   for 3 months or more,</li> <li>perform reviews for 5 non-trivial pull requests,</li> <li>contribute 5 non-trivial pull requests and have them merged,</li> </ul> </li> <li>ability to write quality code and/or documentation,</li> <li>ability to collaborate with the team,</li> <li>understanding of how the team works (policies, processes for testing and code review, etc),</li> <li>understanding of the project's code base and coding and documentation style.    </li> </ul> <p>A new Maintainer must be proposed by an existing maintainer by sending a message to the developer mailing list. A simple majority  vote of existing Maintainers approves the application.</p> <p>Maintainers who are selected will be granted the necessary GitHub rights, and invited to the private maintainer mailing list.</p>"},{"location":"contribution-guidelines/governance-inc/#bootstrapping-maintainers","title":"Bootstrapping Maintainers","text":"<p>To bootstrap the process, 3 maintainers are defined (in the initial PR adding  this to the repository) that do not necessarily follow the above rules. When a  new maintainer is added following the above rules, the existing maintainers  define one not following the rules to step down, until all of them follow the  rules.</p>"},{"location":"contribution-guidelines/governance-inc/#removing-a-maintainer","title":"Removing a Maintainer","text":"<p>Maintainers may resign at any time if they feel that they will not be able to  continue fulfilling their project duties.</p> <p>Maintainers may also be removed after being inactive, failure to fulfill their  Maintainer responsibilities, violating the Code of Conduct, or other reasons.  Inactivity is defined as a period of very low or no activity in the project for  a year or more, with no definite schedule to return to full Maintainer activity.</p> <p>A Maintainer may be removed at any time by a 2/3 vote of the remaining maintainers.</p> <p>Depending on the reason for removal, a Maintainer may be converted to Emeritus  status. Emeritus Maintainers will still be consulted on some project matters,  and can be rapidly returned to Maintainer status if their availability changes.</p>"},{"location":"contribution-guidelines/governance-inc/#meetings","title":"Meetings","text":"<p>Time zones permitting, Maintainers are expected to participate in the public  community call meeting. Maintainers will also have closed meetings in order to  discuss security reports or Code of Conduct violations. Such meetings should be  scheduled by any Maintainer on receipt of a security issue or CoC report.  All current Maintainers must be invited to such closed meetings, except for any  Maintainer who is accused of a CoC violation.</p>"},{"location":"contribution-guidelines/governance-inc/#code-of-conduct","title":"Code of Conduct","text":"<p>Code of Conduct violations by community members will be discussed and resolved on the private Maintainer mailing list.</p>"},{"location":"contribution-guidelines/governance-inc/#security-response-team","title":"Security Response Team","text":"<p>The Maintainers will appoint a Security Response Team to handle security reports. This committee may simply consist of the Maintainer Council themselves. If this  responsibility is delegated, the Maintainers will appoint a team of at least two  contributors to handle it. The Maintainers will review who is assigned to this  at least once a year.</p> <p>The Security Response Team is responsible for handling all reports of security  holes and breaches according to the security policy.</p>"},{"location":"contribution-guidelines/governance-inc/#voting","title":"Voting","text":"<p>While most business in KubeStellar is conducted by \"lazy consensus\", periodically the Maintainers may need to vote on specific actions or changes. A vote can be taken on the developer mailing list or the private Maintainer mailing list for security or conduct matters.  Votes may also be taken at the community call  meeting. Any Maintainer may demand a vote be taken.</p> <p>Most votes require a simple majority of all Maintainers to succeed. Maintainers can be removed by a 2/3 majority vote of all Maintainers, and changes to this Governance require a 2/3 vote of all Maintainers.</p>"},{"location":"contribution-guidelines/governance-inc/#modifying-this-charter","title":"Modifying this Charter","text":"<p>Changes to this Governance and its supporting documents may be approved by a  2/3 vote of the Maintainers.</p> <p>Note: this file \"governance-inc\" is a wrapper that includes the GOVERNANCE.md file found in the root of the KubeStellar repository</p>"},{"location":"contribution-guidelines/license-inc/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright 2023-2025 The KubeStellar Authors</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"contribution-guidelines/onboarding-inc/","title":"Onboarding","text":"<p>KubeStellar GitHub Organization On-boarding and Off-boarding Policy</p> <p>Effective Date: June 1st, 2023</p> <p>At KubeStellar we love our contributors.  Our contributors can make various valuable contributions to our project. They can actively engage in code development by submitting pull requests, implementing new features, or fixing bugs. Additionally, contributors can assist with testing, CICD, documentation, providing clear and comprehensive guides, tutorials, and examples. Moreover, they can contribute to the project by participating in discussions, offering feedback, and helping to improve overall community engagement and collaboration.</p> <ol> <li> <p>Introduction: The purpose of this policy is to ensure a smooth on-boarding and off-boarding process for members of the KubeStellar GitHub organization. This policy applies to all individuals joining or leaving the organization, including community contributors.</p> </li> <li> <p>On-boarding Process: 2.1. Access Request:</p> </li> <li>New members shall submit an access request, via a blank GitHub issue from the KubeStellar repository, mentioning all members of the OWNERS file.</li> <li>The access request should include the member's GitHub username and a brief description of their role and contributions to the KubeStellar project.</li> </ol> <p>2.2. Review and Approval: - The organization's maintainers or designated personnel will review the access request issue. - The maintainers will evaluate the request based on the member's role, contributions, and adherence to the organization's code of conduct. - Upon approval, the member will receive an invitation to join the KubeStellar GitHub organization.</p> <p>2.3. Getting Help: - The organization's maintainers are here to help contributors be efficient and confident in their collaboration effort. If you need help you can reach out to the maintainers on slack at the KubeStellar-dev channel. - Be sure to join the KubeStellar-dev Google Group to get access to important artifacts like proposals, diagrams, and meeting invitations.</p> <p>2.4. Orientation: - Newly on-boarded members will be provided with contribution guidelines. - The guide will include instructions on how to access relevant repositories, participate in discussions, and contribute to ongoing projects.</p> <ol> <li>Off-boarding Process: 3.1. Departure Notification:</li> <li>Members leaving the organization shall notify the maintainers or their respective team lead in advance of their departure date.</li> <li>The notification should include the member's departure date and any necessary transition information.</li> </ol> <p>3.2. Access Termination: - Upon receiving the departure notification, the maintainers or designated personnel will initiate the off-boarding process. - The member's access to the KubeStellar GitHub organization will be revoked promptly to ensure data security and prevent unauthorized access.</p> <p>3.3. Knowledge Transfer: - Departing members should facilitate the transfer of their ongoing projects, tasks, and knowledge to their respective replacements or relevant team members. - Documentation or guidelines related to ongoing projects should be updated and made available to the team for seamless continuity.</p> <ol> <li>Code of Conduct:</li> <li>All members of the KubeStellar GitHub organization are expected to adhere to the organization's code of conduct, promoting a respectful and inclusive environment.</li> <li> <p>Violations of the code of conduct will be addressed following the organization's established procedures for handling such incidents.</p> </li> <li> <p>Policy Compliance:</p> </li> <li>It is the responsibility of all members to comply with the on-boarding and off-boarding policy.</li> <li> <p>The organization's maintainers or designated personnel will oversee the implementation and enforcement of this policy.</p> </li> <li> <p>Policy Review:</p> </li> <li>This policy will be reviewed periodically to ensure its effectiveness and relevance.</li> <li>Any updates or revisions to the policy will be communicated to the organization's members in a timely manner.</li> </ol> <p>Please note that this policy is subject to change, and any modifications will be communicated to all members of the KubeStellar GitHub organization.</p> <p>By joining the organization, all members agree to abide by the terms and guidelines outlined in this policy.</p> <p>Andy Anderson (clubanderson) KubeStellar Maintainer June 1, 2023</p>"},{"location":"contribution-guidelines/operations/code-management/","title":"Code management","text":""},{"location":"contribution-guidelines/operations/code-management/#code-management","title":"Code Management","text":"<p>Fork kubestellar into your own repo, create a local branch, set upstream to kubestellar, add and commit changes to local branch, and squash your commits</p>"},{"location":"contribution-guidelines/operations/code-management/#initial-setup","title":"Initial setup","text":""},{"location":"contribution-guidelines/operations/code-management/#fork-the-github-kubestellar-repo-into-your-own-github-repo","title":"Fork the Github kubestellar repo into your own Github repo:","text":"<p>You can do this either 1: from the kubestellar Github website using the \"Fork\" button or 2: by using the git fork command from your local git command line interface, such as git bash.</p> <p>copy the forked repo from Github to your local system by using the \"git clone\" command or by downloading the repository's zip file.</p> <p>In your new local forked repo, set upstream to kubestellar main</p> <p>check what your repository's remote settings are</p> <pre><code>git remote -v\n</code></pre>"},{"location":"contribution-guidelines/operations/code-management/#set-upstream-to-use-kubestellar","title":"Set upstream to use kubestellar:","text":"<pre><code>git remote add upstream git@github.com:kubestellar/kubestellar.git\n</code></pre> <p>For example:</p> <pre><code>$ git remote -v\norigin  git@github.com:fileppb/kubestellar.git (fetch)\norigin  git@github.com:fileppb/kubestellar.git (push)\n\n$ git remote add upstream git@github.com:kubestellar/kubestellar.git\n\n$ git remote -v\norigin  git@github.com:fileppb/kubestellar.git (fetch)\norigin  git@github.com:fileppb/kubestellar.git (push)\nupstream        git@github.com:kubestellar/kubestellar.git (fetch)\nupstream        git@github.com:kubestellar/kubestellar.git (push)\n\n$ git fetch upstream\nEnter passphrase for key '/c/Users/owner/.ssh/id_rsa':\nremote: Enumerating objects: 60394, done.\nremote: Counting objects: 100% (5568/5568), done.\nremote: Compressing objects: 100% (255/255), done.\nremote: Total 60394 (delta 4768), reused 5457 (delta 4706), pack-reused 54826\nReceiving objects: 100% (60394/60394), 52.38 MiB | 3.25 MiB/s, done.\nResolving deltas: 100% (34496/34496), completed with 415 local objects.\n\n$ git status\n\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n</code></pre>"},{"location":"contribution-guidelines/operations/code-management/#ongoing-contributions","title":"Ongoing contributions","text":""},{"location":"contribution-guidelines/operations/code-management/#prior-to-working-on-an-issue","title":"Prior to working on an issue","text":"<p>Ensure that you personal repository if up to date with the kubestellar repository. You can do this by opening your github repository page, check that the selected branch is \"main\", and press the \"sync fork\" button.</p>"},{"location":"contribution-guidelines/operations/code-management/#select-an-issue-to-work-on-and-create-a-local-branch","title":"Select an issue to work on and create a local branch,","text":"<p>Create a local branch for your work, preferably including the issue number in the branch name</p> <p>for example if working on issue #11187, then you might name your local branch \"issue-1187\"</p> <pre><code>git checkout -b issue-1187\n</code></pre>"},{"location":"contribution-guidelines/operations/code-management/#as-you-work-and-change-files-you-should-try-to-commit-relatively-small-pieces-of-work-using-the-following-commands","title":"As you work and change files, you should try to commit relatively small pieces of work, using the following commands","text":"<pre><code>git add (there are several options you can specify for the git add command)\n\ngit commit -m \"your message\"\n\ngit push -u origin branch-name (-u sets upstream to origin which is your remote github repository)\n</code></pre>"},{"location":"contribution-guidelines/operations/code-management/#when-you-have-completed-your-work-and-tested-it-locally-then-you-should-perform-a-squash-of-the-git-commits-to-make-the-upcoming-push-request-more-manageable","title":"When you have completed your work and tested it locally, then you should perform a squash of the git commits to make the upcoming push request more manageable.","text":"<p>To perform a squash, checkout the branch you want to squash, 1. use the \"git log\" command to see the history of commits to the branch 2. Count the number of commits you want to squash 3. use the \"git rebase -i HEAD~n\" where n is the number of commits you would like to squash together. (There are other ways to do this) 4. The text editor you have configured to use with git should automatically open your source and you will see a list of commits preceded by \"pick\". Leaving the first \"pick\" as it is, replace the remaining \"pick\"s with \"squash\"es.  5. Save the text file and exit the editor. 6. The text editor will open again to let you edit comments for your new squashed commit. 7. Make your edits if any and save and exit the file. The commits will then be squashed into one commit.</p>"},{"location":"contribution-guidelines/operations/code-management/#when-you-are-done-with-the-squash-push-your-changes-to-your-remote-branch-you-can-either","title":"When you are done with the squash, push your changes to your remote branch. You can either:","text":"<pre><code>git push -u origin &lt;branch-name&gt;\n\nor \n\ngit push --force-with-lease\n</code></pre> <p>Note: if using the git push -u origin  command, the -u only needs to specified the first time you push. It will set tracking for subsequent pushes to the branch. On the other hand, keeping the -u in the command does no particular harm."},{"location":"contribution-guidelines/operations/code-management/#run-actions-automated-workflow-tests-manually-in-your-personal-github-repository","title":"Run Actions (automated workflow tests) manually in your personal Github repository","text":"<ol> <li>Select the \"Actions\" tab toward the upper left of your github personal web page. This will cause a list of Actions to show.</li> <li>Select the action you wish to execute from the list of Actions. For example you might chose \"docs-ecutable - example1\". Note: docs-ecutable should be described in a separate section. But in a nutshell it's a Continuous Integration automation technique of embedding scripts and data within the body of documentation, and then parsing and executing those scripts which in turn interpret and execute source code from a branch that you designate. It's somewhat similar to Travis. So the Action \"docs-ecutable - example1\" executes scripts and data embedded within the documentation for the Example 1 scenario, described in the Kubestellar documents. Those scripts will run using the source code pointed to by the next step, step 3.</li> <li>Select the source code branch you wish to exercise by following the next 3 steps:</li> <li>select the black and white \"Run Workflow\" on the right side of your github web page. This will open a dialog box.</li> <li>within the dialog box, select the branch you wish to exercise by opening the dropdown labeled \"use workflow from\"</li> <li>within the dialog box, select the green \"Run Workflow\" button  Your selected Action workflow will execute and the results will be available when it completes.  </li> </ol>"},{"location":"contribution-guidelines/operations/code-management/#create-a-pull-request-pr-from-your-github-repo-branch-in-order-to-request-review-and-approval-from-the-kubestellar-team","title":"Create a Pull Request (PR) from your Github repo branch in order to request review and approval from the Kubestellar team","text":"<p>Take a look at https://github.com/kubestellar/kubestellar/blob/main/CONTRIBUTING.md</p> <p>You can create a Pull Request from your Github web repository by selecting the \"Compare &amp; pull request\" button.</p> <p>You will be presented with a Github web page titled Comparing Changes, which allows you to enter metadata regarding your pull request</p> <p>Reference the issue you are addressing ( add #issue-number) Add one of the listed emojis to the first character of the title of your new PR indicating the type of issue (bug fix, feature, etc) Complete the summary description field Complete the Related issue field by inserting the issue number preceded by the # character, for example \"#1187\" Decide whether this is a draft PR or if it's ready for review, and select the option you want by expanding on the Create Pull Request button. Assign a label to the PR from the available list of labels (a drop down list on the right side of the web page)</p> <p>Kubestellar CI pipeline:</p> <p>Prow (https://docs.prow.k8s.io/docs/overview/)</p>"},{"location":"contribution-guidelines/operations/document-management/","title":"Docs Management Overview","text":""},{"location":"contribution-guidelines/operations/document-management/#kubestellar-website-overview","title":"Kubestellar Website Overview","text":""},{"location":"contribution-guidelines/operations/document-management/#websites","title":"Websites","text":"<p>We have two web sites, as follows.</p> <ul> <li><code>https://kubestellar.io</code>. This is hosted by GoDaddy and administered by Andy Anderson. It contains a few redirects. The most important is that <code>https://kubestellar.io/</code> redirects to <code>https://docs.kubestellar.io/</code>.</li> <li><code>https://docs.kubestellar.io</code>. This is a GitHub pages website based on the <code>github.com/kubestellar/kubestellar/</code> repository.</li> </ul> <p>A contributor may have their own copy of the latter website, at <code>https://${repo_owner}.github.io/${fork_name}</code>, if they have set up the fork properly to render the webpages. This is useful for generating previews of changes before they are merged into the <code>main</code> branch of the shared repository. A PR that modifies the website should include such a preview. See the section below on Serving up documents globally from a fork of the repo via GitHub.</p>"},{"location":"contribution-guidelines/operations/document-management/#dual-use-documentation-sources","title":"Dual-use documentation sources","text":"<p>The documentation that is rendered to the website (as described in the rest of this page) is designed so that it can also be usefully viewed directly at GitHub. For example, you can view this page both (a) on the website at https://docs.kubestellar.io/unreleased-development/contribution-guidelines/operations/document-management/ and (b) directly in GitHub at https://github.com/kubestellar/kubestellar/blob/main/docs/README.md. Unfortunately, those two uses interpret the documentation sources a bit differently. To the degree possible, we choose to write the documentation sources in a way that is rendered consistently in those two views (for example: indenting with four spaces instead of three). When that is not possible, considerations for the website rendering take precedence.</p>"},{"location":"contribution-guidelines/operations/document-management/#github-pages","title":"GitHub pages","text":"<p>Our documentation is powered by mike and MkDocs. MkDocs is powered by Python-Markdown. These are immensely configurable and extensible. You can see our MkDocs configuration in <code>docs/mkdocs.yml</code>. Following are some of the choices we have made.</p> <ul> <li>The MkDocs theme is Material for MkDocs.</li> <li>MkDocs plugin awesome-pages for greater control over how navigation links are shown.</li> <li>MkDocs plugin macros.</li> <li>MkDocs plugin include-markdown, allowing the source to be factored into re-used files.</li> <li>Python-Markdown extension SuperFences, supporting fenced code blocks that play nice with other markdown features.</li> <li>Python-Markdown extension Highlight, for syntax highlighting of fenced code.</li> <li>Pygments for even fancier code highlighting.</li> <li>MkDocs plugin mkdocs-static-i18n to support multiple languages. We currently only have documentation in English. If you're interested in contributing translations, please let us know!</li> </ul>"},{"location":"contribution-guidelines/operations/document-management/#note-on-using-include-markdown-for-wrapper-files","title":"Note on using include-markdown for \"wrapper\" files","text":"<p>There are several places where pages in the website are generated by including a file in the root of the repository by wrapping it in a short file in the docs file tree via an include directive. This is especially used for including assorted REQUIRED files from the root (denoted by all-caps filenames). For ease of navigation during the editing process such wrapper files should be indicated by including -inc or -include as part of their filename. This will reduce the chance of a future documentation contributor editing the wrong file.</p>"},{"location":"contribution-guidelines/operations/document-management/#rendering-and-previewing-modifications-to-the-website","title":"Rendering and Previewing modifications to the website","text":"<p>You may preview possible changes to the website either (a) by downloading and rendering the documents locally, or (b) letting GitHub render a revised website from a branch in your fork of the kubestellar repository (which is highly recommended when making a PR that modifies the website).</p>"},{"location":"contribution-guidelines/operations/document-management/#serving-up-documents-globally-from-a-fork-of-the-repository-via-github","title":"Serving up documents globally from a fork of the repository via GitHub","text":"<p>You can take advantage of the \"Generate and Push Docs\" GitHub Actions workflow---invoked either explicitly in the GitHub web UI or implicitly by a branch naming convention---to create an online, shareable rendering of the website. This is particularly useful for documentation PRs, as it allows you to share a preview of your proposed changes directly via a URL to a working website. To take advantage of this action, you must ensure that you have forked the repository properly. This entails two things: having a version of the branch named <code>gh-pages</code>, and configuring GitHub to publish a website based on that branch.</p> <p>The name of your branch equals the \"version\" of the website where the rendering appears.</p>"},{"location":"contribution-guidelines/operations/document-management/#automatically-render-the-website","title":"Automatically render the website","text":"<p>The website rendering workflow will trigger automatically whenever you commit changes to a branch whose name begins with <code>doc-</code> (e.g. <code>doc-myversion</code>).  If you have prepared your fork for GitHub Pages rendering then the workflow will succeed to create/update a website rendering in the GitHub Pages of your fork. For example: if you name your branch <code>doc-holliday</code> then the GitHub pages for your fork will get a website version named <code>doc-holliday</code>.</p> <p>When preparing a PR that modifies the website, it is strongly recommended that you take advantage of this to get a preview automatically created, and include a URL for the preview in a comment in your PR.</p>"},{"location":"contribution-guidelines/operations/document-management/#creating-a-fork-that-can-use-the-generate-and-push-docs-action","title":"Creating a fork that can use the Generate and Push Docs Action","text":"<ol> <li>Log into your GitHub account via webbrowser</li> <li>Navigate to github.com/kubestellar/kubestellar</li> <li> <p>Select the Forks dropdown and click on the plus sign to create a new fork     Top of \"Code\" section of the shared repo </p> </li> <li> <p>In the resulting dialog select your account as the owner, pick a repository name for the fork, and be sure to uncheck the box next to \"copy the <code>main</code> branch only\" Creating your fork </p> <p>NOTE: If you already created a fork but only included the main branch then you can remedy the problem by propagating the <code>gh-pages</code> branch into your fork using <code>git</code> commands</p> </li> <li> <p>Go to the Settings tab of your fork, select \"Pages\" in the navigation panel on the left, and make sure that you have told GitHub to publish your site based on the contents of the <code>gh-pages</code> branch in your fork. It will look something like the following     Configure publishing your GitHub pages </p> </li> </ol>"},{"location":"contribution-guidelines/operations/document-management/#manually-generating-a-website-rendered-from-a-branch-of-your-fork","title":"Manually generating a website rendered from a branch of your fork","text":"<ol> <li>Work on the documents in a branch of your fork of the repository, and commit the changes</li> <li>(If you have been working on a local copy of the files, push the changes to the fork, then log into the GitHub webpage for your fork)</li> <li>Switch to the Actions tab in the top menu bar of the repository page</li> <li>Select Generate and Push Docs from the list of Actions on the left</li> <li>Click on the Run Workflow button on the right \"Run workflow\" button </li> <li>Select the branch you wish to render and click on the second Run Workflow Button Selecting the branch to render </li> <li>If that workflow completes successfully, it will automatically call the Pages build and deployment workflow.</li> <li>You can observe the progress of the workflows on the Actions page; a green checkmark circle indicates successful completion.Relevant workflow runs </li> <li>After a minute or so, you should be able to preview your new version of the website at <code>https://${repo_owner}.github.io/${fork_name}/${branch_name}</code></li> </ol>"},{"location":"contribution-guidelines/operations/document-management/#informational-errors","title":"Informational Errors","text":"<p>The \"Generate and push docs\" workflow triggers the broken links crawler workflow, which will fail when there are broken links. This failure also marks the \"Generate and push docs\" workflow as failed, even though it did not actually fail to generate and push. Do not be discouraged by these informational failures.</p>"},{"location":"contribution-guidelines/operations/document-management/#switching-between-versions","title":"Switching between versions","text":"<p>Each branch of your fork will render as its own version. You can use the release dropdown inside the rendered pages to quickly switch between versions.</p> <p>Note: the main branch will render as <code>https://${repo_owner}.github.io/${fork_name}/main</code>, NOT as \"unreleased-development\" which is a special alias on the main kubestellar.io website.</p>"},{"location":"contribution-guidelines/operations/document-management/#removing-outdated-draft-branch-versions-after-rendering","title":"Removing outdated (draft branch) versions after rendering","text":"<p>You can use <code>mike</code> to remove versions, or replace gh-pages with a copy of the shared version.  More details on these techniques will be added here soon.</p>"},{"location":"contribution-guidelines/operations/document-management/#serving-up-documents-locally","title":"Serving up documents locally","text":"<p>You can view and modify our documentation in your local development environment.  Simply checkout one of our branches.</p> <pre><code>git clone git@github.com:kubestellar/kubestellar.git\ncd kubestellar/docs\ngit checkout main\n</code></pre> <p>You can view and modify our documentation in the branch you have checked out by using <code>mkdocs serve</code> from mkdocs.  We have a Python requirements file in <code>requirements.txt</code>, and a Makefile target that builds a Python virtual environment and installs the requirements there.  You can either install those requirements into your global Python environment or use the Makefile target.  To install those requirements into your global Python environment, do the following usual thing.</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Alternatively, use the following commands to use the Makefile target to construct an adequate virtual environment and enter it.</p> <pre><code>( cd ..; make venv )\n. venv/bin/activate\n</code></pre> <p>Then, using your chosen environment with the requirements installed, build and serve the documents with the following command.</p> <p><pre><code>mkdocs serve\n</code></pre> Then open a browser to <code>http://localhost:8000/</code></p> <p>Another way to view (not modify - this method reflects what has been deployed to the <code>gh-pages</code> branch of our repo) all branches/versions of our documentation locally using 'mike' mike for mkdocs:</p> <p><pre><code>git clone git@github.com:kubestellar/kubestellar.git\ncd kubestellar\ngit checkout main\ncd docs\nmike set-default main\ncd ..\nmake serve-docs\n</code></pre> Then open a browser to <code>http://localhost:8000/</code></p>"},{"location":"contribution-guidelines/operations/document-management/#supported-aliases-for-our-documentation","title":"Supported aliases for our documentation","text":"<p><code>mike</code> has a concept of aliases. We currently maintain only one alias.</p> <ul> <li><code>latest</code> (https://docs.kubestellar.io/latest), for the latest regular release.</li> </ul> <p>The publishing workflow updates these aliases. The latest regular release is determined by picking the first version listed by <code>mike list</code> that matches the regexp <code>release-[0-9.]*</code>.</p>"},{"location":"contribution-guidelines/operations/document-management/#publishing-from-the-branch-named-main","title":"Publishing from the branch named \"main\"","text":"<p>The branch named \"main\" also gets published as a \"version\" on the website, but with a different name. This is not done by <code>mike</code> aliasing, because that only adds a version. The branch named \"main\" is published as the version named \"unreleased-development\".</p>"},{"location":"contribution-guidelines/operations/document-management/#shortcut-urls","title":"Shortcut URLs","text":"<p>We have a few shortcut urls that come in handy when referring others to our project:</p> <p>note: You need to join our mailing list first to get access to some of the links that follow (https://docs.kubestellar.io/joinus)</p> <ul> <li>https://kubestellar.io/agenda - our community meeting agenda google doc</li> <li>https://kubestellar.io/blog - our medium reading list</li> <li>https://kubestellar.io/code - our current GitHub repo (wherever that is)</li> <li>https://kubestellar.io/community - our stable docs community page</li> <li>https://kubestellar.io/drive - our google drive</li> <li>https://kubestellar.io/joinus - our dev mailing list where you join and get our invites</li> <li>https://kubestellar.io/join_us - also, our dev mailing list</li> <li>https://kubestellar.io/linkedin - our linkedin filter (soon, our page)</li> <li>https://kubestellar.io/tv - our youtube channel</li> <li>https://kubestellar.io/youtube - also, our youtube channel</li> <li>https://kubestellar.io/infomercial - our infomercial that premieres on June 12th at 9am</li> </ul> <p>and.. the very important\u2026 - https://kubestellar.io/quickstart - our 'stable' Getting Started recipe</p>"},{"location":"contribution-guidelines/operations/document-management/#jinja-templating","title":"Jinja templating","text":"<p>Our documentation stack includes Jinja. The Jinja constructs --- {# comment #}, {{ expression }}, and {% statement %} --- can appear in the markdown sources.</p>"},{"location":"contribution-guidelines/operations/document-management/#file-structure","title":"File structure","text":"<p>All documentation-related items live in <code>docs</code> (with the small exception of various <code>make</code> targets and some helper  scripts in <code>hack</code>).</p> <p>The structure of <code>docs</code> is as follows:</p> Path Description config/$language/mkdocs.yml Language-specific <code>mkdocs</code> configuration. content/$language Language-specific website content. generated/branch All generated content for all languages for the current version. generated/branch/$language Generated content for a single language. Never added to git. generated/branch/index.html Minimal index for the current version that redirects to the default language (en) overrides Global (not language-specific) content. Dockerfile Builds the kubestellar-docs image containing mkdocs + associated tooling. mkdocs.yml Minimal <code>mkdocs</code> configuration for <code>mike</code> for multi-version support. requirements.txt List of Python modules used to build the site."},{"location":"contribution-guidelines/operations/document-management/#global-variables","title":"Global Variables","text":"<p>There are many global variables defined in the docs/mkdocs.yml.  The following are some very common variables you are encouraged to use in our documentation.  Use of these variables/macros allows our documentation to have github branch context and take advantage of our evolution without breaking</p> <pre><code>- site_name: KubeStellar\n- repo_url: https://github.com/kubestellar/kubestellar\n- site_url: https://docs.kubestellar.io/unreleased-development\n- repo_default_file_path: kubestellar\n- repo_short_name: kubestellar/kubestellar\n- docs_url: https://docs.kubestellar.io\n- repo_raw_url: https://raw.githubusercontent.com/kubestellar/kubestellar\n- edit_uri: edit/main/docs/content/\n- ks_branch: main\n- ks_tag: latest\n- ks_latest_regular_release: 0.28.0\n- ks_latest_release: 0.28.0\n</code></pre> <p>to use a variables/macro in your documentation reference like this:</p> <p>{{ config.&lt;var_name&gt; }}</p> <p>and in context that can look something like this:</p> <p>bash &lt;(curl -s {{ config.repo_raw_url }}/{{ config.ks_branch }}/bootstrap/bootstrap-kubestellar.sh) --kubestellar-version {{ config.ks_tag }}</p> <p>note:  \u00a0\u00a0\u00a0\u00a0- We also check for broken links as part of our PR pipeline.  For more information check out our Broken Links Crawler</p>"},{"location":"contribution-guidelines/operations/document-management/#navigation-website-menu","title":"Navigation (website menu)","text":"<p>The navigation for the documentation is also configured in  docs/mkdocs.yml. The section which begins with nav: lays out the navigation structure and which markdown files correspond to each topic. </p>"},{"location":"contribution-guidelines/operations/document-management/#page-variables","title":"Page variables","text":"<p>A markdown source file can contribute additional variables by defining them in <code>name: value</code> lines at the start of the file, set off by lines of triple dashes. For example, suppose a markdown file begins with the following.</p> <pre><code>---\nshort_name: example1\nmanifest_name: 'docs/content/Coding Milestones/PoC2023q1/example1.md'\n---\n</code></pre> <p>These variables can be referenced as {{ page.meta.short_name }} and {{ page.meta.manifest_name }}.</p>"},{"location":"contribution-guidelines/operations/document-management/#including-external-markdown","title":"Including external markdown","text":"<p>We make extensive use of 'include-markdown' to help us keep our documentation modular and up-to-date.  To use 'include-markdown' you must add a block in your document that refers to a block in your external document content:</p> <p>In your original markdown document, add a block that refers to the external markdown you want to include: Include Markdown </p> <p>In the document you want to include, add the start and end tags you configured in the include-markdown block in your original document: Included Markdown </p> <p>for more information on the 'include-markdown' plugin for mkdocs look here</p>"},{"location":"contribution-guidelines/operations/document-management/#codeblocks","title":"Codeblocks","text":"<p>mkdocs has some very helpful ways to include blocks of code in a style that makes it clear to our readers that console interaction is necessary in the documentation.  There are options to include a plain codeblock (```), shell (shell), console (console - no used in our documentation), language or format-specific (yaml, etc.), and others.  For more detailed information, checkout the mkdocs information on codeblocks.</p> <p>NOTE: the docs-ecutable technology does not apply Jinja, at any stage; Jinja source inside executed code blocks will not be expanded by Jinja but rather seen directly by <code>bash</code>.</p> <p>Here are some examples of how we use codeblocks.</p>"},{"location":"contribution-guidelines/operations/document-management/#seen-and-executed","title":"Seen and executed","text":"<p>For a codeblock that can be 'tested' (and seen by the reader) as part of our CI, use the <code>shell</code> block: codeblock: <pre><code>```shell\nmkdocs serve\n```\n</code></pre> as seen by reader: <pre><code>mkdocs serve\n</code></pre> </p>"},{"location":"contribution-guidelines/operations/document-management/#executed-but-not-seen","title":"Executed but not seen","text":"<p>(Think hard before hiding stuff from your reader.)</p> <p>For a codeblock that should be 'tested', BUT not seen by the reader, use the <code>.bash</code> with the plain codeblock, and the '.hide-me' style (great for hiding a sleep command that user does not need to run, but CI does): codeblock: <pre><code>``` {.bash .hide-me}\nsleep 10\n```\n</code></pre> as seen by reader: <pre><code>\n</code></pre> </p>"},{"location":"contribution-guidelines/operations/document-management/#seen-but-not-executed","title":"Seen but not executed","text":"<p>(To avoid confusing readers of the HTML, this should be used only for output seen in a shell session.)</p> <p>For a codeblock that should not be 'tested' as part of our CI, use the <code>.bash</code> with the plain codeblock, and without the '.hide-me' style: codeblock: <pre><code>``` {.bash}\nmkdocs server\n```\n</code></pre> as seen by reader: <pre><code>mkdocs server\n</code></pre>"},{"location":"contribution-guidelines/operations/document-management/#seen-but-not-executed-and-no-copy-button","title":"Seen but not executed and no copy button","text":"<p>For a codeblock that should not be 'tested', be seen by the reader, and not include a 'copy' icon (great for output-only instances), use the <code>.bash</code> codeblock without the '.no-copy' style: codeblock: <pre><code>``` {.bash .no-copy}\nI0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\n```\n</code></pre> as seen by reader: <pre><code>I0412 15:15:57.867837   94634 shared_informer.go:282] Waiting for caches to sync for placement-translator\nI0412 15:15:57.969533   94634 shared_informer.go:289] Caches are synced for placement-translator\nI0412 15:15:57.970003   94634 shared_informer.go:282] Waiting for caches to sync for what-resolver\n</code></pre>"},{"location":"contribution-guidelines/operations/document-management/#other-language-specific-highlighting","title":"Other language-specific highlighting","text":"<p>For other language-specific highlighting (yaml, etc.), use the yaml codeblock codeblock: <pre><code>```yaml\nnav:\n  - Home: index.md\n  - QuickStart: Getting-Started/quickstart.md\n  - Contributing: \n      - Guidelines: Contribution guidelines/CONTRIBUTING.md\n```\n</code></pre> as seen by reader: <pre><code>nav:\n  - Home: index.md\n  - QuickStart: Getting-Started/quickstart.md\n  - Contributing: \n      - Guidelines: Contribution guidelines/CONTRIBUTING.md\n</code></pre> </p>"},{"location":"contribution-guidelines/operations/document-management/#codeblock-with-a-title","title":"Codeblock with a title","text":"<p>For a codeblock that has a title, and will not be tested, use the 'title' parameter in conjunction with the plain codeblock (greater for showing or prescribing contents of files): codeblock: <pre><code>``` title=\"testing.sh\"\n#!/bin/sh\necho hello KubeStellar\n```\n</code></pre> as seen by reader: testing.sh<pre><code>#!/bin/sh\necho hello KubeStellar\n</code></pre> </p> <p>(other variations are possible, PR an update to the kubestellar.css file and, once approved, use the style on the plain codeblock in your documentation.)</p>"},{"location":"contribution-guidelines/operations/document-management/#testingrunning-docs","title":"Testing/Running Docs","text":"<p>How do we ensure that our documented examples work?  Simple, we 'execute' our documentation in our CI.  We built automation called 'docs-ecutable' which can be invoked to test any markdown (.md) file in our repository. You could use it in your project as well - afterall it is opensource.</p>"},{"location":"contribution-guidelines/operations/document-management/#the-way-it-works","title":"The way it works:","text":"<ul> <li>create your .md file as you normally would</li> <li>add codeblocks that can be tested, tested but hidden, or not tested at all:<ul> <li>use 'shell' to indicate code you want to be tested</li> <li>use '.bash' with the plain codeblock, and the '.hide-md' style for code you want to be tested, but hidden from the reader (some like this, but its not cool if you want others to run your instructions without hiccups)</li> <li>use plain codeblock (```) if you want to show sample output that is not to be tested</li> </ul> </li> <li>you can use 'include-markdown' blocks, and they will also be executed (or not), depending on the codeblock style you use in the included markdown files.</li> </ul>"},{"location":"contribution-guidelines/operations/document-management/#the-github-workflow","title":"The GitHub Workflow:","text":"<ul> <li>One example of the GitHub Workflow is located in our kubestellar/kubestellar at https://github.com/kubestellar/kubestellar/blob/main/.github/workflows/docs-ecutable-where-resolver.yml</li> <li>An example workflow using the newer technology is located in our kubestellar/kubestellar repo at https://github.com/kubestellar/kubestellar/blob/main/.github/workflows/docs-ecutable-example1.yml</li> </ul>"},{"location":"contribution-guidelines/operations/document-management/#the-original-secret-sauce","title":"The original secret sauce:","text":"<ul> <li>The original code that made all this possible is at https://github.com/kubestellar/kubestellar/blob/main/docs/scripts/docs-ecutable.sh<ul> <li>This code parses the .md file you give it to pull out all the 'shell' and '.bash .hide-me' blocks</li> <li>The code is smart enough to traverse the include-markdown blocks and include the 'shell' and '.bash .hide-me' blocks in them</li> <li>The Jinja constructs are not expanded by this code.</li> <li>It then creates a file called 'generate_script.sh' which is then run at the end of the docs-ecutable execution.</li> </ul> </li> </ul> <p>All of this is invoke in a target in our Makefile <pre><code>.PHONY: docs-ecutable\ndocs-ecutable: \n    MANIFEST=$(MANIFEST) docs/scripts/docs-ecutable.sh\n</code></pre></p> <p>You give the path from that follows the 'https://github.com/kubestellar/kubestellar/docs' path, and name of the .md file you want to 'execute'/'test' as the value for the MANIFEST variable:</p> How to 'make' our docs-ecutable target<pre><code>make MANIFEST=\"'docs/content/Getting-Started/quickstart.md'\" docs-ecutable\n</code></pre> <p>note: there are single and double-quotes used here to avoid issues with 'spaces' used in files names or directories.  Use the single and double-quotes as specified in the quickstart example here.</p>"},{"location":"contribution-guidelines/operations/document-management/#the-new-and-improved-secret-sauce","title":"The new and improved secret sauce:","text":"<ul> <li>The newer code for executing bash snippets in documentation is at https://github.com/kubestellar/kubestellar/blob/main/docs/scripts/execute-html.sh<ul> <li>This code parses the HTML generated by MkDocs to extract all the fenced code blocks tagged for the \"shell\" language.</li> <li>This HTML scraping is relatively easy because it does not have to work on general HTML but only the HTML generated by our stack from our sources. The use of the option setting <code>pygments_lang_class: true</code> for the Python-Markdown extension <code>pymdownx.highlight</code> plays a critical role, getting the source language into the generated HTML.</li> <li>Because it reads the generated HTML, invisible code blocks are not extracted.</li> <li>Because it reads the generated HTML, the Jinja constructs have their usual effects.</li> <li>This script is given the name of the HTML file to read and the current working directory to establish at the start of the extracted bash.</li> <li>It then creates a file called 'generated_script.sh' which is then run.</li> </ul> </li> </ul> <p>All of this is invoked in a target in our Makefile <pre><code>.PHONY: execute-html\nexecute-html: venv\n    . $(VENV)/activate; \\\n    cd docs; \\\n    mkdocs build; \\\n    scripts/execute-html.sh \"$$PWD/..\" \"generated/$(MANIFEST)/index.html\"\n</code></pre></p> <p>The <code>make</code> target requires the variable <code>MANIFEST</code> to be set to the directory that contains the generated <code>index.html</code> file, relative to 'https://github.com/kubestellar/kubestellar/docs/generated'. This is the name of the markdown source file, relative to 'https://github.com/kubestellar/kubestellar/docs/content' and with the <code>.md</code> extension dropped.</p> How to 'make' a docs-ecutable target<pre><code>make MANIFEST=\"Coding Milestones/PoC2023q1/example1\" execute-html\n</code></pre> <p>note: this target has no special needs for quoting --- which is not to deny the quoting that your shell needs.</p>"},{"location":"contribution-guidelines/operations/document-management/#important-files-in-our-gh-pages-branch","title":"Important files in our gh-pages branch","text":""},{"location":"contribution-guidelines/operations/document-management/#indexhtml-and-homehtml","title":"index.html and home.html","text":"<p>These appear in the branch named <code>gh-pages</code> and redirect from the root to the version named <code>latest</code>. The one named <code>index.html</code> is managed by <code>mike set-default</code>. The other should be kept consistent.</p> <ul> <li>https://github.com/kubestellar/kubestellar/blob/gh-pages/home.html</li> <li>https://github.com/kubestellar/kubestellar/blob/gh-pages/index.html</li> </ul> <p>both files have content similar to: index.html and home.html<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;KubeStellar&lt;/title&gt;\n&lt;meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" &gt;\n&lt;meta http-equiv=\"refresh\" content=\"0; URL=https://docs.kubestellar.io/latest\" /&gt;\n&lt;/head&gt;\n</code></pre></p> <p>Do not remove these files!</p>"},{"location":"contribution-guidelines/operations/document-management/#cname","title":"CNAME","text":"<p>The CNAME file has to be in the gh-pages root to allow github to recognize the url tls cert served by our hosting provider.  Do not remove this file!</p> <p>the CNAME file must have the following content in it: CNAME<pre><code>docs.kubestellar.io\n</code></pre></p>"},{"location":"contribution-guidelines/operations/document-management/#versionsjson","title":"versions.json","text":"<p>The versions.json file contains the version and alias information required by 'mike' to properly serve our doc site.  This file is maintained by the 'mike' environment and should not be edited by hand.</p> <pre><code>[{\"version\": \"release-0.22.0\", \"title\": \"release-0.22.0\", \"aliases\": [\"latest\"]}, {\"version\": \"release-0.22.0-rc3\", \"title\": \"release-0.22.0-rc3\", \"aliases\": []}, {\"version\": \"release-0.21.2\", \"title\": \"release-0.21.2\", \"aliases\": []}, {\"version\": \"release-0.21.2-rc1\", \"title\": \"release-0.21.2-rc1\", \"aliases\": []}, {\"version\": \"release-0.21.1\", \"title\": \"release-0.21.1\", \"aliases\": []}, {\"version\": \"release-0.21.0\", \"title\": \"release-0.21.0\", \"aliases\": []}, {\"version\": \"release-0.14\", \"title\": \"release-0.14\", \"aliases\": []}]\n</code></pre>"},{"location":"contribution-guidelines/operations/document-management/#in-case-of-emergency","title":"In case of emergency","text":"<p>If you find yourself in a jam and the pages are not showing up at kubestellar.io or docs.kubestellar.io, check the following 1) Is the index.html, home.html, CNAME, and versions.json file in the gh-pages branch alongside the folders for the compiled documents?  If not, then recreate those files as indicated above (except for versions.json which is programmatically created by 'mike'). 2) Is GitHub settings for 'Pages' for the domain pointing at the https://docs.kubestellar.io url?  If not, paste it in and check off 'enforce https'.  This can happen if the CNAME file goes missing from the gh-pages branch.</p>"},{"location":"contribution-guidelines/operations/document-management/#how-to-recreate-the-gh-pages-branch","title":"How to recreate the gh-pages branch","text":"<p>To recreate the gh-pages branch, do the following: - checkout the gh-pages branch to your local system <pre><code>git clone -b gh-pages https://github.com/kubestellar/kubestellar KubeStellar\ncd KubeStellar\n</code></pre> - delete all files in the branch and push it to GitHub <pre><code>rm -rf *\ngit add; git commit -m \"removing all gh-pages files\"; git push -u origin gh-pages\n</code></pre> - switch to the 'release' branch, switch to /docs and run 'mike deploy' for the release branch. Add the alias 'latest' for the latest release  <pre><code>git checkout release-0.22\ngit pull\nmike deploy --push --rebase --update-aliases release-0.22 latest\n</code></pre> - switch back to the gh-pages branch and recreate the home.html, index.html, and CNAME files as needed (make sure you back out of the docs path first before switching to gh-pages because that path does not exist in that branch) <pre><code>cd ..\ngit checkout gh-pages\ngit pull\nvi index.html\nvi home.html\nvi CNAME\n</code></pre> - push the new files into gh-pages <pre><code>git add .;git commit -m \"add index, home, and CNAME files\";git push -u origin gh-pages\n</code></pre> - go into the GitHub UI and go to the settings for the project and click on 'Pages' to add https://docs.kubestellar.io as the domain and check the box to enforce https.</p> <ul> <li>if the above did not work, then you might have an issue with the GoDaddy domain (expired, files missing, etc.)</li> </ul>"},{"location":"contribution-guidelines/operations/document-management/#how-to-delete-a-rendering-of-a-branch","title":"How to delete a rendering of a branch","text":"<p>Use <code>mike delete $branch_name</code>, either acting locally on your checked out <code>gh-pages</code> branch (after pull and before git commit and push) or acting more directly on the remote repo using <code>--remote</code> and <code>--push</code>. See the mike delete command doc.</p>"},{"location":"contribution-guidelines/operations/document-management/#publishing-workflow","title":"Publishing Workflow","text":"<p>All documentation building and publishing is done using GitHub Actions in <code>.github/workflows/docs-gen-and-push.yml</code>. This workflow is triggered either manually or by a push to a branch named <code>main</code> or <code>release-&lt;something&gt;</code> or <code>doc-&lt;something&gt;</code>. This workflow will actually do something ONLY if either (a) it is acting on the shared GitHub repository at <code>github.com/kubestellar/kubestellar</code> and on behalf of the repository owner or (b) it is acting on a contributor's fork of that repo and on behalf of that same contributor. The published site appears at <code>https://pages.github.io/kubestellar/${branch}</code> in case (a) and at <code>https://${repo_owner}.github.io/${fork_name}/${branch}</code> in case (b). This workflow will build and publish a website version whose name is the same as the name of the branch that it is working on. This workflow will also update the relevant <code>mike</code> alias, if necessary.</p> <p>*Note that this particular page is rendered from the README.md file in the kubestellar/docs folder, but it is served here on the website at via an include directive</p>"},{"location":"contribution-guidelines/operations/testing-doc-prs/","title":"Testing a KubeStellar documentation PR","text":"<p>If a contributor has not created a sharable preview of a documentation PR as documented in the documents management overview , here are the steps to checkout a git pull request for local testing.</p>"},{"location":"contribution-guidelines/operations/testing-doc-prs/#step-1-checkout-the-pull-request","title":"STEP 1: Checkout the Pull Request**","text":"<p>Helpers: GitHub, DevOpsCube</p> <p>Following is one approach to checking out the branch that a PR asks to merge. Alternatively you could use any other technique that accomplishes the same thing.</p>"},{"location":"contribution-guidelines/operations/testing-doc-prs/#11-use-git-fetch-to-get-a-local-copy-of-the-prs-branch-note-be-sure-to-check-out-the-right-pr","title":"1.1 Use <code>git fetch</code> to get a local copy of the PR's branch (note: be sure to check out the right PR!)","text":"<p>Fetch the reference to the pull request based on its ID number, creating a new branch locally. Replace ID with your PR # and BRANCH_NAME with the desired branch name. The branch name will be used only in your local workspace; you can pick anything you like.</p> <p>The following command assumes that your local workspace has a \"git remote\" named \"upstream\" that refers to the shared repository at <code>github.com/kubestellar/kubestellar</code>.</p> <pre><code>git fetch upstream pull/ID/head:BRANCH_NAME\n</code></pre>"},{"location":"contribution-guidelines/operations/testing-doc-prs/#12-switch-to-the-new-branch","title":"1.2 Switch to the new branch","text":"<p>Checkout the BRANCH_NAME where you have all the changes from the pull request.</p> <pre><code>git checkout BRANCH_NAME\n</code></pre> <p>At this point, you can do anything you want with this branch. You can run some local tests, or merge other branches into the branch.</p>"},{"location":"contribution-guidelines/operations/testing-doc-prs/#step-2-test-and-build-the-documentation-optional","title":"STEP 2: Test and Build the Documentation (optional)**","text":"<p>See Serving up documents locally for how to view and modify the documentation in the branch that you have checked out.</p>"},{"location":"contribution-guidelines/security/security-inc/","title":"Policy","text":""},{"location":"contribution-guidelines/security/security-inc/#security-announcements","title":"Security Announcements","text":"<p>Join the kubestellar-security-announce group for emails about security and major API announcements.</p>"},{"location":"contribution-guidelines/security/security-inc/#report-a-vulnerability","title":"Report a Vulnerability","text":"<p>We're extremely grateful for security researchers and users that report vulnerabilities to the KubeStellar Open Source Community. All reports are thoroughly investigated by a set of community volunteers.</p> <p>You can also email the private kubestellar-security-announce@googlegroups.com list with the security details and the details expected for all KubeStellar bug reports.</p>"},{"location":"contribution-guidelines/security/security-inc/#when-should-i-report-a-vulnerability","title":"When Should I Report a Vulnerability?","text":"<ul> <li>You think you discovered a potential security vulnerability in KubeStellar</li> <li>You are unsure how a vulnerability affects KubeStellar</li> <li>You think you discovered a vulnerability in another project that KubeStellar depends on</li> <li>For projects with their own vulnerability reporting and disclosure process, please report it directly there</li> </ul>"},{"location":"contribution-guidelines/security/security-inc/#when-should-i-not-report-a-vulnerability","title":"When Should I NOT Report a Vulnerability?","text":"<ul> <li>You need help tuning KubeStellar components for security</li> <li>You need help applying security related updates</li> <li>Your issue is not security related</li> </ul>"},{"location":"contribution-guidelines/security/security-inc/#security-vulnerability-response","title":"Security Vulnerability Response","text":"<p>Each report is acknowledged and analyzed by the maintainers of KubeStellar within 3 working days.</p> <p>Any vulnerability information shared with Security Response Committee stays within KubeStellar project and will not be disseminated to other projects unless it is necessary to get the issue fixed.</p> <p>As the security issue moves from triage, to identified fix, to release planning we will keep the reporter updated.</p>"},{"location":"contribution-guidelines/security/security-inc/#public-disclosure-timing","title":"Public Disclosure Timing","text":"<p>A public disclosure date is negotiated by the KubeStellar Security Response Committee and the bug submitter. We prefer to fully disclose the bug as soon as possible once a user mitigation is available. It is reasonable to delay disclosure when the bug or the fix is not yet fully understood, the solution is not well-tested, or for vendor coordination. The timeframe for disclosure is from immediate (especially if it's already publicly known) to a few weeks. For a vulnerability with a straightforward mitigation, we expect report date to disclosure date to be on the order of 7 days. The KubeStellar maintainers hold the final say when setting a disclosure date.</p>"},{"location":"contribution-guidelines/security/security_contacts-inc/","title":"Contacts","text":"<p>Defined below are the security contacts for this repo.</p> <p>They are the contact point for the Product Security Committee to reach out to for triaging and handling of incoming issues.</p> <p>The below names agree to address security concerns if and when they arise.</p> <p>DO NOT REPORT SECURITY VULNERABILITIES DIRECTLY TO THESE NAMES, SEND INFORMATION TO kubestellar-security-announce@googlegroups.com</p> <p>clubanderson MikeSpreitzer ezrasilvera pdettori</p>"},{"location":"direct/acquire-hosting-cluster/","title":"A cluster for KubeFlex hosting","text":"<p>This document tells you what makes a Kubernetes cluster suitable to serve as the KubeFlex hosting cluster and shows some ways to create such a cluster.</p>"},{"location":"direct/acquire-hosting-cluster/#requirements-on-the-kubeflex-hosting-cluster","title":"Requirements on the KubeFlex hosting cluster","text":"<p>The KubeFlex hosting cluster needs to run an Ingress controller with SSL passthrough enabled.</p>"},{"location":"direct/acquire-hosting-cluster/#connectivity-from-clients","title":"Connectivity from clients","text":"<p>The clients in KubeStellar need to be able to open a TCP connection to where the Ingress controller is listening for HTTPS connections.</p> <p>The clients in KubeStellar comprise the following.</p> <ul> <li>The OCM Agent and the OCM Status Add-On Agent in each WEC.</li> <li>The KubeStellar controller-manager and the transport controller for each WDS, running in the KubeFlex hosting cluster.</li> </ul> <p>TODO: finish writing this subsection for real. Following are some clues.</p> <p>When everything runs on one machine, the defaults just work. When core and some WECs are on different machines, it gets more challenging. When the KubeFlex hosting cluster is an OpenShift cluster with a public domain name, the defaults just work.</p> <p>After the Getting Started setup, I looked at an OCM Agent (klusterlet-agent, to be specific) and did not find a clear passing of kubeconfig. I found adjacent Secrets holding kubeconfigs in which <code>cluster[0].cluster.server</code> was <code>https://kubeflex-control-plane:31048</code>. Note that <code>kubeflex-control-plane</code> is the name of the Docker container running <code>kind</code> cluster serving as KubeFlex hosting cluster. I could not find an explanation for the port number 31048; that Docker container maps port 443 inside to 9443 on the outside.</p> <p><code>kflex init</code> takes a command line flag <code>--domain string</code> described as <code>domain for FQDN (default \"localtest.me\")</code>.</p>"},{"location":"direct/acquire-hosting-cluster/#creating-a-hosting-cluster","title":"Creating a hosting cluster","text":"<p>Following are some ways to create a Kubernetes cluster that is suitable to use as a KubeFlex hosting cluster. This is not an exhaustive list.</p>"},{"location":"direct/acquire-hosting-cluster/#create-and-init-a-kind-cluster-as-hosting-cluster-with-kflex","title":"Create and init a kind cluster as hosting cluster with kflex","text":"<p>The following command will use <code>kind</code> to create a cluster with an Ingress controller with SSL passthrough AND ALSO proceed to install the KubeFlex implementation in it and set your current kubeconfig context to access that cluster as admin.</p> <pre><code>kflex init --create-kind\n</code></pre>"},{"location":"direct/acquire-hosting-cluster/#create-and-init-a-kind-cluster-as-hosting-cluster-with-curl-to-bash-script","title":"Create and init a kind cluster as hosting cluster with curl-to-bash script","text":"<p>There is a bash script at <code>https://raw.githubusercontent.com/kubestellar/kubestellar/v0.28.0/scripts/create-kind-cluster-with-SSL-passthrough.sh</code> that can be fed directly into <code>bash</code> and will create a <code>kind</code> cluster AND ALSO initialize it as the KubeFlex hosting cluster. This script accepts the following command line flags.</p> <ul> <li><code>--name name</code>: set a specific name of the kind cluster (default: kubestellar).</li> <li><code>--port port</code>: map the specified host port to the kind cluster port 443 (default: 9443).</li> <li><code>--nowait</code>: when given, the script proceeds without waiting for the nginx ingress patching to complete.</li> <li><code>--nosetcontext</code>: when given, the script does not change the current kubectl context to the newly created cluster.</li> <li><code>-X</code> enable verbose execution of the script for debugging.</li> </ul>"},{"location":"direct/acquire-hosting-cluster/#create-a-k3d-cluster","title":"Create a k3d cluster","text":"<p>This has been tested with version 5.6.0 of k3d.</p> <ol> <li> <p>Create a K3D hosting cluster with nginx ingress controller:     <pre><code>k3d cluster create -p \"9443:443@loadbalancer\" --k3s-arg \"--disable=traefik@server:*\" kubeflex\nhelm install ingress-nginx ingress-nginx --repo https://kubernetes.github.io/ingress-nginx --version 4.12.1 --namespace ingress-nginx --create-namespace\n</code></pre></p> </li> <li> <p>When we use kind, the name of the container is kubeflex-control-plane and that is what we use     in the internal URL for <code>--force-internal-endpoint-lookup</code>.    Here the name of the container created by K3D is <code>k3d-kubeflex-server-0</code> so we rename it:     <pre><code>docker stop k3d-kubeflex-server-0\ndocker rename k3d-kubeflex-server-0 kubeflex-control-plane\ndocker start kubeflex-control-plane\n</code></pre>     Wait 1-2 minutes for all pods to be restarted.     Use the following command to confirm all are fully running:     <pre><code>kubectl --context k3d-kubeflex get po -A\n</code></pre></p> </li> <li> <p>Enable SSL passthrough:    We are using nginx ingress with tls passthrough.    The current install for kubeflex installs also nginx ingress but specifically for kind.    To specify passthrough for K3D, edit the ingress placement controller with the following command and add <code>--enable-ssl-passthrough</code> to the list of arguments for the container     <pre><code>kubectl edit deployment ingress-nginx-controller -n ingress-nginx  \n</code></pre></p> </li> </ol>"},{"location":"direct/architecture/","title":"KubeStellar Architecture","text":"<p>KubeStellar provides multi-cluster deployment of Kubernetes objects, controlled by simple <code>BindingPolicy</code> objects, where Kubernetes objects are expressed in their native format with no wrapping or bundling. The high-level architecture for KubeStellar is illustrated in Figure 1.</p> <p> Figure 1 - High Level Architecture </p> <p>KubeStellar relies on the concept of spaces. A Space is an abstraction to represent an API service that  behaves like a Kubernetes kube-apiserver (including the persistent storage behind it)  and the subset of controllers in the kube-controller-manager that are concerned with  API machinery generalities (not management of containerized workloads).  A KubeFlex <code>ControlPlane</code> is an example. A regular Kubernetes cluster is another example. Users can use spaces to perform these tasks:</p> <ol> <li>Create Workload Definition Spaces (WDSes) to store the definitions of their workloads. A Kubernetes workload is an application that runs on Kubernetes. A workload can be made by a  single Kubernetes object or several objects that work together.</li> <li>Create Inventory and Transport Spaces (ITSes) to manage the inventory of clusters and  the transport of workloads.</li> <li>Register and label Workload Execution Clusters (WECs) with the Inventory and  Transport Space, to keep track of the available clusters and their characteristics.</li> <li>Define <code>BindingPolicy</code> to specify what objects and where should be  deployed on the WECs.</li> <li>Submit objects in the native Kubernetes format to the WDSes,  and let the <code>BindingPolicy</code> govern which WECs should receive them.</li> <li>Check the status of submitted objects from the WDS.</li> </ol> <p>In KubeStellar, users can assume a variety of roles and responsibilities.  These roles could range from system administrators and application owners  to CISOs and DevOps Engineers. However, for the purpose of this document,  we will not differentiate between these roles. Instead we will use the term  'user' broadly, without attempting to make distinctions among roles.</p> <p>Examples of user interactions with KubeStellar are illustrated in the KubeStellar Usage Example Scenarios document.</p> <p>The KubeStellar architecture has the following main modules.</p> <ul> <li> <p>KubeFlex. KubeStellar builds on the services of KubeFlex, using it to keep track of, and possibly provide, the Inventory and Transport spaces and the Workload Description spaces. Each of those appears as a <code>ControlPlane</code> object in the KubeFlex hosting cluster.</p> </li> <li> <p>KubeStellar Controller Manager: this module is instantiated once per WDS and is responsible for watching <code>BindingPolicy</code> objects and create from it a matching <code>Binding</code> object that contains list of references to the concrete objects and list of references to the concrete clusters, and for returning reported state from the ITS into the WDS.</p> </li> <li> <p>Pluggable Transport Controller: this module is instantiated once per WDS and is responsible for projecting KubeStellar workload and control objects of the WDS into OCM workload/control objects in the ITS.</p> </li> <li> <p>Space Manager: This module manages the lifecycle of spaces.</p> </li> <li> <p>OCM Cluster Manager: This module is instantiated once per ITS and syncs objects from that ITS to the Workload Execution  Clusters (WECs). In the ITS, each mailbox namespace is associated with one WEC. Objects  that are put in a mailbox namespace are delivered to the matching WEC.</p> </li> <li> <p>OCM Agent: This module registers the WEC to the OCM Hub, watches for  ManifestWork.v1.work.open-cluster-management.io objects and unwraps and syncs the objects into the WEC.</p> </li> <li> <p>OCM Status Add-On Controller: This module is instantiated once per ITS and uses the OCM Add-on Framework to get the OCM Status Add-On Agent installed in each WEC along with supporting RBAC objects.</p> </li> <li> <p>OCM Status Add-On Agent: This module watches AppliedManifestWork.v1.work.open-cluster-management.io objects  to find objects that are synced by the OCM agent, gets their status  and updates <code>WorkStatus</code> objects in the ITS namespace associated with the WEC.</p> </li> </ul> <p> Figure 2 - Main Modules </p>"},{"location":"direct/architecture/#kubestellar-controller-manager","title":"KubeStellar Controller Manager","text":"<p>This module manages the binding controller and the status controller. </p> <ul> <li> <p>The binding controller watches <code>BindingPolicy</code> and workload objects on the Workload Definition Space (WDS), and maintains a <code>Binding</code> object for each <code>BindingPolicy</code> in the WDS. A <code>Binding</code> object contains (a) the concrete list of references to workload objects (and associated modulations on downsync behavior) and (b) the concrete list of clusters that were selected by the <code>BindingPolicy</code> selectors.</p> </li> <li> <p>The status controller watches for WorkStatus objects on the ITS   and, based on the instructions in the <code>BindingPolicy</code> and   <code>StatusCollector</code> objects, returns reported state into the WDS in   the two defined ways.</p> </li> </ul> <p>There is one instance of a KubeStellar Controller Manager for each WDS.  Currently this controller-manager runs in the KubeFlex hosting cluster and is responsible for installing the required  CRDs in the associated WDS. More details on the internals of this module are provided in KubeStellar Controllers Architecture.</p>"},{"location":"direct/architecture/#pluggable-transport-controller","title":"Pluggable Transport Controller","text":"<p>This controller's job is to (possibly through delegating some responsibilities): (a) get workload objects from WDS to WECs as prescribed by the <code>Binding</code> objects and their referenced <code>CustomTransform</code> objects and inventory objects and (b) get corresponding reported state back into <code>WorkStatus</code> objects in the ITS.</p> <p>Different implementations of this controller are possible; it would be possible to enable even more different implementations by taking a more general approach to inventory.</p> <p>The implementations need not be in this Git repository. Currently there is one implementation, and it is in this repository. This implementation uses Open Cluster Management. The OCM Status Add-On Controller and Agent are part of the way this transport controller gets its job done.</p> <p>The OCM (based) Transport Controller maintains, in the ITS, a set of <code>ManifestWork</code> objects that constitute an OCM representation of what is requested by the KubeStellar workload and control objects in the WDS. Based on the associations in the <code>Binding</code> objects, this transport controller bundles workload objects from the WDS into <code>ManifestWork</code> objects in the ITS. The bundling is controllable, with configured limits on both the number of objects in a bundle and the size of the <code>ManifestWorkSpec</code>.</p> <p>There is one instance of the pluggable transport controller for each WDS, managed according to a <code>Deployment</code> object in the KubeFlex hosting cluster.  More details on the internals of this module are provided in KubeStellar Controllers Architecture.</p>"},{"location":"direct/architecture/#space-manager","title":"Space Manager","text":"<p>The Space Manager handles the lifecycle of spaces.  KubeStellar uses the KubeFlex project for space management. In KubeFlex, a space is named a <code>ControlPlane</code>, and we will use  both terms in this document. KubeStellar currently prereqs KubeFlex to  provide one or more spaces. We plan to make this optional in the near future.</p> <p>KubeFlex is a flexible framework that supports various kinds of control planes, such as k8s, a basic Kubernetes API Server with a subset of kube controllers, and  vcluster: a virtual cluster that runs on the hosting cluster based on the vCluster Project. More detailed information on the different types of control planes and architecture are described in the KubeFlex Architecture.</p> <p>There are currently two roles for spaces managed by KubeFlex: Inventory and Transport Space  (ITS) and Workload Description Space (WDS). The former runs the OCM Cluster Manager on a vcluster-type control plane, and the latter runs on a k8s-type control plane.</p> <p>An ITS holds the inventory and the mailbox namespaces. The inventory is anchored by ManagedCluster.v1.cluster.open-cluster-management.io objects that describe the WECs. For each WEC there may also be a <code>ConfigMap</code> object (in the <code>customization-properties</code> namespace) that carries additional properties of that WEC; this <code>ConfigMap</code> is used in customizing the workload to the WEC. The mailbox namespaces and their contents are transport implementation details that users do not need to deal with. Each mailbox namespace corresponds 1:1 with a WEC and holds <code>ManifestWork</code> objects managed by the central KubeStellar controllers.</p> <p>A WDS holds user workload objects and the user's objects that form the interface to KubeStellar control.  Currently, the user control objects are <code>BindingPolicy</code> and <code>Binding</code> objects. Future development may define more kinds of control objects hosted in the WDS.</p> <p>KubeFlex provides the ability to start controllers connected to a Control Plane API Server or to deploy Helm Charts into a Control Plane API server with post-create hooks. This feature is currently adopted for KubeStellar modules startup, as it allows to create a Workload Description Space (WDS) and start the KubeStellar Controller Manager, and create an Inventory and Transport Space (ITS) in a <code>vcluster</code> and install the Open Cluster Management Hub there.</p>"},{"location":"direct/architecture/#ocm-cluster-manager","title":"OCM Cluster Manager","text":"<p>This module is based on the Open Cluster Management Project, a community-driven project that focuses on multicluster and multicloud scenarios for Kubernetes apps.  It provides APIs for cluster registration, work distribution and much more.  The project is based on a hub-spoke architecture, where a single hub cluster  handles the distribution of workloads through manifests, and one or more spoke clusters  receive and apply the workload objects from the manifests. In Open Cluster Management, spoke clusters  are called managed clusters, and the component running on the hub cluster is the cluster manager. Manifests provide a summary for the status of each object, however in some use  cases this might not be sufficient as the full status for objects may be required.  OCM provides an add-on framework that allows to automatically install additional  agents on the managed clusters to provide specific features. This framework is used to install the status add-on on all managed clusters. KubeStellar currently exposes users directly to OCM inventory management and WEC registration.</p>"},{"location":"direct/architecture/#ocm-agent","title":"OCM Agent","text":"<p>The OCM Agent Module (a.k.a klusterlet) has two main controllers: the registration agent and the work agent. </p> <p>The registration agent is responsible for registering  a new cluster into OCM. The agent creates an unaccepted ManagedCluster into  the hub cluster along with a temporary CertificateSigningRequest.v1.certificates (CSR) object.  The cluster will be accepted by the hub control plane if the CSR is approved and  signed by any certificate provider setting filling <code>.status.certificate</code> with legit  X.509 certificates, and the ManagedCluster resource is approved by setting  <code>.spec.hubAcceptsClient</code> to true in the spec. Upon approval, the registration  agent observes the signed certificate and persists them as a local secret  named <code>hub-kubeconfig-secret</code> (by default in the <code>open-cluster-management-agent</code> namespace)  which will be mounted to the other fundamental components of klusterlet such as  the work agent. The registration process in OCM is called double opt-in mechanism,  which means that a successful cluster registration requires both sides of approval  and commitment from the hub cluster and the managed cluster.</p> <p>The work agent monitors the <code>ManifestWork</code> resource in the cluster namespace  on the hub cluster. The work agent tracks all the resources defined in ManifestWork  and updates its status. There are two types of status in ManifestWork: the resourceStatus  tracks the status of each manifest in the ManifestWork, and conditions reflects the overall  status of the ManifestWork. The work agent checks whether a resource is Available,  meaning the resource exists on the managed cluster, and Applied, meaning the resource  defined in ManifestWork has been applied to the managed cluster. To ensure the resources  applied by ManifestWork are reliably recorded, the work agent creates an <code>AppliedManifestWork</code>  on the managed cluster for each ManifestWork as an anchor for resources relating to ManifestWork.  When ManifestWork is deleted, the work agent runs a Foreground deletion, and that ManifestWork  will stay in deleting state until all its related resources have been fully cleaned in the managed  cluster.</p>"},{"location":"direct/architecture/#ocm-status-add-on-controller","title":"OCM Status Add-On Controller","text":"<p>This module automates the installation of the OCM status add-on agent  on all managed clusters. It is based on the  OCM Add-on Framework,  which is a framework that helps developers to develop extensions  for working with multiple clusters in custom cases. A module based on  the add-on framework has two components: a controller and an  agent. The controller interacts with the add-on manager to register  the add-on, manage the distribution of the add-on to all clusters, and set  up the RBAC permissions required by the add-on agent to interact with the mailbox  namespace associated with the managed cluster. More specifically, the status  add-on controller sets up RBAC permissions to allow the add-on agent to  list and get <code>ManifestWork</code> objects and create and update WorkStatus objects.</p>"},{"location":"direct/architecture/#ocm-status-add-on-agent","title":"OCM Status Add-On Agent","text":"<p>The OCM Status Add-On Agent is a controller that runs alongside the OCM Agent  in the managed cluster. Its primary function is to track objects delivered  by the work agent and report the full status of those objects back to the ITS.  Other KubeStellar controller(s) then propagate and/or summarize that status information into the WDS. The OCM Status Add-On Agent watches AppliedManifestWork.v1.work.open-cluster-management.io objects in the WEC to observe the status reported there by the OCM Agent. Each <code>AppliedManifestWork</code> object is specific to one workload object, and holds both the local (in the WEC) status from that object and a reference to that object. For each <code>AppliedManifest</code>, the OCM Status Add-On Agent maintains a corresponding <code>WorkStatus</code> object in the relevant mailbox namespace in the ITS. Such a <code>WorkStatus</code> object also is about exactly one workload object, so that status updates for one object do not require updates of a whole bundle. A <code>WorkStatus</code> object holds the status of a workload object and a reference to that object. </p> <p>Installing the Status Add-On Agent in the WEC causes status to be returned to <code>WorkStatus</code> objects for all downsynced objects.</p>"},{"location":"direct/architecture/#kubestellar-controllers-architecture","title":"KubeStellar Controllers Architecture","text":"<p>The KubeStellar controllers architecture is based on common patterns and best  practices for Kubernetes controllers, such as the  Kubernetes Sample Controller.  A Kubernetes controller uses informers to watch for changes in Kubernetes objects, caches to store the objects, event handlers to react to events, work queues for parallel processing of tasks, and a reconciler to ensure the actual state matches the desired state. However, that pattern has been extended to provide the following features:</p> <ul> <li>Using dynamic informers for workload objects</li> <li>Starting informers on all API Resources (except some that do not need   watching)</li> <li>Workload Informers and Listers are maintained in a hash map that is   indexed by GVR (Group, Version, Resource) of the watched objects.</li> <li>Using a common work queue and set of workers, multiplexing multiple types of object references into that queue.<ul> <li>A reference to a workload object carries its API Group, Version, Resource, and Kind. No need for a <code>RESMapper</code>, the \"Kind\" and \"Resource\" are learned together from the API discovery process.</li> </ul> </li> <li>Starting &amp; stopping informers dynamically based on creation or   deletion of CRDs (which add/remove APIs on the WDS).</li> <li>One client connected to the WDS space and one (or more in the future)   to connect to one or more OCM shards.<ul> <li>The WDS-connected client is used to start the dynamic   informers/listers for workload and control objects in the WDS</li> <li>The OCM-connected client is used to start informers/listers for OCM   ManagedClusters and to copy/update/remove the wrapped objects   into/from the OCM mailbox namespaces.</li> </ul> </li> </ul> <p>There are two controllers in the KubeStellar controller manager:</p> <ul> <li> <p>Binding Controller - one client connected to the WDS and one   (or more in the future) to connect to one or more ITS shards.</p> <ul> <li> <p>The WDS-connected client is used to start the dynamic   informers/listers for workload objects and KubeStellar control   objects in the WDS.</p> </li> <li> <p>The OCM-connected client is used to start informers/listers for   OCM ManagedClusters. This is a temporary state until cluster   inventory abstraction is implemented and decoupled from OCM (and   then this client should be removed and we would need to use   client to inventory space).</p> </li> <li> <p>This controller maintains an internal data structure called the   <code>BindingPolicyResover</code> that tracks what <code>Binding</code> should   correspond to each <code>BindingPolicy</code>, and uses it to make that so.</p> </li> </ul> </li> <li> <p>Status controller - one client connected to the WDS and one   connected to the ITS; also uses informer-like services from the   Binding Controller, regarding workload objects and   BindingPolicies. The Status Controller gets reported state from the   ITS back to the WDS, in the two supported   ways: combining reported state from multiple   WECs to a query result object, and copying status from a single WEC   to the original workload object.</p> </li> </ul> <p>There is also a separate Transport Controller. This also has a WDS-connected client, used to monitor workload and control objects, and an ITS-connected-client, used to monitor and create/update/delete <code>ManifestWork</code> objects.</p>"},{"location":"direct/architecture/#binding-controller","title":"Binding Controller","text":"<p>The Binding controller is responsible for watching workload objects and <code>BindingPolicy</code> objects, and maintains for each of the latter a matching <code>Binding</code> object in the WDS.  A <code>Binding</code> object is mapped 1:1 to a <code>BindingPolicy</code> object and contains the concrete list of references to workload objects and the concrete list of references to inventory objects that were selected by the policy.</p> <p>The Binding Controller is centered on its workqueue and an internal data structure, called a <code>BindingPolicyResover</code>, that represents the set of <code>Binding</code> objects that should exist based on the controller's inputs. The controller has informers for all of its inputs: a static collection for the control objects (<code>BindingPolicy</code> and inventory objects) and a dynamic collection (based on continual API discovery) for the workload objects. The controller also has informers for its output objects (i.e., <code>Binding</code> objects). Every notification from an informer is handled by putting a relevant object reference into the work queue. Working on a reference to an input involves updating the <code>BindingPolicyResover</code> and enqueuing a reference to any output object (<code>Binding</code>) that might need a change. Working on a reference to a <code>Binding</code> involves comparing what is actually in that <code>Binding</code> with what the <code>BindingPolicyResover</code> says should be there, and creating/updating/deleting the <code>Binding</code> if there is a difference.</p> <p>The Binding Controller also provides two informer-like services that the Status Controller uses. One is notifying about any change to that internal data structure, and the ability to read from it. The other is notifying about workload object events.</p> <p>The architecture and the event flow of the code for create/update object events is illustrated in Figure 3 (some details are omitted to make the flow easier to understand).</p> <p> Figure 3 - Binding Controller </p> <p>At startup, the controller code sets up the dynamic informers, the event handler and the work queue as follows:</p> <ul> <li>lists all API preferred resources (using discovery client's <code>ServerPreferredResources()</code>   to return only one preferred storage version for API group)</li> <li>Filters out some resources</li> <li>For each resource:<ul> <li>Creates GVR key</li> <li>Registers Event Handler</li> <li>Starts Informer</li> <li>Stores informer and lister in a map indexed by GVR</li> </ul> </li> <li>Waits for all caches to sync</li> <li>Gets the list of all <code>BindingPolicy</code> objects and, for each one, invokes the <code>BindingPolicyResover</code> method for the presence of the <code>BindingPolicy</code>.</li> <li>Starts N workers to process work queue</li> </ul> <p>The informer and watches specific resources on the WDS API Server; on create/update/delete object events it puts a copy of the object into the informer's local cache, which is what the lister reads. The informer invokes the event handler. The handler implements the event handling functions (<code>AddFunc</code>, <code>UpdateFunc</code>, <code>DeleteFunc</code>)</p>"},{"location":"direct/architecture/#sync-bindingpolicy","title":"Sync BindingPolicy","text":"<p>For a <code>BindingPolicy</code> that is deleted or being deleted, sync consists of the following steps.</p> <ol> <li>Ensure the absence of the KubeStellar finalizer on the <code>BindingPolicy</code>.</li> <li>Invoke the <code>BindingPolicyResover</code> method for the absence of the <code>BindingPolicy</code>.</li> </ol> <p>For a <code>BindingPolicy</code> that is neither deleted nor being deleted, sync consists of the following steps.</p> <ol> <li>Ensure the presence of the KubeStellar finalizer on the <code>BindingPolicy</code>.</li> <li>Invoke the <code>BindingPolicyResover</code> method for the presence of the <code>BindingPolicy</code>.</li> <li>Find all the WECs (which are represented by inventory objects) that match the <code>BindingPolicy</code>.</li> <li>Invoke the <code>BindingPolicyResover</code> method that associates a BindingPolicy's name with its current set of matching WECs.</li> <li>Enqueue a reference to every workload object.</li> </ol>"},{"location":"direct/architecture/#sync-workload-object","title":"Sync Workload Object","text":"<p>If the workload object is a CRD then, in addition to the steps below, the controller makes the corresponding change in the results of API discovery.</p> <p>If the workload object is being deleted then the controller invokes the <code>BindingPolicyResolver</code> method that handles with the non-existence of an object; this completes sync in this case.</p> <p>Otherwise the controller proceeds as follows, independently for each <code>BindingPolicy</code> that exists in the informer's local cache and the <code>BindingPolicyResolver</code> is aware of.</p> <ol> <li> <p>The workload object is tested against the downsync policy clauses    of the <code>BindingPolicy</code> and results accumulated.</p> </li> <li> <p>The controller calls the <code>BindingPolicyResolver</code> method that copes    with the accumulated results.</p> </li> <li> <p>If the resolver reported that this made a difference then the    controller enqueues a reference to the corresponding <code>Binding</code>    object.</p> </li> </ol>"},{"location":"direct/architecture/#sync-binding","title":"Sync Binding","text":"<p>If the <code>BindingPolicyResover</code> is unaware of the existence of a corresponding <code>BindingPolicy</code> then almost nothing needs to be done: the <code>BindingPolicy</code> is either being created or deleted and there will be more syncing done due to other events. All that need be done here and now is have the resolver notify its registered handlers (which are from the Status Controller) for <code>BindingPolicy</code> events.</p> <p>If the corresponding <code>BindingPolicy</code> object does not exist, then nothing more is done.</p> <p>In the remaining cases, the controller takes the following steps.</p> <ol> <li> <p>The controller generates the proper <code>BindingSpec</code> from the    information in the <code>BindingPolicyResover</code>. The controller compares    that with the <code>BindingSpec</code> (if any) from the <code>Binding</code> lister. If    there is a difference then the controller updates the <code>Binding</code>    object and has the resolver notify the registered handlers for    <code>BindingPolicy</code> events. When creating a <code>Binding</code> object, the    controller sets the corresponding <code>BindingPolicyObject</code> as a    controlling owner in the object metadata.</p> </li> <li> <p>The controller writes the <code>.status</code> of the corresponding    <code>BindingPolicy</code>.  This includes propagating the errors from the    <code>.status.errors</code> of the <code>Binding</code> and adding reports of invalid    requests for singleton reported state return (requests where the    object is not distributed to exactly 1 WEC).</p> </li> </ol>"},{"location":"direct/architecture/#status-controller","title":"Status Controller","text":"<p>The status controller implements the last stage of reported state propagation, from the ITS into the WDS. This includes both singleton reported state return into the <code>.status</code> section of workload objects and programmed aggregation into <code>CombinedStatus</code> objects.</p> <p>The <code>WorkStatus</code> objects are created, updated, and deleted in the ITS by the chosen transport. For the OCM transport, that is the OCM Status Add-On Agent described above.</p> <p>The status controller has informers for its unique inputs, which are <code>StatusCollector</code> objects in the WDS and <code>WorkStatus</code> objects in the ITS. The status controller also gets informer-like services from the binding controller: getting notified of and being able to read the current state resulting from (a) workload object create/update/delete, (b) change in an intended <code>Binding</code>, and (c) change in whether singleton reported state return is requested for a workload object. The status controller also has informers for its unique outputs, which are the <code>CombinedStatus</code> objects.</p> <p>The high-level flow for the singleton status update is described in Figure 4.</p> <p> Figure 4 - Status Controller </p>"},{"location":"direct/architecture/#transport-controller","title":"Transport Controller","text":"<p>The transport controller is pluggable and allows the option to plug different implementations of the transport interface. The interface between the plugin and the generic code is a Go language interface (in <code>pkg/transport/transport.go</code>) that the plugin has to implement. This interface requires the following from the plugin.</p> <ul> <li>Upon registration of a new WEC, plugin should create a namespace for the WEC in the ITS and delete the namespace once the WEC registration goes away (mailbox namespace per WEC);</li> <li>Plugin must be able to wrap any number of objects into a single wrapped object;</li> <li>Have an agent that can be used to pull the wrapped objects from the mailbox namespace and apply them to the WEC. A single example for such an agent is an agent that runs on the WEC and watches the wrapped object in the corresponding namespace in the central hub and is able to unwrap it and apply the objects to the WEC. </li> <li>Have inventory representation for the clusters.</li> </ul> <p>The above list is required in order to comply with SIG Multi-Cluster Work API.</p> <p>Each plugin has an executable with a <code>main</code> function that calls the generic code (in <code>pkg/transport/cmd/generic-main.go</code>), passing the plugin object that implements the plugin interface. The generic code does the rule-based customization; the plugin is given customized objects. The generic code also ensures that the namespace named \"customization-properties\" exists in the ITS.</p> <p>KubeStellar currently has one transport plugin implementation which is based on CNCF Sandbox project Open Cluster Management. OCM transport plugin implements the above interface and supplies a function to start the transport controller using the specific OCM implementation. Code is available here. We expect to have more transport plugin options in the future.</p> <p>The following section describes how transport controller works, while the described behavior remains the same no matter which transport plugin is selected. The high level flow for the transport controller is described in Figure 5.</p> <p> Figure 5 - Transport Controller </p> <p>The transport controller is driven by <code>Binding</code> objects in the WDS. There is a 1:1 correspondence between <code>Binding</code> objects and <code>BindingPolicy</code> objects, but the transport controller does not care about the latter. A <code>Binding</code> object contains (a) a list of references to workload objects that are selected for distribution and (b) a list of references to the destinations for those workload objects.</p> <p>The transport controller watches for <code>Binding</code> objects on the WDS, using an informer. Upon every add, update, and delete event from that informer, the controller puts a reference to that <code>Binding</code> object in its work queue. The transport controller also has informers on the inventory objects (both <code>ManagedCluster</code> and their associated <code>ConfigMap</code>) and on the wrapped objects (<code>ManifestWork</code>). Forked goroutines process items from the work queue. For a reference to a control or workload object, that processing starts with retrieving the informer's cached copy of that object. </p> <p>The transport controller also maintains a finalizer on each Binding object. When processing a reference to a <code>Binding</code> object that no longer exists, the transport controller has nothing more to do (because it processes the deletion before removing its finalizer).</p> <p>When processing a reference to a <code>Binding</code> object that still exists, the transport controller looks at whether that <code>Binding</code> is in the process of being deleted. If so then the controller ensures that the corresponding wrapped object (<code>ManifestWork</code>) in the ITS no longer exists and then removes the finalizer from the <code>Binding</code>.</p> <p>When processing a <code>Binding</code> object that is not being deleted, the transport controller first ensures that the finalizer is on that object. Then the controller constructs an internal function from destination to the customized wrapped object for that destination. The controller then iterates over the <code>Binding</code>'s list of destinations and propagates the corresponding wrapped object (reported by the function just described) to the corresponding mailbox namespace.  Once the wrapped object is in the mailbox namespace of a cluster on the ITS, it's the agent responsibility to pull the wrapped object from there and apply/update/delete the workload objects on the WEC.</p> <p>To construct the function from destination to customized wrapped object, the transport controller reads the <code>Binding</code>'s list of references to workload objects. The controller reads those objects from the WDS using a Kubernetes \"dynamic\" client. Immediately upon reading each workload object, the controller applies the WEC-independent transforms (from the <code>CustomTransform</code> objects). After doing that for all the listed workload objects, the controller goes through those objects one-by-one and applies template expansion for each destination if the object requests template expansion. If any of those objects requests template expansion and has a string that actually involves template expansion: the controller accumulates a map from destination to slice of customized objects and then invokes the transport plugin on each of those slices, to ultimately produce the function from destination to wrapped object. If none of the selected workload objects actually involved any template expansion then the controller wraps the slice of workload objects to get one wrapped object and produces a constant function from destination to that one wrapped object. </p> <p>Transport controller is based on the controller design pattern and aims to bring the current state to the desired state. If a WEC was removed from the <code>Binding</code>, the transport controller will also make sure to remove the matching wrapped object(s) from the WEC's mailbox namespace.</p>"},{"location":"direct/architecture/#custom-transform-cache","title":"Custom transform cache","text":"<p>To support efficient application of the <code>CustomTransform</code> objects, the transport controller maintains a cache of the results of internalizing what the users are asking for. In relational algebra terms, that cache consists of the following relations.</p> <p>Relation \"USES\": has a row whenever the <code>Binding</code>'s list of workload objects uses the <code>GroupResource</code>.</p> column name type in key <code>bindingName</code> string yes <code>gr</code> metav1.GroupResource yes <p>Relation \"INSTRUCTIONS\": has a row saying what to do for each <code>GroupResource</code>.</p> column name type in key <code>gr</code> metav1.GroupResource yes <code>removes</code> SET(jsonpath.Query) no <p>Relation \"SPECS\": remembers the specs of <code>CustomTransform</code> objects.</p> column name type in key <code>ctName</code> string yes <code>gr</code> metav1.GroupResource no <code>removes</code> SET(string) no <p>The cache maintains the following invariants on those relations. Note how these invariants require removal of data that is no longer interesting.</p> <ol> <li>INSTRUCTIONS has a row for a given <code>GroupResource</code> if and only if USES has one or more rows for that <code>GroupResource</code>.</li> <li>SPECS has a row for a given <code>CustomTransform</code> name if and only if that <code>CustomTransform</code> contributed to an existing row in INSTRUCTIONS.</li> </ol> <p>Whenever it removes a row from INSTRUCTIONS due to loss of confidence in that row, the cache has the controller enqueue a reference to every related <code>Binding</code> from USES, so that eventually a revised row will be derived and applied to every dependent <code>Binding</code>.</p> <p>The interface to the cache is <code>customTransformCollection</code> and the implementation is in a <code>*customTransformCollectionImpl</code>. This represents those relations as follows.</p> <ol> <li>USES is represented by two indices, each a map from one column value to the set of related other column values. The two indices are in <code>bindingNameToGroupResources</code> and <code>grToTransformData/bindingsThatCare</code>.</li> <li>INSTRUCTIONS is represented by <code>grToTransformData/removes</code>.</li> <li>SPECS is represented by <code>ctNameToSpec</code> and an index, <code>grToTransformData/ctNames</code>.</li> </ol> <p>The cache interface has the following methods.</p> <ul> <li> <p><code>getCustomTransformChanges</code> ensures that the cache has an entry for a given usage (a (<code>Binding</code>, <code>GroupResource</code>) pair) and returns the corresponding instructions (i.e., set of JSONPath to remove) for that <code>GroupResource</code>. This method sets the status of each <code>CustomTransform</code> API object that it processes.</p> <p>Of course this method maintains the cache's invariants. That means adding rows to SPECS as necessary. It also means removing a row from INSTRUCTIONS upon discovery that a <code>CustomTransform</code>'s Spec has changed its <code>GroupResource</code>. Note that the cache's invariants require this removal by this method, not relying on an eventual call to <code>NoteCustomTransform</code> (because the cache records at most the latest Spec for each <code>CustomTransform</code>, a later cache operation will not know about the previous <code>GroupResource</code>).</p> <p>Removing a row from INSTRUCTIONS also entails removing the corresponding rows from SPECS, to maintain the cache's invariants.</p> </li> <li> <p><code>noteCustomTransform</code> reacts to a create/update/delete of a <code>CustomTransform</code> object. In the update case, if the <code>CustomResourceSpec</code> changed its <code>GroupResource</code> then this method removes two rows from INSTRUCTIONS (if they were present): the one for the old <code>GroupResource</code> and the one for the new. In case of create, delete, or other change in Spec, this method removes the one relevant row (if present) in INSTRUCTIONS.</p> </li> <li> <p><code>setBindingGroupResources</code> reacts to knowing the full set of <code>GroupResource</code> that a given <code>Binding</code> uses. This removes outdated rows from USES (updates the two indices that represent it) and removes rows from INSTRUCTIONS that are no longer allowed.</p> </li> </ul>"},{"location":"direct/architecture/#customization-properties-cache","title":"Customization properties cache","text":"<p>The transport controller maintains a cached set of customization properties for each destination, and an association between <code>Binding</code> and the set of destinations that it references. When a relevant informer delivers an event about an inventory object (either a <code>ManagedCluster</code> object or a <code>ConfigMap</code> object that adds properties for that destination) the controller enqueues a work item of type <code>recollectProperties</code>. This work item carries the name of the inventory object. Processing that work item starts by re-computing the full map of properties for that destination. If the cache has an entry for that destination and the cached properties differ from the ones freshly computed, the controller updates that cache entry and enqueues a reference to every <code>Binding</code> object that depends on the properties of that destination.</p>"},{"location":"direct/argo-to-wds1/","title":"Install ArgoCD for delivery to a WDS","text":"<p>This document tells you how to install ArgoCD in the KubeFlex hosting cluster and configure ArgoCD to deliver applications to a WDS.  The commands shown here assume that you access the KubeFlex hosting cluster via a kubeconfig context named \"kind-kubeflex\" and that you access the WDS via a kubeconfig context named \"wds1\"; adapt as appropriate to your particular circumstances.</p> <p>Install ArgoCD on kind-kubeflex:</p> <pre><code>kubectl --context kind-kubeflex create namespace argocd\nkubectl --context kind-kubeflex apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre> <p>Install CLI:</p> <p>on MacOS:</p> <pre><code>brew install argocd\n</code></pre> <p>on Linux:</p> <pre><code>curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\nrm argocd-linux-amd64\n</code></pre> <p>Check the ArgoCD releases page for the obtaining the latest  stable release for other architectures and operating systems.</p> <p>Configure Argo to work with the ingress installed in the hosting cluster:</p> <pre><code>kubectl --context kind-kubeflex apply -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argocd-server-ingress\n  namespace: argocd\n  annotations:\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: argocd.localtest.me\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: argocd-server\n            port:\n              name: https\nEOF\n</code></pre> <p>Open a browser to ArgoCD console:</p> <pre><code>open https://argocd.localtest.me:9443\n</code></pre> <p>Note: if you are working on a VM via SSH, just take the IP of the VM (VM_IP) and add the line <code>&lt;VM_IP&gt; argocd.localtest.me</code> to your '/etc/hosts' file, replacing  with the actual IP of your desktop. <p>Get the password for Argo with:</p> <pre><code>kubectl config use-context kind-kubeflex\nargocd admin initial-password -n argocd\n</code></pre> <p>Login into the ArgoCD console with <code>admin</code> and the password just retrieved. Type the following on a shell terminal in your desktop (or just enter the address https://argocd.localtest.me:9443 on your browser):</p> <pre><code>open https://argocd.localtest.me:9443\n</code></pre> <p>Also, login with the argocd CLI with the same credentials.</p> <pre><code>argocd login --insecure argocd.localtest.me:9443\n</code></pre> <p>Add the <code>wds1</code> space as cluster to ArgoCD:</p> <pre><code>CONTEXT=wds1\nkubectl config view --minify --context=${CONTEXT} --flatten &gt; /tmp/${CONTEXT}.kubeconfig\nkubectl config --kubeconfig=/tmp/${CONTEXT}.kubeconfig set-cluster ${CONTEXT}-cluster --server=https://${CONTEXT}.${CONTEXT}-system 2&gt;/dev/null\nkubectl config use-context kind-kubeflex\nARGO_SERVER_POD=$(kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server -o 'jsonpath={.items[0].metadata.name}')\nkubectl cp /tmp/${CONTEXT}.kubeconfig -n argocd ${ARGO_SERVER_POD}:/tmp\nPASSWORD=$(argocd admin initial-password -n argocd | cut -d \" \" -f 1)\nkubectl exec -it -n argocd $ARGO_SERVER_POD -- argocd login argocd-server.argocd --username admin --password $PASSWORD --insecure\nkubectl exec -it -n argocd $ARGO_SERVER_POD -- argocd cluster add ${CONTEXT} --kubeconfig /tmp/${CONTEXT}.kubeconfig -y\n</code></pre> <p>Configure Argo to label resources with the \"argocd.argoproj.io/instance\" label:</p> <pre><code>kubectl --context kind-kubeflex patch cm -n argocd argocd-cm -p '{\"data\": {\"application.instanceLabelKey\": \"argocd.argoproj.io/instance\"}}'\n</code></pre>"},{"location":"direct/binding/","title":"Binding workload with WEC","text":"<p>This document is about associating WECs with workload objects. The primary concept is sometimes called \"downsync\", which confusingly refers to both the propagation and transformation of desired state from core to WECs and the propagation and summarization of reported state from WECs to core.</p>"},{"location":"direct/binding/#binding-basics","title":"Binding Basics","text":"<p>The user controls downsync primarily through API objects of kinds <code>BindingPolicy</code> and <code>Binding</code>. These go in a WDS and associate workload objects in that WDS with WECs, along with adding some modulations on how downsync is done.</p> <p><code>BindingPolicy</code> is a higher level concept than <code>Binding</code>. KubeStellar has a controller that translates each <code>BindingPolicy</code> to a <code>Binding</code>. A user could eschew the <code>BindingPolicy</code> and directly maintain a <code>Binding</code> object or let a different controller maintain the <code>Binding</code> object (TODO: check that this is true). The <code>Binding</code> object shows which workload objects and which WECs matched the predicates in the <code>BindingPolicy</code> and so is also useful as feedback to the user about that.</p>"},{"location":"direct/binding/#bindingpolicy","title":"BindingPolicy","text":"<p>The <code>spec</code> of a <code>BindingPolicy</code> has two predicates that (1) identify a subset of the WECs in the inventory of the ITS associated with the WDS and (2) identify a subset of the workload objects in the WDS. The primary function of the <code>BindingPolicy</code> is to assert the desired association between (1) and (2). A <code>BindingPolicy</code> can also add some modulations on how those workload objects are downsynced to/from those WECs.</p> <p>The WEC-selecting predicate is an array of label selectors in <code>spec.clusterSelectors</code>. These label selectors test the labels of the inventory objects describing the WECs. The bound WECs are the ones whose inventory object passes at least one of the the label selectors in <code>spec.clusterSelectors</code>.</p> <p>The workload object selection predicate is in <code>spec.downsync</code>, which holds a list of <code>DownsyncPolicyClause</code>s; each includes both a workload object selection predicate and also three kinds of information that modulate the downsync. Note that each such clause must have at least one field specifying part of the workload selection predicate.</p> <p>For more definitional details about a <code>BindingPolicy</code>, see the API reference.</p> <p>Following is an example of a <code>BindingPolicy</code> object, used in the end-to-end test of <code>createOnly</code> functionality.</p> <pre><code>apiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: nginx\nspec:\n  clusterSelectors:\n  - matchLabels:\n      location-group: edge\n  downsync:\n  - objectSelectors:\n    - matchLabels:\n        app.kubernetes.io/name: nginx\n    resources:\n    - namespaces\n  - createOnly: true\n    objectSelectors:\n    - matchLabels:\n        app.kubernetes.io/name: nginx\n    resources:\n    - deployments\n</code></pre>"},{"location":"direct/binding/#binding","title":"Binding","text":"<p>TODO: write this</p>"},{"location":"direct/combined-status/","title":"Combined Status from WECs","text":"<p>Note on terminology: the general idea that we wish to address is returning reported state about a workload object to its WDS. At the current level of development, we equate reported state with the status section of an object --- while anticipating a more general treatment in the future.</p> <p>There are two methods of returning reported state: a general one and a special case. The general method returns reported state from any number of WECs. The special case applies when the number of WECs is exactly 1, and returns the reported state into the original object in the WDS.</p>"},{"location":"direct/combined-status/#introduction-to-the-general-technique","title":"Introduction to the General Technique","text":"<p>The general technique for combining reported state from WECs is built upon the following ideas:</p> <ol> <li> <p>The way that reported state is combined is specified by the user, in a simple but powerful way modeled on SQL. This is chosen because it is a well worked out set of ideas, is widely known, and is something that we may someday want to use in our implementation. We do not need to support anything like full SQL (for any version of SQL). This proposal only involves one particular pattern of relatively simple SELECT statement, and a different expression language (CEL, which is the most prominent expression language in the Kubernetes milieu).</p> </li> <li> <p>The expressions may reference the content of the workload object as it sits in the WDS as well as the state returned from the WECs.</p> </li> <li> <p>The specification of how to combine reported state is defined in <code>StatusCollector</code> objects. These objects are referenced in the <code>BindingPolicy</code> objects right next to the criteria for selecting workload objects. This saves users the trouble of having to write selection criteria twice. With the specification being separate rather than embedded, it is possible to have a library of <code>StatusCollectors</code> that can be reused across different <code>BindingPolicy</code> objects. In the future KubeStellar would provide a library of <code>StatusCollector</code> objects that cover convenient use-cases for kubernetes built-in resources such as deployments.</p> </li> <li> <p>The <code>Binding</code> objects also hold references to <code>StatusCollector</code> objects. Each reference to a workload object is paired with references to all the <code>StatusCollectors</code> mentioned in all the <code>DownsyncObjectTestAndStatusCollection</code> structs that matched the workload object.</p> </li> <li> <p>The combined reported state appears in a new kind of object, one per (workload object, <code>Binding</code> object) pair.</p> </li> <li> <p>A user can request a list without aggregation, possibly after filtering, but certainly with a limit on list length. The expectation is that such a list makes sense only if the length of the list will be modest. For users that want access to the full reported state from each WEC for a large number of WECs, KubeStellar should have an abstraction that gives the users access --- in a functional way, not by making another copy --- to that state (which is already in the mailbox namespaces).</p> </li> <li> <p>The reported state for a given workload object from a given WEC is implicitly augmented with metadata about the WEC and about the end-to-end propagation from WDS to that WEC. This extra information is available just like the regular contents of the object, for use in combining reported state.</p> <ul> <li>The specifics of queryable objects and implicit augmentations can be found in <code>api/control/v1alpha1/types.go</code> and are specified in Queryable Objects.</li> </ul> </li> <li> <p>Errors in the user-supplied expressions are reported in relevant API objects (<code>StatusCollector</code> and <code>CombinedStatus</code>). For aggregation operations, input rows with expression errors are skipped.</p> </li> </ol>"},{"location":"direct/combined-status/#relation-with-sql","title":"Relation with SQL","text":""},{"location":"direct/combined-status/#overview-of-relation-with-sql","title":"Overview of Relation with SQL","text":"<p>To a given workload object, and in the context of a given <code>Binding</code> object, the user has bound some <code>StatusCollector</code> objects. The meaning of a <code>StatusCollector</code> in the context of a (workload object, <code>Binding</code> object) pair is analogous to an SQL SELECT statement that does the following things.</p> <ol> <li> <p>The SELECT statement has one input table, which has a row per WEC that the Binding says the workload object should go to.</p> </li> <li> <p>The SELECT statement can have a WHERE clause that filters out some of the rows.</p> </li> <li> <p>The SELECT statement either does aggregation or does not. In the case of not doing aggregation, the SELECT statement simply has a collection of named expressions defining the columns of its output.</p> </li> <li> <p>In the case of aggregation, the SELECT statement has the following.</p> <ul> <li> <p>An optional <code>GROUP BY</code> clause saying how the rows (WECs) are   grouped to form the inputs for aggregation, in terms of named   expressions. For convenience here, each of these named   expressions is implicitly included in the output columns.</p> </li> <li> <p>A collection of named expressions using aggregation functions to define   additional output columns.</p> </li> </ul> </li> <li> <p>The SELECT statement has a LIMIT on the number of rows that it will yield.</p> </li> </ol>"},{"location":"direct/combined-status/#detailed-relation-with-sql","title":"Detailed Relation with SQL","text":"<p>For a given workload object, <code>Binding</code>, and <code>StatusCollector</code>: start with a table named <code>PerWEC</code>. This table has one primary key column and it holds the name of the WEC that the reported state is from. The dependent columns hold the workload object content from the WDS, the workload object content returned from the WEC, and the augmentations.</p> <p>There are three forms of <code>StatusCollector</code>, equivalent to three forms of SQL statement.</p>"},{"location":"direct/combined-status/#plain-selection","title":"Plain selection","text":"<p>When the <code>StatusCollector</code> has selection but no \"GROUP BY\" and no aggregation, this is equivalent to the following form of SELECT statement. The List of stale WECs example below is an example of this form.</p> <pre><code>SELECT &lt;selected columns&gt;\nFROM PerWEC WHERE &lt;filter condition&gt;\nLIMIT &lt;limit&gt;\n</code></pre>"},{"location":"direct/combined-status/#aggregation-without-group-by","title":"Aggregation without <code>GROUP BY</code>","text":"<p>When there is aggregation but no plain selection and no <code>GROUP BY</code>, this is equivalent to the following form of SELECT statement. The Number of WECs example below is an example of this form.</p> <pre><code>SELECT &lt;aggregation columns&gt;\nFROM PerWEC WHERE &lt;filter condition&gt;\nLIMIT &lt;limit&gt;\n</code></pre>"},{"location":"direct/combined-status/#aggregation-with-group-by","title":"Aggregation with <code>GROUP BY</code>","text":"<p>When there is <code>GROUP BY</code> and aggregation but no plain selection, this is equivalent to the following form of SELECT statement. The Histogram of Pod phase example below is an example of this form.</p> <pre><code>SELECT &lt;group-by column names&gt;, &lt;aggregation columns&gt;\nFROM (SELECT &lt;group-by column 1 expr&gt; AS &lt;group-by column 1 name&gt;,\n             ...\n             &lt;group-by column N expr&gt; AS &lt;group-by column N name&gt;,\n             *\n      FROM PerWEC WHERE &lt;filter condition&gt;)\nGROUP BY &lt;group-by column names&gt;\nLIMIT &lt;limit&gt;\n</code></pre> <p>When there are N <code>GROUP BY</code> columns, the result has a row for each tuple of values (v1, v2, ... v<code>N</code>) such that there exists a WEC for which (v1, v2, ... v<code>N</code>) are the values of the <code>GROUP BY</code> columns. The result has no more rows than that.</p>"},{"location":"direct/combined-status/#specification-of-the-general-technique","title":"Specification of the general technique","text":"<p>In <code>types.go</code> see (a) <code>StatusCollector</code>, (b) the references to those from <code>DownsyncPolicyClause</code>, <code>NamespaceScopeDownsyncClause</code>, and <code>ClusterScopeDownsyncClause</code>, and (c) <code>CombinedStatus</code>.</p>"},{"location":"direct/combined-status/#queryable-objects","title":"Queryable Objects","text":"<p>A CEL expression within a <code>StatusCollector</code> can reference the following objects:</p> <ol> <li> <p><code>inventory</code>: The inventory object for the workload object:</p> <ul> <li><code>inventory.name</code>: The name of the inventory object.</li> </ul> </li> <li> <p><code>obj</code>: The workload object from the WDS:</p> <ul> <li>All fields of the workload object except the status subresource.</li> </ul> </li> <li> <p><code>returned</code>: The reported state from the WEC:</p> <ul> <li><code>returned.status</code>: The status section of the object returned from the WEC.</li> </ul> </li> <li> <p><code>propagation</code>: Metadata about the end-to-end propagation process:</p> <ul> <li><code>propagation.lastReturnedUpdateTimestamp</code>: metav1.Time of last update to any returned state.</li> </ul> </li> </ol>"},{"location":"direct/combined-status/#examples-of-using-the-general-technique","title":"Examples of using the general technique","text":""},{"location":"direct/combined-status/#number-of-wecs","title":"Number of WECs","text":"<p>The <code>StatusCollector</code> would look like the following.</p> <pre><code>apiVersion: control.kubestellar.io/v1alpha1\nkind: StatusCollector\nmetadata:\n  name: count-wecs\nspec:\n  combinedFields:\n     - name: count\n       type: COUNT\n  limit: 10\n</code></pre> <p>To specify using that, the <code>BindingSpec</code> would reference it from the <code>statusCollectors</code> in the relevant <code>DownsyncPolicyClause</code>(s). Following is an example.</p> <pre><code>apiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: example-binding-policy\nspec:\n  clusterSelectors:\n  - matchLabels: {\"location-group\":\"edge\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/name\":\"nginx\"}\n    statusCollectors: [ count-wecs ]\n</code></pre> <p>The analogous SQL statement would look something like the following.</p> <pre><code>SELECT COUNT(*) AS count FROM PerWEC LIMIT &lt;something&gt;\n</code></pre> <p>The table resulting from this would have one column and one row. The one value in this table would be the number of WECs.</p> <p>Following is an example of a consequent <code>CombinedStatus</code> object.</p> <pre><code>apiVersion: control.kubestellar.io/v1alpha1\nkind: CombinedStatus\nmetadata:\n  creationTimestamp: \"2024-11-07T20:15:27Z\"\n  generation: 1\n  labels:\n    status.kubestellar.io/api-group: apps\n    status.kubestellar.io/binding-policy: nginx-bindingpolicy\n    status.kubestellar.io/name: nginx-deployment\n    status.kubestellar.io/namespace: nginx\n    status.kubestellar.io/resource: deployments\n  name: 0990056b-ccbc-4c46-b0fe-366ef3a2de5e.332d2c17-7b55-44f6-9a6e-21445523c808\n  namespace: nginx\n  resourceVersion: \"604\"\n  uid: cc167004-073e-4a20-9857-449f692e9643\nresults:\n- columnNames:\n  - count\n  name: count-wecs\n  rows:\n  - columns:\n    - float: \"2\"\n      type: Number\n</code></pre>"},{"location":"direct/combined-status/#histogram-of-pod-phase","title":"Histogram of Pod phase","text":"<p>The <code>spec</code> of the <code>StatusCollector</code> would look like the following.</p> <pre><code>  groupBy:\n     - name: phase\n       def: returned.status.phase\n  combinedFields:\n     - name: count\n       type: COUNT\n</code></pre> <p>The analogous SQL statement would look something like the following.</p> <pre><code>SELECT phase, COUNT(*) AS count\nFROM (SELECT &lt;SQL expression for returned.status.phase&gt; AS phase, *\n      FROM PerWEC)\nGROUP BY phase\nLIMIT &lt;something&gt;\n</code></pre> <p>The result would have two columns, holding a phase value and a count. The number of rows equals the number of different values of <code>returned.status.phase</code> that appear among the WECs. For each row (P, N): P is a phase value that appears in at least one WEC, and N is the number of WECs where the phase value is P.</p>"},{"location":"direct/combined-status/#histogram-of-number-of-available-replicas-of-a-deployment","title":"Histogram of number of available replicas of a Deployment","text":"<p>This reports, for each number of available replicas, how many WECs have that number. The <code>spec</code> of the <code>CombinedStatus</code> would look like the following.</p> <pre><code>  groupBy:\n     - name: numAvailable\n       def: returned.status.availableReplicas\n  combinedFields:\n     - name: count\n       type: COUNT\n</code></pre>"},{"location":"direct/combined-status/#list-of-wecs-where-the-deployment-is-not-as-available-as-desired","title":"List of WECs where the Deployment is not as available as desired","text":"<p>The <code>spec</code> of the <code>CombinedStatus</code> would look like the following.</p> <pre><code>  filter: \"obj.spec.replicas != returned.status.availableReplicas\"\n  select:\n     - name: wec\n       def: inventory.name\n</code></pre>"},{"location":"direct/combined-status/#full-status-from-each-wec-with-information-retrieval-time","title":"Full status from each WEC with information retrieval time","text":"<p>The <code>spec</code> of the <code>CombinedStatus</code> would look like the following. This produces a listing of object status paired with inventory object name.</p> <pre><code>  select:\n     - name: wec\n       def: inventory.name\n     - name: status\n       def: returned.status\n     - name: retrievalTime\n       def: propagation.lastReturnedUpdateTimestamp\n</code></pre>"},{"location":"direct/combined-status/#special-case-for-1-wec","title":"Special case for 1 WEC","text":"<p>When a workload object is distributed from a WDS to exactly one WEC, the reported state from that WEC can be returned into the copy of the workload object in the WDS. The design of most kinds of Kubernetes API object implicitly assumes that the object exists and has its defined effect in only one cluster. That is why it makes sense to return the reported state from the WEC to the WDS only when the object goes to exactly one WEC.</p> <p>As mentioned above, currently KubeStellar equates \"reported state\" with the <code>.status</code> section of the API object.</p> <p>The user has to specifically request this last step of <code>.status</code> propagation. This is done in an optional boolean field, named <code>wantSingletonReportedState</code>, in a <code>DownsyncPolicyClause</code> in a <code>BindingPolicy</code>. This is one of the three kinds of downsync modulations that a policy clause can associate with the matching workload objects. In a <code>Binding</code>, this same optional boolean field appears in <code>NamespaceScopeDownsyncClause</code> and <code>ClusterScopeDownsyncClause</code>. If multiple policy clauses in a <code>BindingPolicy</code> match a given workload object, the settings for <code>.wantSingletonReportedState</code> are combined by OR to get the one boolean value that appears with the reference to the object in the corresponding <code>Binding</code>. If multiple <code>Binding</code> objects in one WDS reference a given workload object, there is another level of multiplicity to consider. Read on.</p> <p>For a given workload object and WDS, we say that \"singleton status return is requested\" if and only if there exists at least one BindingPolicy or Binding that has <code>wantSingletonReportedState==true</code> in a clause that matches/references the workload object.</p> <p>The \"qualified WEC set\" of a given workload object in a given WDS is the set of WECs that are associated with that workload object by at least one BindingPolicy or Binding that has <code>wantSingletonReportedState==true</code> in a clause that matches/references the workload object.</p> <p>For a given workload object in a given WDS, while singleton status return is requested, KubeStellar maintains a label on the object whose name (key) is <code>kubestellar.io/executing-count</code> and whose value is a string representation of the size of the qualified WEC set of that object.  While singleton status return is not requested, KubeStellar suppresses the existence of a label with that name (key).  While singleton status return is requested and the size of the qualified WEC set is 1, KubeStellar propagates the object's <code>.status</code> from that WEC to the <code>.status</code> section of the object in the WDS.  While either singleton status return is NOT requested or the size of the qualified WEC set is NOT 1, there is nothing in the <code>.status</code> of the object in the WDS that was propagated there from a WEC by KubeStellar.</p>"},{"location":"direct/contribute/","title":"Contributing to KubeStellar","text":"<p>Welcome to the KubeStellar Contribution Guide! We are excited to have you here. </p> <p>You can join the community via our Slack channel.</p> <p>This section provides information on the Code of Conduct, guidelines, terms, and conditions that define the KubeStellar contribution processes. By contributing, you are enabling the success of KubeStellar users, and that goes a long way to make everyone happier, including you. We welcome individuals who are new to open-source contributions.</p> <p>There are different ways you can contribute to the KubeStellar development:</p> <ul> <li> <p>Documentation: Enhance the documentation by fixing typos, enabling semantic clarity, adding links, updating information on changelogs and release versions, and implementing content strategy.</p> </li> <li> <p>Code: Indicate your interest in developing new features, modifying existing features, raising concerns, or fixing bugs.</p> </li> </ul> <p>Before you start contributing, familiarize yourself with our community Code of Conduct.</p>"},{"location":"direct/contribute/#visit-the-github-repository","title":"Visit the GitHub repository","text":"<p>The KubeStellar GitHub organization is a collection of the different KubeStellar repositories that you can start contributing to.</p>"},{"location":"direct/contribute/#sign-off-your-contribution","title":"Sign off your contribution","text":"<p>Ensure that you comply with the rules and policy guiding the repository contribution indicated in the Developer Certificate of Origin (DCO). </p> <p>If you are contributing via the GitHub web interface, navigate to the Settings section of your forked repository and enable the Require contributors to sign off on web-based commits setting. This will allow you to automatically sign off your commits via GitHub directly, as shown below.</p> <p> signoff-via-github-ui </p> <p>If you are contributing via the command line terminal, run the <code>git commit --signoff --message [commit message]</code> or <code>git commit -s -m [commit message]</code> command when making each commit.</p>"},{"location":"direct/contribute/#contribution-resources","title":"Contribution Resources","text":"<p>Read the resources to gain a better understanding of the contribution processes.</p> <ul> <li>Code of Conduct The CNCF code of conduct for the KubeStellar community</li> <li>Contribution Guidelines General Guidelines for our Github processes</li> <li>License The Apache 2.0 license under which KubeStellar is published</li> <li>Governance The protocols under which the KubeStellar project is run</li> <li>Onboarding The procedures for adding/removing members of our Github organization</li> <li>Website<ul> <li>Build Overview How our website is built and how to collaboratively work on changes to it using Github staging</li> <li>Testing website PRs how to test website changes using only your local workstation</li> </ul> </li> <li>Security<ul> <li>Policy Security Policies</li> <li>Contacts Who to contact with security concerns</li> </ul> </li> <li>Testing How to use the preconfigured tests in the repository</li> <li>Packaging How the components of KubeStellar are organized</li> <li>Release Process All the steps involved in creating and publishing a new release of KubeStellar</li> <li>Release Testing Steps involved in testing a release or release candidate before merging it into the main branch.</li> <li>Sign-off and Signing Contributions How to properly configure your commits so they are both signed and \"signed off\" (and how those terms differ)</li> </ul>"},{"location":"direct/control/","title":"Controlling KubeStellar","text":"<p>This is the parent document for docs about particular kinds of control.</p> <ul> <li>Binding between workload objects and WECs</li> <li>Transforming workload objects on their way to WECs</li> <li>Combining returned status</li> </ul> <p>TODO: write this for real.</p>"},{"location":"direct/core-chart-argocd/","title":"Using Argo CD with KubeStellar Core chart","text":""},{"location":"direct/core-chart-argocd/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Pre-requisites</li> <li>Installing Argo CD using KubeStellar Core chart</li> <li>Deploying Argo CD applications</li> </ul>"},{"location":"direct/core-chart-argocd/#overview","title":"Overview","text":"<p>This documents explains how to use the KubeStellar core Helm chart to:</p> <ul> <li>deploy Argo CD in KubeFlex hosting cluster;</li> <li>register every WDS as a target cluster in Argo CD; and</li> <li>create Argo CD applications as specified by the chart values.</li> </ul> <p>For a detailed step-by-step installation guide with expected outputs, see Step-by-Step Installation Guide.</p>"},{"location":"direct/core-chart-argocd/#pre-requisites","title":"Pre-requisites","text":"<p>Before installing Argo CD with KubeStellar Core chart, ensure you have:</p> <ul> <li>All prerequisites from installing KubeStellar using the Core chart</li> <li>A properly configured KubeFlex hosting cluster</li> <li>Helm installed and configured</li> <li>kubectl access to your cluster</li> </ul> <p>The settings described in this document are an extension of the KubeStellar Core chart settings described here.</p> <p>The KubeStellar core chart can optionally be used to install Argo CD in the KubeFlex hosting cluster and register each KubeStellar WDS as an Argo CD target cluster. The core chart also has the option to define some Argo CD Applications. This section will cover installing Argo CD and mapping WDSes to target clusters; the next section will show how to also define some Applications.</p>"},{"location":"direct/core-chart-argocd/#installing-argo-cd-using-kubestellar-core-chart","title":"Installing Argo CD using KubeStellar Core chart","text":"<p>To enable the installation of Argo CD by the KubeStellar Core chart, use the flag <code>--set argocd.install=true</code>. Besides deploying an instance of Argo CD, KubeStellar Core chart will take care of registering all the WDSes installed by the chart as Argo CD target clusters.</p> <p>When deploying in an OpenShift cluster, add the flag <code>--set argocd.openshift.enabled=true</code>.</p> <p>When deploying in a Kubernetes cluster, use the flag <code>--set argocd.global.domain=&lt;url&gt;</code> to provide the URL for the nginx ingress, which defaults to <code>argocd.localtest.me</code>.</p> <p>Note that when creating a local Kubernetes cluster using our scripts for Kind or k3s, the nginx ingress will be accessible on host port <code>9443</code>; therefore the Argo CD UI can be accessed at the address <code>https://argocd.localtest.me:9443</code>.</p>"},{"location":"direct/core-chart-argocd/#example-installation-with-argo-cd","title":"Example installation with Argo CD","text":"<pre><code>helm upgrade --install ks-core core-chart \\\n  --set argocd.install=true\n</code></pre> <p>Expected output: <pre><code>Release \"ks-core\" has been upgraded. Happy Helming!\nNAME: ks-core\nLAST DEPLOYED: Thu Jun 12 11:02:16 2025\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 3\nTEST SUITE: None\nNOTES:\nFor your convenience you will probably want to add contexts to your\nkubeconfig named after the non-host-type control planes (WDSes and\nITSes) that you just created (a host-type control plane is just an\nalias for the KubeFlex hosting cluster). You can do that with the\nfollowing `kflex` commands; each creates a context and makes it the\ncurrent one. See\nhttps://github.com/kubestellar/kubestellar/blob/0.28.0-alpha.2/docs/content/direct/core-chart.md#kubeconfig-files-and-contexts-for-control-planes\nfor a way to do this without using `kflex`.\nStart by setting your current kubeconfig context to the one you used\nwhen installing this chart.\n\nkubectl config use-context $the_one_where_you_installed_this_chart\nkflex ctx --set-current-for-hosting # make sure the KubeFlex CLI's hidden state is right for what the Helm chart just did\n\nFinally, you can use `kflex ctx` to switch back to the kubeconfig\ncontext for your KubeFlex hosting cluster.\n\nAccess Argo CD UI at https://argocd.localtest.me (append :9443 for Kind or k3s installations).\nObtain Argo CD admin password using the command:\nkubectl -n default get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre></p>"},{"location":"direct/core-chart-argocd/#retrieve-argo-cd-admin-password","title":"Retrieve Argo CD admin password","text":"<p>The initial password for the <code>admin</code> user can be retrieved using the following command:</p> <pre><code>kubectl -n default get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre> <p>Expected output(similar): <pre><code>EpQ2-OMgvfdHiMiD\n</code></pre></p>"},{"location":"direct/core-chart-argocd/#verify-argo-cd-installation","title":"Verify Argo CD installation","text":"<p>Verify that all Argo CD components are running:</p> <pre><code>kubectl get pods -A | grep -i argo\n</code></pre> <p>Expected output (similar to): <pre><code>default              ks-core-argocd-application-controller-0                     1/1     Running     0          15m\ndefault              ks-core-argocd-applicationset-controller-6669c9f789-wd5h7   1/1     Running     0          15m\ndefault              ks-core-argocd-dex-server-8464bc64b9-dplv5                  1/1     Running     0          15m\ndefault              ks-core-argocd-notifications-controller-66b8ccc4c7-8mjdx    1/1     Running     0          15m\ndefault              ks-core-argocd-redis-76c6b4db57-7cfrs                       1/1     Running     0          15m\ndefault              ks-core-argocd-repo-server-6774bd65db-rxmtz                 1/1     Running     0          15m\ndefault              ks-core-argocd-server-84cbbd8cbd-bpl92                      1/1     Running     0          15m\n</code></pre></p>"},{"location":"direct/core-chart-argocd/#access-argo-cd-ui","title":"Access Argo CD UI","text":"<p>Open your browser and navigate to: <code>https://argocd.localtest.me:9443/</code></p> <p>Login credentials: - Username: <code>admin</code> - Password: Use the password obtained from the previous command (e.g., <code>EpQ2-OMgvfdHiMiD</code>)</p> <p>Note: If you encounter SSL certificate warnings in your browser, proceed with \"Advanced\" \u2192 \"Proceed to argocd.localtest.me (unsafe)\" or similar option, as this is expected for local development setups. alt text </p>"},{"location":"direct/core-chart-argocd/#deploying-argo-cd-applications","title":"Deploying Argo CD applications","text":"<p>The KubeStellar Core chart can also be used to deploy Argo CD applications as specified by chart values. The example below shows the relevant fragment of the chart values that could be used for deploying an application corresponding to <code>scenario-6</code> in KubeStellar docs.</p> <pre><code>argocd:\n  applications: # list of Argo CD applications to be create\n  - name: scenario-6 # required, must be unique\n    project: default # default: default\n    repoURL: https://github.com/pdettori/sample-apps.git\n    targetRevision: HEAD # default: HEAD\n    path: nginx\n    destinationWDS: wds1\n    destinationNamespace: nginx-sa # default: default\n    syncPolicy: auto # default: manual\n</code></pre> <p>Alternatively, the same result can be achieved from Helm CLI by using the followig minimal argument (note that the default values are not explicitely set):</p> <pre><code>--set-json='argocd.applications=[ { \"name\": \"scenario-6\", \"repoURL\": \"https://github.com/pdettori/sample-apps.git\", \"path\": \"nginx\", \"destinationWDS\": \"wds1\", \"destinationNamespace\": \"nginx-sa\" } ]'\n</code></pre> <p> alt text </p> <p>Important: Currently, the KubeStellar controller does not return resource status correctly to Argo CD. This means that deployed applications may not show as \"Healthy\" or green in the Argo CD UI, even when they are actually running correctly on the workload execution clusters. This is a known limitation and does not indicate that your deployment has failed.</p>"},{"location":"direct/core-chart/","title":"KubeStellar Core Chart Documentation","text":""},{"location":"direct/core-chart/#table-of-contents","title":"\ud83d\udcda Table of Contents","text":"<ul> <li>Pre-requisites</li> <li>KubeStellar Core Chart values</li> <li>KubeStellar Core Chart Usage Step-by-Step</li> <li>Kubeconfig Files and Contexts for Control Planes</li> <li>Argo CD Integration</li> <li>Uninstalling the KubeStellar Core Chart</li> </ul> <p>This documents explains how to use KubeStellar Core chart to do three of the 11 installation and usage steps; please see the full outline for generalities and Getting Started for an example of usage.</p> <p>This Helm chart can do any subset of the following things.</p> <ul> <li>Initialize a pre-existing cluster to serve as the KubeFlex hosting cluster.</li> <li>Create some ITSes.</li> <li>Create some WDSes.</li> </ul> <p>The information provided is specific for the following release:</p> <pre><code>export KUBESTELLAR_VERSION=0.28.0\n</code></pre>"},{"location":"direct/core-chart/#pre-requisites","title":"Pre-requisites","text":"<p>To install the Helm chart the only requirement is Helm. However, additional executables may be required to create/manage the cluster(s) (e.g., Kind and kubectl), to join Workload Execution Clusters (WECs) (e.g., clusteradm), and to interact with Control Planes (e.g., kubectl), etc. For such purpose, a full list of executable that may be required can be found here.</p> <p>The setup of KubeStellar via the Core chart requires the existence of a KubeFlex hosting cluster.</p> <p>While not a complete list of supported hosting clusters, here we discuss how to use KubeStellar in:</p> <ol> <li> <p>A local Kind or k3s cluster with an ingress with SSL passthrough and a mapping to host port 9443</p> <p>This option is particularly useful for first time users or users that would like to have a local deployment.</p> <p>It is important to note that, when the hosting cluster was created by kind or k3s and its Ingress domain name is left to default to localtest.me, then the name of the container running hosting cluster must be also be referenced during the Helm chart installation by setting <code>--set \"kubeflex-operator.hostContainer=&lt;control-plane-container-name&gt;\"</code>. The <code>&lt;control-plane-container-name&gt;</code> is the name of the container in which kind or k3d is running the relevant control plane. One may use <code>docker ps</code> to find the <code>&lt;control-plane-container-name&gt;</code>.</p> <p>If a host port number different from the expected 9443 is used for the Kind cluster, then the same port number must be specified during the chart installation by adding the following argument <code>--set \"kubeflex-operator.externalPort=&lt;port&gt;\"</code>.</p> <p>By default the KubeStellar Core chart uses a test domain <code>localtest.me</code>, which is OK for testing on a single host machine. However, for scenarios that span more than one machine, it is necessary to set <code>--set \"kubeflex-operator.domain=&lt;domain&gt;\"</code> to a more appropriate <code>&lt;domain&gt;</code> that can be reached from Workload Execution Clusters (WECs).</p> <p>For convenience, a new local Kind cluster that satisfies the requirements for KubeStellar setup and that can be used to exercises the examples can be created with the following command:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/v$KUBESTELLAR_VERSION/scripts/create-kind-cluster-with-SSL-passthrough.sh) --name kubeflex --port 9443\n</code></pre> <p>Alternatively, a new local k3s cluster that satisfies the requirements for KubeStellar setup and that can be used to exercises the examples can be created with the following command:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/v$KUBESTELLAR_VERSION/scripts/create-k3s-cluster-with-SSL-passthrough.sh) --port 9443\n</code></pre> </li> <li> <p>An OpenShift cluster</p> <p>When using this option, one is required to explicitly set the <code>isOpenShift</code> variable to <code>true</code> by including <code>--set \"kubeflex-operator.isOpenShift=true\"</code> in the Helm chart installation command.</p> </li> </ol>"},{"location":"direct/core-chart/#kubestellar-core-chart-values","title":"KubeStellar Core Chart values","text":"<p>The KubeStellar chart makes available to the user several values that may be used to customize its installation into an existing cluster:</p> <pre><code># Control controller log verbosity\n# The \"default\" verbosity value will be used for all controllers unless a specific controller verbosity override is specified\nverbosity:\n  default: 2\n  # Specific controller verbosity overrides:\n  # kubestellar: 6 (controller-manager)\n  # clusteradm: 6\n  # transport: 6\n\n# KubeFlex override values\nkubeflex-operator:\n  install: true # enable/disable the installation of KubeFlex by the chart (default: true)\n  installPostgreSQL: true # enable/disable the installation of the appropriate version of PostgreSQL required by KubeFlex (default: true)\n  isOpenShift: false # set this variable to true when installing the chart in an OpenShift cluster (default: false)\n  # Kind cluster specific settings:\n  domain: localtest.me # used to define the DNS domain name used from outside the KubeFlex hosting cluster to reach that cluster's Ingress endpoint (default: localtest.me)\n  externalPort: 9443 # used to set the port to access the Control Planes API (default: 9443)\n  hostContainer: kubeflex-control-plane # used to set the name of the container that runs the KubeFlex hosting cluster (default: kubeflex-control-plane, which corresponds to a Kind cluster with name kubeflex)\n\n# Determine if the Post Create Hooks should be installed by the chart\nInstallPCHs: true\n\n# List the Inventory and Transport Spaces (ITSes) to be created by the chart\n# Each ITS consists of:\n# - a mandatory unique name\n# - an optional type, which could be host, vcluster, or external (default to vcluster, if not specified)\n# - an optional install_clusteradm flag, which could be true  or false  (default to true) to enable/disable the installation of OCM in the control plane\n# - an optional bootstrapSecret secion to be used for Control Plabes of type external (more details below)\nITSes: # ==&gt; installs ocm (optional) + ocm-status-addon\n\n# List the Workload Description Spaces (WDSes) to be created by the chart\n# Each WDS consists of a mandatory unique name and several optional parameters:\n# - type: host or k8s (default to k8s, if not specified)\n# - APIGroups: a comma separated list of APIGroups\n# - ITSName: the name of the ITS control plane to be used by the WDS. Note that the ITSName MUST be specified if more than one ITS exists.\nWDSes: # ==&gt; installs kubestellar + ocm-transport-plugin\n</code></pre> <p>The first section of the <code>values.yaml</code> file refers to parameters that are specific to the KubeFlex installation, see here for more information.</p> <p>In particular: - <code>kubeflex-operator.install</code> accepts a boolean value to enable/disable the installation of KubeFlex into the cluster by the chart - <code>kubeflex-operator.isOpenShift</code> must be set to true by the user when installing the chart into a OpenShift cluster</p> <p>By default, the chart will install the KubeFlex and its PostgreSQL dependency.</p> <p>The second section allows a user of the chart to determine if Post Create Hooks (PCHes) needed for creating ITSes and WDSes control planes should be installed by the chart. By default <code>InstallPCHs</code> is set to <code>true</code> to enable the installation of the PCHes, however one may want to set this value to <code>false</code> when installing multiple copies of the chart to avoid conflicts. A single copy of the PCHes is required and allowed per cluster.</p> <p>The third section of the <code>values.yaml</code> file allows one to create a list of Inventory and Transport Spaces (ITSes). By default, this list is empty and no ITS will be created by the chart. A list of ITSes can be specified using the following format:</p> <pre><code>ITSes: # all the CPs in this list will execute the its.yaml PCH\n  - name: &lt;its1&gt;          # mandatory name of the control plane\n    type: &lt;vcluster|host|external&gt; # optional type of control plane: host, vcluster, or external (default to vcluster, if not specified)\n    install_clusteradm: true|false  # optional flag to enable/disable the installation of OCM in the control plane (default to true, if not specified)\n    bootstrapSecret: # this section is ignored unless type is \"external\"\n      name: &lt;secret-name&gt; # default: \"&lt;control-plane-name&gt;-bootstrap\"\n      namespace: &lt;secret-namespace&gt; # default: Helm chart installation namespace\n      key: &lt;key-name&gt; # default: \"kubeconfig-incluster\"\n  - name: &lt;its2&gt;          # mandatory name of the control plane\n    type: &lt;vcluster|host|external&gt; # optional type of control plane: host, vcluster, or external (default to vcluster, if not specified)\n    install_clusteradm: true|false  # optional flag to enable/disable the installation of OCM in the control plane (default to true, if not specified)\n    bootstrapSecret: # this section is ignored unless type is \"external\"\n      name: &lt;secret-name&gt; # default: \"&lt;control-plane-name&gt;-bootstrap\"\n      namespace: &lt;secret-namespace&gt; # default: Helm chart installation namespace\n      key: &lt;key-name&gt; # default: \"kubeconfig-incluster\"\n  ...\n</code></pre> <p>where <code>name</code> must specify a name unique among all the control planes in that KubeFlex deployment, the optional <code>type</code> can be vcluster (default), host, or external, see here for more information, and the optional <code>install_clusteradm</code>can be either true (default) or false to enable or disable the installation of OCM in the control plane.</p> <p>When the ITS <code>type</code> is <code>external</code>, the <code>bootstrapSecret</code> sub-section can be used to indicate the bootstrap secret used by KubeFlex to connect to the external cluster. Specifically, it can be used to specify any combination of (a) the name of the secret, (b) the namespace containing the secret, and (c) the name of the key containg the kubeconfig of the external cluster if they need to be different from their default value.</p> <p>If the secret was created using the create-external-bootstrap-secret.sh script and the value passed to the argument <code>--controlplane</code> matches the name of the Control Plane specified by the Helm chart, then the sub-section <code>bootstrapSecret</code> is not required because all default values will identify the bootstrap secret created by the script. More specifically, if an external kind cluster was created with the command <code>kind create cluster --name its1</code> and the <code>create-external-bootstrap-secret.sh --controlplane its1 --verbose</code> command was used to create the bootstrap secret, then it would be enough to inform the Helm chart with <code>--set-json='ITSes=[{\"name\":\"its1\",\"type\":\"external\"}]'</code>.</p> <p>The fourth section of the <code>values.yaml</code> file allows one to create a list of Workload Description Spaces (WDSes). By default, this list is empty and no WDS will be created by the chart. A list of WDSes can be specified using the following format:</p> <pre><code>WDSes: # all the CPs in this list will execute the wds.yaml PCH\n  - name: &lt;wds1&gt;     # mandatory name of the control plane\n    type: &lt;host|k8s&gt; # optional type of control plane host or k8s (default to k8s, if not specified)\n    APIGroups: \"\"    # optional string holding a comma-separated list of APIGroups\n    ITSName: &lt;its1&gt;  # optional name of the ITS control plane, this MUST be specified if more than one ITS exists at the moment the WDS PCH starts\n  - name: &lt;wds2&gt;     # mandatory name of the control plane\n    type: &lt;host|k8s&gt; # optional type of control plane host or k8s (default to k8s, if not specified)\n    APIGroups: \"\"    # optional string holding a comma-separated list of APIGroups\n    ITSName: &lt;its2&gt;  # optional name of the ITS control plane, this MUST be specified if more than one ITS exists at the moment the WDS PCH starts\n  ...\n</code></pre> <p>where <code>name</code> must specify a name unique among all the control planes in that KubeFlex deployment (note that this must be unique among both ITSes and WDSes), the optional <code>type</code> can be either k8s (default) or host, see here for more information, the optional <code>APIGroups</code> provides a list of APIGroups, see here for more information, and <code>ITSName</code> specify the ITS connected to the new WDS being created (this parameter MUST be specified if more that one ITS exists in the cluster, if no value is specified and only one ITS exists in the cluster, then it will be automatically selected).</p>"},{"location":"direct/core-chart/#kubestellar-core-chart-usage-step-by-step","title":"KubeStellar Core Chart usage step by step","text":"<p>The local copy of the core chart can be installed in an existing cluster using the commands: <pre><code>git clone https://github.com/kubestellar/kubestellar.git\ncd kubestellar\n</code></pre> <pre><code>helm dependency update core-chart\n</code></pre> Output(similar): <pre><code>Saving 2 charts\nDownloading kubeflex-operator from repo oci://ghcr.io/kubestellar/kubeflex/chart\nPulled: ghcr.io/kubestellar/kubeflex/chart/kubeflex-operator:v0.8.9\nDigest: sha256:2be43de71425ad682edca6544f6c3a5864afbfad09a4b7e1e57bde6dae664334\nDownloading argo-cd from repo oci://ghcr.io/argoproj/argo-helm\nPulled: ghcr.io/argoproj/argo-helm/argo-cd:7.8.5\nDigest: sha256:662f4687e8e525f86ff9305020632b337a09ffacb7b61b7c42a841922c91da7b\nDeleting outdated charts\n</code></pre> <pre><code>helm upgrade --install ks-core core-chart\n</code></pre> Output: <pre><code>Release \"ks-core\" does not exist. Installing it now.\nNAME: ks-core\nLAST DEPLOYED: Thu Jun 12 09:58:44 2025\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nFor your convenience you will probably want to add contexts to your kubeconfig named after the non-host-type control planes (WDSes and ITSes) that you just created (a host-type control plane is just an alias for the KubeFlex hosting cluster). You can do that with the following `kflex` commands; each creates a context and makes it the current one.\n\nSee https://github.com/kubestellar/kubestellar/blob/0.28.0-alpha.2/docs/content/direct/core-chart.md#kubeconfig-files-and-contexts-for-control-planes for a way to do this without using `kflex`.\n\nStart by setting your current kubeconfig context to the one you used when installing this chart.\n\nkubectl config use-context $the_one_where_you_installed_this_chart\nkflex ctx --set-current-for-hosting # make sure the KubeFlex CLI's hidden state is right for what the Helm chart just did\n\nFinally, you can use `kflex ctx` to switch back to the kubeconfig context for your KubeFlex hosting cluster.\n</code></pre></p> <p>Alternatively, a specific version of the KubeStellar core chart can be simply installed in an existing cluster using the following command:</p> <pre><code>helm upgrade --install ks-core oci://ghcr.io/kubestellar/kubestellar/core-chart --version $KUBESTELLAR_VERSION\n</code></pre> <p>Either of the previous ways of installing KubeStellar core chart will install KubeFlex and the Post Create Hooks, but it will not create any Control Plane.</p> <p>Please remember to add <code>--set \"kubeflex-operator.isOpenShift=true\"</code> when installing/updating into an OpenShift cluster.</p> <p>User defined control planes can be added using additional values files or <code>--set</code> arguments, e.g.:</p> <ul> <li>add a single ITS named its1 of default vcluster type: <code>--set-json='ITSes=[{\"name\":\"its1\"}]'</code></li> <li>add two ITSes named its1 and its2 of of type vcluster and host, respectively: <code>--set-json='ITSes=[{\"name\":\"its1\"},{\"name\":\"its2\",\"type\":\"host\"}]'</code></li> <li>add a single WDS named wds1 of default k8s type connected to the one and only ITS: <code>--set-json='WDSes=[{\"name\":\"wds1\"}]'</code></li> </ul> <p>A KubeStellar Core installation that is consistent with Getting Started and and supports the example scenarios could be achieved with the following command:</p> <p><pre><code>helm upgrade --install ks-core oci://ghcr.io/kubestellar/kubestellar/core-chart --version \"$KUBESTELLAR_VERSION\" \\\n  --set-json ITSes='[{\"name\":\"its1\"}]' \\\n  --set-json WDSes='[{\"name\":\"wds1\"}]'\n</code></pre> Output: <pre><code>Release \"ks-core\" has been upgraded. Happy Helming!\nNAME: ks-core\nLAST DEPLOYED: Thu Jun 12 10:08:50 2025\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\nNOTES:\nFor your convenience you will probably want to add contexts to your\nkubeconfig named after the non-host-type control planes (WDSes and\nITSes) that you just created (a host-type control plane is just an\nalias for the KubeFlex hosting cluster). You can do that with the\nfollowing `kflex` commands; each creates a context and makes it the\ncurrent one. See\nhttps://github.com/kubestellar/kubestellar/blob/0.28.0-alpha.2/docs/content/direct/core-chart.md#kubeconfig-files-and-contexts-for-control-planes\nfor a way to do this without using `kflex`.\nStart by setting your current kubeconfig context to the one you used\nwhen installing this chart.\n\nkubectl config use-context $the_one_where_you_installed_this_chart\nkflex ctx --set-current-for-hosting # make sure the KubeFlex CLI's hidden state is right for what the Helm chart just did\n\nkflex ctx --overwrite-existing-context its1\nkflex ctx --overwrite-existing-context wds1\n\nFinally, you can use `kflex ctx` to switch back to the kubeconfig\ncontext for your KubeFlex hosting cluster.\n</code></pre> The core chart also supports the use of a pre-existing cluster (or any space, really) as an ITS. A specific application is to connect to existing OCM clusters. As an example, create a first local kind cluster with OCM installed in it:</p> <pre><code>kind create cluster --name ext1\n\nclusteradm init\n</code></pre> <p>Then, create a second kind cluster suitable for KubeStellar installation and create a bootstrap secret in the new cluster with the kubeconfig information of the <code>ext1</code> cluster:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/v$KUBESTELLAR_VERSION/scripts/create-kind-cluster-with-SSL-passthrough.sh) --name kubeflex --port 9443\n\nbash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/v$KUBESTELLAR_VERSION/scripts/create-external-bootstrap-secret.sh) --controlplane its1 --source-context kind-ext1 --address https://ext1-control-plane:6443 --verbose\n</code></pre> <p>Note that the last command above creates a secret named <code>its1-bootstrap</code> in the Helm chart installation namespace of the <code>kind-kubeflex</code> cluster.</p> <p>The <code>--address</code> URL needs to be one that the KubeFlex controller can use to open a connection to the external cluster's Kubernetes apiserver(s). In this example, the external cluster is a kind cluster with one kube-apiserver and it listens on port 6443 in its node's network namespace. This example relies on the DNS resolver in Docker networking to map the domain name <code>ext1-control-plane</code> to the Docker network address of the container of that same name.</p> <p>Finally, install the core chart using the <code>ext1</code> cluster as ITS:</p> <pre><code>helm upgrade --install core-chart oci://ghcr.io/kubestellar/kubestellar/core-chart --version $KUBESTELLAR_VERSION \\\n  --set-json='ITSes=[{\"name\":\"its1\",\"type\":\"external\",\"install_clusteradm\":false}]' \\\n  --set-json='WDSes=[{\"name\":\"wds1\"}]'\n</code></pre> <p>Note that by default, the <code>its1</code> Control Plane of type <code>external</code> will look for a secret named <code>its1-bootstrap</code> in the Helm chart installation namespace. Additionally the <code>\"install_clusteradm\":false</code> value is specified to avoid reinstalling OCM in the <code>ext1</code> cluster.</p> <p>After the initial installation is completed, there are two main ways to install additional control planes (e.g., create a second <code>wds2</code> WDS):</p> <ol> <li> <p>Upgrade the initial chart. This choice requires to relist the existing control planes, which would otherwise be deleted:</p> <pre><code>helm upgrade --install ks-core oci://ghcr.io/kubestellar/kubestellar/core-chart --version $KUBESTELLAR_VERSION \\\n  --set-json='ITSes=[{\"name\":\"its1\"}]' \\\n  --set-json='WDSes=[{\"name\":\"wds1\"},{\"name\":\"wds2\"}]'\n</code></pre> </li> <li> <p>Install a new chart with a different name. This choice does not requires to relist the existing control planes, but requires to disable the reinstallation of KubeFlex and PCHes:</p> <pre><code>helm upgrade --install add-wds2 oci://ghcr.io/kubestellar/kubestellar/core-chart --version $KUBESTELLAR_VERSION \\\n  --set='kubeflex-operator.install=false,InstallPCHs=false' \\\n  --set-json='WDSes=[{\"name\":\"wds2\"}]'\n</code></pre> </li> </ol>"},{"location":"direct/core-chart/#kubeconfig-files-and-contexts-for-control-planes","title":"Kubeconfig files and contexts for Control Planes","text":"<p>It is convenient to use one kubeconfig file that has a context for each of your control planes. That can be done in two ways, one using the <code>kflex</code> CLI and one not.</p> <ol> <li> <p>Using <code>kflex</code> CLI</p> <p>The following commands will add a context, named after the given control plane, to your current kubeconfig file and make that the current context. The deletion is to remove an older vintage if it is present.</p> <pre><code>kubectl config delete-context $cpname\nkflex ctx $cpname\n</code></pre> <p>The <code>kflex ctx</code> command is unable to create a new context if the current context does not access the KubeFlex hosting cluster AND the KubeFlex kubeconfig extension remembering that context's name is not set; see the KubeFlex user guide for your release of KubeFlex for more information.</p> <p>To automatically add all Control Planes as contexts of the current kubeconfig, one can use the convenience script below:</p> <pre><code>echo \"Getting the kubeconfig of all Control Planes...\"\nfor cpname in `kubectl get controlplane -o name`; do\n  cpname=${cpname##*/}\n  echo \"Getting the kubeconfig of Control Planes \\\"$cpname\\\"...\"\n  kflex ctx $cpname\ndone\n</code></pre> <p>After doing the above context switching you may wish to use <code>kflex ctx</code> to switch back to the hosting cluster context.</p> <p>Afterwards the content of a Control Plane <code>$cpname</code> can be accessed by specifying its context:</p> <pre><code>kubectl --context \"$cpname\" ...\n</code></pre> </li> <li> <p>Using plain <code>kubectl</code> commands</p> <p>The following commands can be used to create a fresh kubeconfig file for each of the KubeFlex Control Planes in the hosting cluster:</p> <pre><code>echo \"Creating a kubeconfig for each KubeFlex Control Plane:\"\nfor cpname in `kubectl get controlplane -o name`; do\n  cpname=${cpname##*/}\n  echo \"Getting the kubeconfig of \\\"$cpname\\\" ==&gt; \\\"kubeconfig-$cpname\\\"...\"\n  if [[ \"$(kubectl get controlplane $cpname -o=jsonpath='{.spec.type}')\" == \"host\" ]] ; then\n    kubectl config view --minify --flatten &gt; \"kubeconfig-$cpname\"\n  else\n    kubectl get secret $(kubectl get controlplane $cpname -o=jsonpath='{.status.secretRef.name}') \\\n      -n $(kubectl get controlplane $cpname -o=jsonpath='{.status.secretRef.namespace}') \\\n      -o=jsonpath=\"{.data.$(kubectl get controlplane $cpname -o=jsonpath='{.status.secretRef.key}')}\" \\\n      | base64 -d &gt; \"kubeconfig-$cpname\"\n  fi\n  curname=$(kubectl --kubeconfig \"kubeconfig-$cpname\" config current-context)\n  if [ \"$curname\" != \"$cpname\" ]\n  then kubectl --kubeconfig \"kubeconfig-$cpname\" config rename-context \"$curname\" $cpname\n  fi\ndone\n</code></pre> <p>The code above puts the kubeconfig for a control plane <code>$cpname</code> into a file name <code>kubeconfig-$cpname</code> in the local folder. The current context will be renamed to <code>$cpname</code>, if it does not already have that name (which it will for control planes of type \"k8s\", for example).</p> <p>With the above kubeconfig files in place, the control plane named <code>$cpname</code> can be accessed as follows.</p> <pre><code>kubectl --kubeconfig \"kubeconfig-$cpname\" ...\n</code></pre> <p>The individual kubeconfigs can also be merged as contexts of the current <code>~/.kube/config</code> with the following commands:</p> <pre><code>echo \"Merging the Control Planes kubeconfigs into ~/.kube/config ...\"\ncp ~/.kube/config ~/.kube/config.bak\nKUBECONFIG=~/.kube/config:$(find . -maxdepth 1 -type f -name 'kubeconfig-*' | tr '\\n' ':') kubectl config view --flatten &gt; ~/.kube/kubeconfig-merged\nmv ~/.kube/kubeconfig-merged ~/.kube/config\n</code></pre> <p>Afterwards the content of a Control Plane <code>$cpname</code> can be accessed by specifying its context:</p> <pre><code>kubectl --context \"$cpname\" ...\n</code></pre> </li> <li> <p>Using <code>import-cp-contexts.sh</code> script</p> <p>The following convenience command can also be used to import all the KubeFlex Control Planes in the current hosting cluster as contexts of the current kubeconfig. The script involved requires that you have <code>yq</code> (also available from Homebrew) installed.</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/v$KUBESTELLAR_VERSION/scripts/import-cp-contexts.sh) --merge\n</code></pre> <p>The script above only requires <code>kubectl</code> and <code>yq</code>.</p> <p>The script accepts the following arguments:</p> <ul> <li><code>--kubeconfig &lt;filename&gt;</code> specify the kubeconfig of the hosting cluster where the KubeFlex Control Planes are located. Note that this argument will override the content of the <code>KUBECONFIG</code> environment variable</li> <li><code>--context &lt;name&gt;</code> specify a context of the current kubeconfig where to look for KubeFlex Control Planes. If this argument is not specified, then all contexts will be searched.</li> <li><code>--names|-n &lt;name1&gt;,&lt;name2&gt;,..</code> comma separated list of KubeFlex Control Planes names to import. If this argument is not specified then all available KubeFlex Control Planes will be imported.</li> <li><code>--replace-localhost|-r &lt;host&gt;</code> replaces server addresses \"127.0.0.1\" with a desired <code>&lt;host&gt;</code>. This parameter is useful for making KubeFlex Control Planes of type <code>host</code> accessible from outside the machine hosting the cluster.</li> <li><code>--merge|-m</code> merge the kubeconfig with the contexts of the control planes with the existing cluster kubeconfig. If this flag is not specified, then only the kubeconfig with the contexts of the KubeFlex Control Planes will be produced.</li> <li><code>--output|-o &lt;filename&gt;|-</code> specify a kubeconfig file to save the kubeconfig to. Use <code>-</code> for stdout. If this argument is not provided, then the kubeconfig will be saved to the input specified kubeconfig, if provided, or to <code>~/.kube/config</code>.</li> <li><code>--silent|-s</code> quiet mode, do not print information. This may be useful when using <code>-o -</code>.</li> <li><code>-X</code> enable verbose execution of the script for debugging</li> </ul> </li> </ol>"},{"location":"direct/core-chart/#argo-cd-integration","title":"Argo CD integration","text":"<p>KubeStellar Core Helm chart allows to deploy ArgoCD in the KubeFlex hosting cluster, register every WDS as a target cluster in Argo CD, and create Argo CD applications as specified by chart values, as explained here.</p>"},{"location":"direct/core-chart/#uninstalling-the-kubestellar-core-chart","title":"Uninstalling the KubeStellar Core chart","text":"<p>The chart can be uninstalled using the command:</p> <pre><code>helm uninstall ks-core\n</code></pre> <p>This will remove KubeFlex, PostgreSQL, Post Create Hooks (PCHes), and all KubeFlex Control Planes (i.e., ITSes and WDSes) that were created by the chart.</p> <p>Additionally, if a Kind cluster was created with the provide script, it can be deleted with the command:</p> <pre><code>kind delete cluster --name kubeflex\n</code></pre> <p>Alternatively, if a k3s cluster was created with the provide script, it can be deleted with the command:</p> <pre><code>/usr/local/bin/k3s-uninstall.sh\n</code></pre>"},{"location":"direct/example-scenarios/","title":"KubeStellar Example Scenarios","text":"<p>This document shows some simple examples of using the release that contains this version of this document. These scenarios can be used to test a KubeStellar installation for proper functionality. These scenarios suppose that you have done \"setup\". General setup instructions are outlined in the User Guide Overview; a simple example setup is in the Setup section of Getting Started.</p>"},{"location":"direct/example-scenarios/#assumptions-and-variables","title":"Assumptions and variables","text":"<p>Each scenario supposes that one ITS and one WDS have been created, and that two WECs have been created and registered and also labeled for selection by KubeStellar control objects. These scenarios are written as shell commands (bash or zsh). These commands assume that you have defined the following shell variables to convey the needed information about that ITS and WDS and those WECs. For a concrete example of settings of these variables, see the end of Getting Started.</p> <ul> <li><code>host_context</code>: the name of the kubeconfig context to use when accessing the KubeFlex hosting cluster.</li> <li><code>its_cp</code>: the name of the KubeFlex control plane that is playing the role of ITS.</li> <li><code>its_context</code>: the name of the kubeconfig context to use when accessing the ITS.</li> <li><code>wds_cp</code>: the name of the KubeFlex control plane that is playing the role of WDS.</li> <li><code>wds_context</code>: the name of the kubeconfig context to use when accessing the WDS.</li> <li><code>wec1_name</code>, <code>wec2_name</code>: the names of the <code>ManagedCluster</code> objects in the ITS representing the two WECs.</li> <li><code>wec1_context</code>, <code>wec2_context</code>: the names of the kubeconfig contexts to use when accessing the two WECs.</li> <li><code>label_query_both</code>: a restricted <code>kubectl</code> label query over <code>ManagedCluster</code> objects in the ITS that matches both WECs. The general form of label query usable here is a comma-separated series of <code>key=value</code> requirements.</li> <li><code>label_query_one</code>: a restricted <code>kubectl</code> label query over <code>ManagedCluster</code> objects that picks out just one of the WECs.</li> </ul> <p>Each example scenario concludes with instructions on how to undo its effects.</p> <p>There are also end-to-end (E2E) tests that are based on scenario 4 and an extended variant of scenario 1. These tests normally exercise the copy of the repo containing them (rather than a release). They can alternatively test a release. See the e2e tests (in <code>test/e2e</code>). Contributors can run these tests, and CI includes checking that these E2E tests pass. Some of these tests, and the setup for all of them, are written in <code>bash</code> so that contributors can easily follow them.</p>"},{"location":"direct/example-scenarios/#scenario-0-look-around","title":"Scenario 0 - look around","text":"<p>The following command will list all the <code>ManagedCluster</code> objects that will be relevant to these scenarios.</p> <pre><code>kubectl --context \"$its_context\" get managedclusters -l \"$label_query_both\"\n</code></pre> <p>Expect to get a listing of your two <code>ManagedCluster</code> objects.</p>"},{"location":"direct/example-scenarios/#scenario-1-multi-cluster-workload-deployment-with-kubectl","title":"Scenario 1 - multi-cluster workload deployment with kubectl","text":"<p>Create a BindingPolicy to deliver an app to all clusters in the WDS:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: nginx-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {$(echo \"$label_query_both\" | tr , $'\\n' | while IFS=\"=\" read key val; do echo -n \", \\\"$key\\\": \\\"$val\\\"\"; done | tail -c +3)}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/name\":\"nginx\"}\nEOF\n</code></pre> <p>This BindingPolicy configuration determines where to deploy the workload by using the label selector expressions found in clusterSelectors. It also specifies what to deploy through the downsync.labelSelectors expressions. Each matchLabels expression is a criterion for selecting a set of objects based on their labels. Other criteria can be added to filter objects based on their namespace, api group, resource, and name. If these criteria are not specified, all objects with the matching labels are selected. If an object has multiple labels, it is selected only if it matches all the labels in the matchLabels expression. If there are multiple objectSelectors, an object is selected if it matches any of them.</p> <p>Now deploy the app:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    app.kubernetes.io/name: nginx\n  name: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: nginx\n  labels:\n    app.kubernetes.io/name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: public.ecr.aws/nginx/nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Verify that the deployment has been created in both clusters</p> <pre><code>kubectl --context \"$wec1_context\" get deployments -n nginx\nkubectl --context \"$wec2_context\" get deployments -n nginx\n</code></pre> <p>Please note, in line with Kubernetes\u2019 best practices, the order in which you apply a BindingPolicy and the objects doesn\u2019t affect the outcome. You can apply the BindingPolicy first followed by the objects, or vice versa. The result remains consistent because the binding controller identifies any changes in either the BindingPolicy or the objects, triggering the start of the reconciliation loop.</p>"},{"location":"direct/example-scenarios/#optional-teardown-scenario-1","title":"[Optional] Teardown Scenario 1","text":"<pre><code>kubectl --context \"$wds_context\" delete ns nginx\nkubectl --context \"$wds_context\" delete bindingpolicies nginx-bpolicy\n</code></pre>"},{"location":"direct/example-scenarios/#scenario-2-out-of-tree-workload","title":"Scenario 2 - Out-of-tree workload","text":"<p>This scenario is like the previous one but involves a workload whose kind of objects is not built into Kubernetes. Instead, the workload object kind is defined by a <code>CustomResourceDefinition</code> object. While KubeStellar can handle the case where the CRD is part of the workload, this example concerns the case where the CRD is established in the WECs by some other means.</p> <p>For this example, we use the <code>AppWrapper</code> custom resource defined in the multi cluster app dispatcher project.</p> <p>Install the AppWrapper CRD in the WDS and the WECs.</p> <pre><code>clusters=(\"$wds_context\" \"$wec1_context\" \"$wec2_context\");\n  for cluster in \"${clusters[@]}\"; do\n  kubectl --context ${cluster} apply -f https://raw.githubusercontent.com/project-codeflare/multi-cluster-app-dispatcher/v1.39.0/config/crd/bases/workload.codeflare.dev_appwrappers.yaml\ndone\n</code></pre> <p>Run the following command to give permission for the klusterlet to operate on the appwrapper cluster resource.</p> <pre><code>clusters=(\"$wec1_context\" \"$wec2_context\");\nfor cluster in \"${clusters[@]}\"; do\nkubectl --context ${cluster} apply -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: appwrappers-access\nrules:\n- apiGroups: [\"workload.codeflare.dev\"]\n  resources: [\"appwrappers\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: klusterlet-appwrappers-access\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: appwrappers-access\nsubjects:\n- kind: ServiceAccount\n  name: klusterlet-work-sa\n  namespace: open-cluster-management-agent\nEOF\ndone\n</code></pre> <p>This step will be eventually automated, see this issue for more details.</p> <p>Next, apply an appwrapper object to the WDS:</p> <pre><code>kubectl --context \"$wds_context\" apply -f  https://raw.githubusercontent.com/project-codeflare/multi-cluster-app-dispatcher/v1.39.0/test/yaml/0008-aw-default.yaml\n</code></pre> <p>Label the appwrapper to match the binding policy:</p> <pre><code>kubectl --context \"$wds_context\" label appwrappers.workload.codeflare.dev defaultaw-schd-spec-with-timeout-1 app.kubernetes.io/part-of=my-appwrapper-app\n</code></pre> <p>Finally, apply the BindingPolicy:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: aw-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {$(echo \"$label_query_both\" | tr , $'\\n' | while IFS=\"=\" read key val; do echo -n \", \\\"$key\\\": \\\"$val\\\"\"; done | tail -c +3)}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/part-of\":\"my-appwrapper-app\"}\nEOF\n</code></pre> <p>Check that the app wrapper has been delivered to both clusters:</p> <pre><code>kubectl --context \"$wec1_context\" get appwrappers\nkubectl --context \"$wec2_context\" get appwrappers\n</code></pre>"},{"location":"direct/example-scenarios/#optional-teardown-scenario-2","title":"[Optional] Teardown Scenario 2","text":"<pre><code>kubectl --context \"$wds_context\" delete bindingpolicies aw-bpolicy\nkubectl --context \"$wds_context\" delete appwrappers --all\n</code></pre> <p>Wait until the following commands show no appwrappers in the two WECs.</p> <pre><code>kubectl --context \"$wec1_context\" get appwrappers -A\nkubectl --context \"$wec2_context\" get appwrappers -A\n</code></pre> <p>Then continue.</p> <pre><code>for cluster in \"$wec1_context\" \"$wec2_context\"; do\n  kubectl --context $cluster delete clusterroles appwrappers-access\n  kubectl --context $cluster delete clusterrolebindings klusterlet-appwrappers-access\ndone\n</code></pre> <p>Delete the CRD from the WDS and the WECs.</p> <pre><code>clusters=(\"$wds_context\" \"$wec1_context\" \"$wec2_context\");\n  for cluster in \"${clusters[@]}\"; do\n  kubectl --context ${cluster} delete -f https://raw.githubusercontent.com/project-codeflare/multi-cluster-app-dispatcher/v1.39.0/config/crd/bases/workload.codeflare.dev_appwrappers.yaml\ndone\n</code></pre>"},{"location":"direct/example-scenarios/#scenario-3-multi-cluster-workload-deployment-with-helm","title":"Scenario 3 - multi-cluster workload deployment with helm","text":"<p>Create a BindingPolicy for the helm chart app:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: postgres-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {$(echo \"$label_query_both\" | tr , $'\\n' | while IFS=\"=\" read key val; do echo -n \", \\\"$key\\\": \\\"$val\\\"\"; done | tail -c +3)}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\n      \"app.kubernetes.io/managed-by\": Helm,\n      \"app.kubernetes.io/instance\": postgres}\nEOF\n</code></pre> <p>Note that helm sets <code>app.kubernetes.io/instance</code> to the name of the installed release.</p> <p>Create and label the namespace and install the chart:</p> <pre><code>kubectl --context \"$wds_context\" create ns postgres-system\nkubectl --context \"$wds_context\" label ns postgres-system app.kubernetes.io/managed-by=Helm app.kubernetes.io/instance=postgres\nhelm --kube-context \"$wds_context\" install -n postgres-system postgres oci://registry-1.docker.io/bitnamicharts/postgresql\n</code></pre> <p>Verify that <code>StatefulSet</code> has been created in both clusters</p> <pre><code>kubectl --context \"$wec1_context\" get statefulsets -n postgres-system\nkubectl --context \"$wec2_context\" get statefulsets -n postgres-system\n</code></pre>"},{"location":"direct/example-scenarios/#optional-propagate-helm-metadata-secret-to-managed-clusters","title":"[Optional] Propagate helm metadata Secret to managed clusters","text":"<p>Run \"helm list\" on the WDS:</p> <pre><code>helm --kube-context \"$wds_context\" list -n postgres-system\n</code></pre> <p>and expect to see output like the following.</p> <pre><code>NAME            NAMESPACE       REVISION        UPDATED                                 STATUS       CHART                    APP VERSION\npostgres        postgres-system 1               2023-10-31 13:39:52.550071 -0400 EDT    deployed     postgresql-13.2.0        16.0.0\n</code></pre> <p>And try that on the managed clusters; you will get empty output.</p> <pre><code>helm list --kube-context \"$wec1_context\" -n postgres-system\nhelm list --kube-context \"$wec2_context\" -n postgres-system\n</code></pre> <p>This is because Helm creates a <code>Secret</code> object to hold its metadata about a \"release\" (chart instance) but Helm does not apply the usual labels to that object, so it is not selected by the <code>BindingPolicy</code> above and thus does not get delivered. The workload is functioning in the WECs, but <code>helm list</code> does not recognize its handiwork there. That labeling could be done for example with:</p> <pre><code>kubectl --context \"$wds_context\" label secret -n postgres-system $(kubectl --context \"$wds_context\" get secrets -n postgres-system -l name=postgres -l owner=helm  -o jsonpath='{.items[0].metadata.name}') app.kubernetes.io/managed-by=Helm app.kubernetes.io/instance=postgres\n</code></pre> <p>Verify that the chart shows up on the managed clusters:</p> <pre><code>helm list --kube-context \"$wec1_context\" -n postgres-system\nhelm list --kube-context \"$wec2_context\" -n postgres-system\n</code></pre> <p>Implementing this in a controller for automated propagation of helm metadata is tracked in this issue.</p>"},{"location":"direct/example-scenarios/#optional-teardown-scenario-3","title":"[Optional] Teardown Scenario 3","text":"<pre><code>helm --kube-context \"$wds_context\" uninstall -n postgres-system postgres\nkubectl --context \"$wds_context\" delete ns postgres-system\nkubectl --context \"$wds_context\" delete bindingpolicies postgres-bpolicy\n</code></pre>"},{"location":"direct/example-scenarios/#scenario-4-singleton-status","title":"Scenario 4 - Singleton status","text":"<p>This scenario shows how to get the full status updated, by setting <code>wantSingletonReportedState</code> in a <code>DownsyncPolicyClause</code>. This still an experimental feature.</p> <p>Apply a BindingPolicy with the <code>wantSingletonReportedState</code> flag set:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: nginx-singleton-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {\"name\":\"cluster1\"}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/name\":\"nginx-singleton\"}\n    wantSingletonReportedState: true\nEOF\n</code></pre> <p>Apply a new deployment for the singleton BindingPolicy:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-singleton-deployment\n  labels:\n    app.kubernetes.io/name: nginx-singleton\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: public.ecr.aws/nginx/nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Verify that the status is available in the WDS for the deployment by running the command:</p> <pre><code>kubectl --context \"$wds_context\" get deployments nginx-singleton-deployment -o yaml\n</code></pre> <p>Finally, scale the deployment from 1 to 2 replicas in the WDS:</p> <pre><code>kubectl --context \"$wds_context\" scale deployment nginx-singleton-deployment --replicas=2\n</code></pre> <p>and verify that replicas has been updated in the WEC and the WDS:</p> <pre><code>kubectl --context \"$wec1_context\" get deployment nginx-singleton-deployment\nkubectl --context \"$wds_context\" get deployment nginx-singleton-deployment\n</code></pre>"},{"location":"direct/example-scenarios/#optional-teardown-scenario-4","title":"[Optional] Teardown Scenario 4","text":"<pre><code>kubectl --context \"$wds_context\" delete bindingpolicies nginx-singleton-bpolicy\nkubectl --context \"$wds_context\" delete deployments nginx-singleton-deployment\n</code></pre>"},{"location":"direct/example-scenarios/#scenario-5-resiliency-testing","title":"Scenario 5 - Resiliency testing","text":"<p>This is a test that you can do after finishing Scenario 1.</p> <p>TODO: rewrite this so that it makes sense after Scenario 4.</p> <p>Bring down the control plane: stop and restart the ITS and WDS API servers, KubeFlex and KubeStellar controllers:</p> <p>First stop all:</p> <pre><code>kubectl --context \"$host_context\" scale deployment -n \"$wds_cp\"-system kube-apiserver --replicas=0\nkubectl --context \"$host_context\" scale statefulset -n \"$its_cp\"-system vcluster --replicas=0\nkubectl --context \"$host_context\" scale deployment -n kubeflex-system kubeflex-controller-manager --replicas=0\nkubectl --context \"$host_context\" scale deployment -n \"$wds_cp\"-system kubestellar-controller-manager --replicas=0\nkubectl --context \"$host_context\" scale deployment -n \"$wds_cp\"-system transport-controller --replicas=0\n</code></pre> <p>Then restart all:</p> <pre><code>kubectl --context \"$host_context\" scale deployment -n \"$wds_cp\"-system kube-apiserver --replicas=1\nkubectl --context \"$host_context\" scale statefulset -n \"$its_cp\"-system vcluster --replicas=1\nkubectl --context \"$host_context\" scale deployment -n kubeflex-system kubeflex-controller-manager --replicas=1\nkubectl --context \"$host_context\" scale deployment -n \"$wds_cp\"-system kubestellar-controller-manager --replicas=1\nkubectl --context \"$host_context\" scale deployment -n \"$wds_cp\"-system transport-controller --replicas=1\n</code></pre> <p>Wait for about a minute for all pods to restart, then apply a new BindingPolicy:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: nginx-res-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {$(echo \"$label_query_both\" | tr , $'\\n' | while IFS=\"=\" read key val; do echo -n \", \\\"$key\\\": \\\"$val\\\"\"; done | tail -c +3)}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"app.kubernetes.io/name\":\"nginx-res\"}\nEOF\n</code></pre> <p>and a new workload:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    app.kubernetes.io/name: nginx-res\n  name: nginx-res\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-res-deployment\n  namespace: nginx-res\n  labels:\n    app.kubernetes.io/name: nginx-res\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-res\n  template:\n    metadata:\n      labels:\n        app: nginx-res\n    spec:\n      containers:\n      - name: nginx-res\n        image: public.ecr.aws/nginx/nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <p>Verify that deployment has been created in both clusters</p> <pre><code>kubectl --context \"$wec1_context\" get deployments -n nginx-res\nkubectl --context \"$wec2_context\" get deployments -n nginx-res\n</code></pre>"},{"location":"direct/example-scenarios/#optional-teardown-scenario-5","title":"[Optional] Teardown Scenario 5","text":"<pre><code>kubectl --context \"$wds_context\" delete ns nginx-res\nkubectl --context \"$wds_context\" delete bindingpolicies nginx-res-bpolicy\n</code></pre>"},{"location":"direct/example-scenarios/#scenario-6-multi-cluster-workload-deployment-of-app-with-serviceaccount-with-argocd","title":"Scenario 6 - multi-cluster workload deployment of app with ServiceAccount with ArgoCD","text":"<p>Before running this scenario, install ArgoCD on the hosting cluster and configure it work with the WDS as outlined here.</p> <p>Including a ServiceAccount tests whether there will be a controller fight over a token Secret for that ServiceAccount, which was observed in some situations with older code.</p> <p>Apply the following BindingPolicy to the WDS:</p> <pre><code>kubectl --context \"$wds_context\" apply -f - &lt;&lt;EOF\napiVersion: control.kubestellar.io/v1alpha1\nkind: BindingPolicy\nmetadata:\n  name: argocd-sa-bpolicy\nspec:\n  clusterSelectors:\n  - matchLabels: {$(echo \"$label_query_both\" | tr , $'\\n' | while IFS=\"=\" read key val; do echo -n \", \\\"$key\\\": \\\"$val\\\"\"; done | tail -c +3)}\n  downsync:\n  - objectSelectors:\n    - matchLabels: {\"argocd.argoproj.io/instance\":\"nginx-sa\"}\nEOF\n</code></pre> <p>Switch context to hosting cluster and argocd namespace (this is required by argo to create an app with the CLI)</p> <pre><code>kubectl config use-context \"$host_context\"\nkubectl config set-context --current --namespace=argocd\n</code></pre> <p>Create a new application in ArgoCD:</p> <pre><code>argocd app create nginx-sa --repo https://github.com/pdettori/sample-apps.git --path nginx --dest-server https://\"${wds_cp}.${wds_cp}-system\" --dest-namespace nginx-sa\n</code></pre> <p>Open browser to Argo UI:</p> <pre><code>open https://argocd.localtest.me:9443\n</code></pre> <p>Open the app <code>nginx-sa</code> and sync it by clicking the \"sync\" button and then \"synchronize\".</p> <p>Alternatively, use the CLI to sync the app:</p> <pre><code>argocd app sync nginx-sa\n</code></pre> <p>Finally, check if the app has been deployed to the two clusters.</p> <pre><code>kubectl --context \"$wec1_context\" -n nginx-sa get deployments,sa,secrets\nkubectl --context \"$wec2_context\" -n nginx-sa get deployments,sa,secrets\n</code></pre> <p>Repeat multiple syncing on Argo and verify that extra secrets for the service account are not created in the WDS and both clusters:</p> <pre><code>kubectl --context \"$wds_context\" -n nginx-sa get secrets\nkubectl --context \"$wec1_context\" -n nginx-sa get secrets\nkubectl --context \"$wec2_context\" -n nginx-sa get secrets\n</code></pre>"},{"location":"direct/example-scenarios/#optional-teardown-scenario-6","title":"[Optional] Teardown Scenario 6","text":"<p>(Assuming that kubectl is still using the context for the hosting cluster and namespace <code>argocd</code>.)</p> <pre><code>argocd app delete nginx-sa --cascade\nkubectl --context \"$wds_context\" delete bindingpolicies argocd-sa-bpolicy\n</code></pre>"},{"location":"direct/galaxy-intro/","title":"galaxy","text":"<p>The KubeStellar galaxy is a secondary repository of as-is KubeStellar-related tools and packages that are not part of the regular KubeStellar releases. These integrations are beyond the scope of the core kubestellar repo, so are located here in a separate repository. It's name is galaxy in line with our space theme and to indicate a broader constellation of projects for establishing integrations/collaborations.</p> <p>It includes additional modules, tools and documentation to facilitate KubeStellar integration with other community projects, as well as some more experimental code we may be tinkering with for possible inclusion at some point.</p> <p>Right now, galaxy includes some bash-based utility, and scripts to replicate demos and PoCs such as KFP + KubeStellar integration and Argo Workflows + KubeStellar integration.</p>"},{"location":"direct/galaxy-intro/#utility-scripts","title":"Utility Scripts","text":"<ul> <li> <p><code>suspend-webhook</code> - webhook used to suspend argo workflows (and in the future other types of workloads supporting the suspend flag)</p> </li> <li> <p><code>shadow-pods</code> - controller used to support streaming logs in Argo Workflows and KFP.</p> </li> <li> <p><code>clustermetrics</code> - a CRD and controller that provide basic cluster metrics info for each node in a cluster, designed to work together with KubeStellar sync/status sync mechanisms.</p> </li> <li> <p><code>mc-scheduling</code> - A Multi-cluster scheduling framework supporting pluggable schedulers.</p> </li> </ul>"},{"location":"direct/galaxy-intro/#kubeflow-pipelines-v2","title":"KubeFlow Pipelines v2","text":""},{"location":"direct/galaxy-intro/#argo-workflows","title":"Argo Workflows","text":""},{"location":"direct/galaxy-intro/#learn-more","title":"Learn More","text":"<p>To learn more visit the repository at https://github.com/kubestellar/galaxy **_Note that all the code in the galaxy repo is experimental and is available on an as-is basis **</p>"},{"location":"direct/get-started/","title":"Getting Started with KubeStellar","text":""},{"location":"direct/get-started/#set-up-a-demo-system","title":"Set Up A Demo System","text":"<p>This page shows two ways to create one particular simple configuration that is suitable for kicking the tires (not production usage). This configuration has one <code>kind</code> cluster serving as your KubeFlex hosting cluster and two more serving as WECs. This page covers steps 2--7 from the full installation and usage outline. This page concludes with forwarding you to some example scenarios that illustrate the remaining steps.</p> <p>The two ways to create this simple configuration are as follows.</p> <ol> <li> <p>A quick automated setup using our demo setup script, which creates a basic working environment for those who want to start experimenting right away.</p> </li> <li> <p>A Step by step walkthrough that demonstrates the core concepts and components, showing how to manually set up a simple single-host system.</p> </li> </ol>"},{"location":"direct/get-started/#note-for-windows-users","title":"Note for Windows users","text":"<p>For some users on WSL, use of the setup procedure on this page and/or the demo environment creation script may require running as the user <code>root</code> in Linux. There is a known issue about this.</p>"},{"location":"direct/get-started/#quick-start-using-the-automated-script","title":"Quick Start Using the Automated Script","text":"<p>If you want to quickly setup a basic environment, you can use our automated installation script.</p>"},{"location":"direct/get-started/#install-software-prerequisites","title":"Install software prerequisites","text":"<p>Be sure to install the software prerequisites before running the script!</p> <p>The script will check for the pre-reqs and exit if they are not present.</p>"},{"location":"direct/get-started/#run-the-script","title":"Run the script!","text":"<p>The script can install KubeStellar's demonstration environment on top of kind or k3d</p> <p>For use with kind <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/refs/tags/v0.28.0/scripts/create-kubestellar-demo-env.sh) --platform kind\n</code></pre></p> <p>For use with k3d <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/refs/tags/v0.28.0/scripts/create-kubestellar-demo-env.sh) --platform k3d\n</code></pre></p> <p>If successful, the script will output the variable definitions that you would use when proceeding to the example scenarios. After successfully running the script, proceed to the Exercise KubeStellar section below.</p> <p>Note: the script does the same things as described in the Step by Step Setup but with maximum concurrency, so it can complete faster. This makes the script actually more complicated than the step-by-step process below. While this is great for getting started quickly with a demo system, you may want to follow the manual setup below to better understand the components and how to create a configuration that meets your needs.</p>"},{"location":"direct/get-started/#step-by-step-setup","title":"Step by Step Setup","text":"<p>This walks you through the steps to produce the same configuration as does the script above, suitable for study but not production usage. For general setup information, see the full story.</p>"},{"location":"direct/get-started/#install-software-prerequisites_1","title":"Install software prerequisites","text":"<p>The following command will check for the prerequisites that you will need for the later steps. See the prerequisites doc for more details.</p> <pre><code>bash &lt;(curl https://raw.githubusercontent.com/kubestellar/kubestellar/v0.28.0/scripts/check_pre_req.sh) kflex ocm helm kubectl docker kind\n</code></pre> <p>If that script complains then take it seriously! For example, the following indicates that you have a version of clusteradm that KubeStellar cannot use.</p> <pre><code>$ bash &lt;(curl https://raw.githubusercontent.com/kubestellar/kubestellar/v0.27.1/scripts/check_pre_req.sh) kflex ocm helm kubectl docker kind\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  9278  100  9278    0     0   135k      0 --:--:-- --:--:-- --:--:--  137k\n\u2714 KubeFlex (Kubeflex version: v0.8.2.5fd5f9c 2025-03-10T14:58:02Z)\n\u2714 OCM CLI (:v0.11.0-0-g73281f6)\n  structured version ':v0.11.0-0-g73281f6' is less than required minimum ':v0.7' or ':v0.10' but less than ':v0.11'\n</code></pre> <p>This setup recipe uses kind to create three Kubernetes clusters on your machine. Note that <code>kind</code> does not support three or more concurrent clusters unless you raise some limits as described in this <code>kind</code> \"known issue\": Pod errors due to \"too many open files\".</p>"},{"location":"direct/get-started/#cleanup-from-previous-runs","title":"Cleanup from previous runs","text":"<p>If you have run this recipe or any related recipe previously then you will first want to remove any related debris. The following commands tear down the state established by this recipe.</p> <pre><code>kind delete cluster --name kubeflex\nkind delete cluster --name cluster1\nkind delete cluster --name cluster2\nkubectl config delete-context cluster1\nkubectl config delete-context cluster2\n</code></pre> <p>After that cleanup, you may want to <code>set -e</code> so that failures do not go unnoticed (the various cleanup commands may legitimately \"fail\" if there is nothing to clean up).</p>"},{"location":"direct/get-started/#set-the-version-appropriately-as-an-environment-variable","title":"Set the Version appropriately as an environment variable","text":"<pre><code>kubestellar_version=0.28.0\n</code></pre>"},{"location":"direct/get-started/#create-a-kind-cluster-to-host-kubeflex","title":"Create a kind cluster to host KubeFlex","text":"<p>For convenience, a new local Kind cluster that satisfies the requirements for playing the role of KubeFlex hosting cluster can be created with the following command:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/v0.28.0/scripts/create-kind-cluster-with-SSL-passthrough.sh) --name kubeflex --port 9443\n</code></pre>"},{"location":"direct/get-started/#use-core-helm-chart-to-initialize-kubeflex-and-create-its-and-wds","title":"Use Core Helm chart to initialize KubeFlex and create ITS and WDS","text":"<pre><code>helm upgrade --install ks-core oci://ghcr.io/kubestellar/kubestellar/core-chart \\\n    --version \"$kubestellar_version\" \\\n    --set-json ITSes='[{\"name\":\"its1\"}]' \\\n    --set-json WDSes='[{\"name\":\"wds1\"},{\"name\":\"wds2\",\"type\":\"host\"}]' \\\n    --set verbosity.default=5  # so we can debug your problem reports\n</code></pre> <p>That command will print some notes about how to get kubeconfig \"contexts\" named \"its1\", \"wds1\", and \"wds2\" defined. Do that, because those contexts are used in the steps that follow.</p> <pre><code>kubectl config use-context kind-kubeflex # this is here only to remind you, it will already be the current context if you are following this recipe exactly\nkflex ctx --set-current-for-hosting # make sure the KubeFlex CLI's hidden state is right for what the Helm chart just did\nkflex ctx --overwrite-existing-context wds1\nkflex ctx --overwrite-existing-context wds2\nkflex ctx --overwrite-existing-context its1\n</code></pre>"},{"location":"direct/get-started/#wait-for-its-to-be-fully-initialized","title":"Wait for ITS to be fully initialized","text":"<p>The Helm chart above has a Job that initializes the ITS as an OCM \"hub\" cluster. Helm does not have a way to wait for that initialization to finish. So you have to do the wait yourself. The following commands will do that.</p> <pre><code>kubectl --context kind-kubeflex wait controlplane.tenancy.kflex.kubestellar.org/its1 --for 'jsonpath={.status.postCreateHooks.its-with-clusteradm}=true' --timeout 90s\nkubectl --context kind-kubeflex wait -n its1-system job.batch/its-with-clusteradm --for condition=Complete --timeout 150s\n</code></pre> <p>To learn more about the Core Helm Chart, refer to the Core Helm Chart documentation</p>"},{"location":"direct/get-started/#create-and-register-two-workload-execution-clusters","title":"Create and register two workload execution clusters","text":"<p>The following steps show how to create two new <code>kind</code> clusters and register them with the hub as described in the official open cluster management docs.</p> <p>Note that <code>kind</code> does not support three or more concurrent clusters unless you raise some limits as described in this <code>kind</code> \"known issue\": Pod errors due to \"too many open files\".</p> <ol> <li> <p>Execute the following commands to create two kind clusters, named <code>cluster1</code> and <code>cluster2</code>, and register them with the OCM hub. These clusters will serve as workload clusters. If you have previously executed these commands, you might already have contexts named <code>cluster1</code> and <code>cluster2</code>. If so, you can remove these contexts using the commands <code>kubectl config delete-context cluster1</code> and <code>kubectl config delete-context cluster2</code>.</p> <pre><code>: set flags to \"\" if you have installed KubeStellar on an OpenShift cluster\nflags=\"--force-internal-endpoint-lookup\"\nclusters=(cluster1 cluster2);\nfor cluster in \"${clusters[@]}\"; do\n   kind create cluster --name ${cluster}\n   kubectl config rename-context kind-${cluster} ${cluster}\n   clusteradm --context its1 get token | grep '^clusteradm join' | sed \"s/&lt;cluster_name&gt;/${cluster}/\" | awk '{print $0 \" --context '${cluster}' --singleton '${flags}'\"}' | sh\ndone\n</code></pre> <p>The <code>clusteradm</code> command grabs a token from the hub (<code>its1</code> context), and constructs the command to apply the new cluster to be registered as a managed cluster on the OCM hub.</p> </li> <li> <p>Repeatedly issue the command:</p> <pre><code>kubectl --context its1 get csr\n</code></pre> <p>until you see that the certificate signing requests (CSR) for both cluster1 and cluster2 exist. Note that the CSRs condition is supposed to be <code>Pending</code> until you approve them in step 4.</p> </li> <li> <p>Once the CSRs are created, approve the CSRs complete the cluster registration with the command:</p> <pre><code>clusteradm --context its1 accept --clusters cluster1\nclusteradm --context its1 accept --clusters cluster2\n</code></pre> </li> <li> <p>Check the new clusters are in the OCM inventory and label them:</p> <pre><code>kubectl --context its1 get managedclusters\nkubectl --context its1 label managedcluster cluster1 location-group=edge name=cluster1\nkubectl --context its1 label managedcluster cluster2 location-group=edge name=cluster2\n</code></pre> </li> </ol>"},{"location":"direct/get-started/#variables-for-running-the-example-scenarios","title":"Variables for running the example scenarios.","text":"<p>Before moving on to try exercising KubeStellar, you will need the following shell variable settings to inform the scenario commands about the configuration.</p> <pre><code>host_context=kind-kubeflex\nits_cp=its1\nits_context=its1\nwds_cp=wds1\nwds_context=wds1\nwec1_name=cluster1\nwec2_name=cluster2\nwec1_context=$wec1_name\nwec2_context=$wec2_name\nlabel_query_both=location-group=edge\nlabel_query_one=name=cluster1\n</code></pre>"},{"location":"direct/get-started/#exercise-kubestellar","title":"Exercise KubeStellar","text":"<p>Now that your system is running, you can try some example scenarios</p> <ol> <li> <p>Define the needed shell variables, using either the settings output as the script completes or the settings shown just above from the step-by-step instructions. Their meanings are defined at the start of the example scenarios document.</p> </li> <li> <p>Proceed to Scenario 1 (multi-cluster workload deployment with kubectl) in the example scenarios and/or other examples on the same page, after defining the shell variables that characterize the configuration created above.</p> </li> </ol>"},{"location":"direct/get-started/#next-steps","title":"Next Steps","text":"<p>The configuration created here was a basic one suitable for learning. The full Installation and Usage outline shows that KubeStellar has a lot of flexibility.</p> <ul> <li>Create Kubernetes clusters any way you want</li> <li>Multiple Inventory and Transport Spaces (ITS)</li> <li>Multiple Workload Definition Spaces (WDS)</li> <li>Dynamic addition and removal of ITSes</li> <li>Dynamic addition and removal of WDSes</li> <li>Use the KubeFlex hosting cluster or a KubeFlex Control Plane as ITS</li> <li>Use the KubeFlex hosting cluster or a KubeFlex Control Plane as WDS</li> <li>Dynamic addition and removal of Workload Execution Clusters (WECs)</li> </ul> <p>For general setup information, see the full story.</p>"},{"location":"direct/get-started/#troubleshooting","title":"Troubleshooting","text":"<p>In the event something goes wrong, check out the troubleshooting page to see if someone else has experienced the same thing</p>"},{"location":"direct/init-hosting-cluster/","title":"Initializing the KubeFlex hosting cluster","text":"<p>The KubeFlex implementation has to be installed in the cluster chosen to play the role of KubeFlex hosting cluster. This can be done in any of the following ways.</p>"},{"location":"direct/init-hosting-cluster/#bundled-with-cluster-creation","title":"Bundled with cluster creation","text":"<p>As mentioned earlier, there are a couple of ways to both create the hosting cluster and initialize it for KubeFlex in one operation.</p> <ul> <li>Using kflex init --create-kind.</li> <li>curl-to-bash script.</li> </ul>"},{"location":"direct/init-hosting-cluster/#kflex-init","title":"kflex init","text":"<p>The following command will install the KubeFlex implementation in the cluster that <code>kubectl</code> is configured to access, if you have sufficient privileges.</p> <pre><code>kflex init\n</code></pre>"},{"location":"direct/init-hosting-cluster/#using-an-existing-openshift-cluster-as-the-hosting-cluster","title":"Using an existing OpenShift cluster as the hosting cluster","text":"<p>When the hosting cluster is an OpenShift cluster, the recipe for registering a WEC with the ITS (to be written) needs to be modified. In the <code>clusteradm</code> command, omit the <code>--force-internal-endpoint-lookup</code> flag. If following Getting Started literally, this means to define <code>flags=\"\"</code> rather than <code>flags=\"--force-internal-endpoint-lookup\"</code>.</p>"},{"location":"direct/init-hosting-cluster/#kubestellar-core-helm-chart","title":"KubeStellar core Helm chart","text":"<p>The KubeStellar core Helm chart will install the KubeFlex implementation in the cluster that <code>kubectl</code> is configured to access, as well as create ITSes and WDSes.</p>"},{"location":"direct/installation-errors/","title":"Kind host not configured for more than two clusters","text":"<p>Kind uses a docker-in-docker technique to create multiple Kubernetes clusters on your host. But, in order for this to work for three or more clusters, the host running Docker typically needs an expanded configuration. This is mostly described in a known issue of kind. However, that document does not mention the additional complexity that arises when the OS running the containers is the guest OS inside a virtual machine on your host (e.g., a Mac, which does not natively run containers and so uses a virtual machine with a Linux guest OS).</p>"},{"location":"direct/installation-errors/#symptoms","title":"Symptoms","text":"<p>Many KubeStellar setup paths check for the needed configuration. When the check fails, you get an error message like the following.</p> <pre><code>sysctl fs.inotify.max_user_watches is only 155693 but must be at least 524288\n</code></pre> <p>If you avoid the check but the configuration is not expanded then the symptom will most likely be setup ceasing to make progress at some point. Or maybe other errors about things not happening or things not existing.</p>"},{"location":"direct/installation-errors/#solution","title":"Solution","text":"<p>To resolve this error, you need to increase the value of <code>fs.inotify.max_user_watches</code> and/or <code>fs.inotify.max_user_instances</code>. Follow the steps below:</p>"},{"location":"direct/installation-errors/#for-rancher-desktop","title":"For Rancher Desktop","text":"<ol> <li> <p>Open the configuration file:    <pre><code>vi \"~/Library/Application Support/rancher-desktop/lima/_config/override.yaml\"\n</code></pre></p> </li> <li> <p>Add the following script to the <code>provision</code> section:    <pre><code>provision:\n- mode: system\n  script: |\n    #!/bin/sh\n    sysctl fs.inotify.max_user_watches=524288\n    sysctl fs.inotify.max_user_instances=512\n</code></pre></p> </li> <li> <p>Restart Rancher Desktop.</p> </li> </ol>"},{"location":"direct/installation-errors/#docker-on-linux","title":"Docker on Linux","text":"<p>The resolution in the kind known issue can be used directly.</p> <ol> <li> <p>Create a new configuration file:    <pre><code>sudo vi /etc/sysctl.d/99-sysctl.conf\n</code></pre></p> </li> <li> <p>Add the following lines:    <pre><code>fs.inotify.max_user_watches=1048576\nfs.inotify.max_user_instances=1024\n</code></pre></p> </li> <li> <p>Apply the changes:    <pre><code>sudo sysctl -p /etc/sysctl.d/99-sysctl.conf\n</code></pre></p> </li> </ol>"},{"location":"direct/its/","title":"Inventory and Transport Spaces","text":"<ul> <li>What is an ITS?</li> <li>Creating an ITS</li> <li>Using the KubeStellar Core Helm Chart</li> <li>Using the KubeFlex CLI</li> <li>KubeFlex Hosting Cluster as ITS</li> <li>Important Note on ITS Registration</li> <li> <p>Architecture and Components An Inventory and Transport Space (ITS) is a core component of the KubeStellar architecture that serves two primary functions:</p> </li> <li> <p>Inventory Management: It maintains a registry of all Workload Execution Clusters (WECs) available in the system.</p> </li> <li>Transport Facilitation: It handles the movement of workloads from Workload Description Spaces (WDSes) to the appropriate WECs.</li> </ul>"},{"location":"direct/its/#what-is-an-its","title":"What is an ITS?","text":"<p>An ITS is a space (a Kubernetes-like API server with storage) that:</p> <ul> <li>Holds inventory information about all registered WECs using ManagedCluster.v1.cluster.open-cluster-management.io objects</li> <li>Contains a \"customization-properties\" namespace with ConfigMaps carrying additional properties for each WEC</li> <li>Manages mailbox namespaces that correspond 1:1 with each WEC, holding ManifestWork objects</li> <li>Runs the OCM (Open Cluster Management) Cluster Manager to synchronize objects with the WECs</li> </ul>"},{"location":"direct/its/#creating-an-its","title":"Creating an ITS","text":"<p>An ITS can be created in several ways:</p>"},{"location":"direct/its/#using-the-kubestellar-core-helm-chart","title":"Using the KubeStellar Core Helm Chart","text":"<p>The recommended approach is to use the KubeStellar Core Chart:</p> <pre><code>helm upgrade --install ks-core oci://ghcr.io/kubestellar/kubestellar/core-chart \\\n  --set-json='ITSes=[{\"name\":\"its1\", \"type\":\"vcluster\"}]'\n</code></pre> <p>You can customize your ITS by specifying: - <code>name</code>: A unique name for the ITS - <code>type</code>:    - <code>vcluster</code> (default): Creates a virtual cluster   - <code>host</code>: Uses the KubeFlex hosting cluster itself   - <code>external</code>: Uses an external cluster - <code>install_clusteradm</code>: <code>true</code> (default) or <code>false</code> to control OCM installation</p>"},{"location":"direct/its/#using-the-kubeflex-cli","title":"Using the KubeFlex CLI","text":"<p>You can also create an ITS using the KubeFlex CLI:</p> <pre><code>kflex create its1 --type vcluster -p ocm\n</code></pre>"},{"location":"direct/its/#kubeflex-hosting-cluster-as-its","title":"KubeFlex Hosting Cluster as ITS","text":"<p>The KubeFlex hosting cluster can be configured to act as an ITS by specifying <code>type: host</code> when creating the ITS:</p> <pre><code>helm upgrade --install ks-core oci://ghcr.io/kubestellar/kubestellar/core-chart \\\n  --set-json='ITSes=[{\"name\":\"its1\", \"type\":\"host\"}]'\n</code></pre> <p>This approach: - Avoids creating a separate virtual cluster - Simplifies the architecture by reusing the hosting cluster - Makes the ITS directly accessible through the hosting cluster's API server</p>"},{"location":"direct/its/#important-note-on-its-registration","title":"Important Note on ITS Registration","text":"<p>Creating an ITS includes installing the relevant OCM (Open Cluster Management) machinery in it. However, registering the ITS as a KubeFlex control plane is a separate step that happens automatically when using the Core Helm Chart or KubeFlex CLI with the appropriate parameters.</p>"},{"location":"direct/its/#architecture-and-components","title":"Architecture and Components","text":"<p>The ITS runs the OCM Cluster Manager, which: - Accepts registrations from WECs through the OCM registration agent - Manages the distribution of workloads to WECs - Maintains status information from the WECs - Creates and manages mailbox namespaces for each registered WEC</p>"},{"location":"direct/known-issues/","title":"Some known problems","text":"<p>Here are some user and/or environment problems that we have seen.</p> <p>For bugs, see the issues on GitHub and the release notes.</p>"},{"location":"direct/known-issues/#wrong-value-stuck-in-hidden-kflex-state-in-kubeconfig","title":"Wrong value stuck in hidden kflex state in kubeconfig","text":"<p>The symptom is <code>kflex ctx ...</code> commands failing. See Confusion due to hidden state in your kubeconfig.</p>"},{"location":"direct/known-issues/#kind-clusters-failing-to-work","title":"Kind clusters failing to work","text":"<p>The symptom is <code>kind</code> cluster(s) that get created but fail to get their job done. See Potential Error with Kubestellar Installation related to Issues with Kind backed by Rancher Desktop.</p>"},{"location":"direct/known-issues/#authorization-fail-for-helm-fetching-chart-from-ghcr","title":"Authorization fail for Helm fetching chart from ghcr","text":"<p>The symptom is that attempting to instantiate the core Helm chart gets an authorization failure. See Authorization failure while fetching Helm chart from ghcr.io.</p>"},{"location":"direct/known-issues/#missing-results-in-a-combinedstatus-object","title":"Missing results in a CombinedStatus object","text":"<p>The symptom is a missing entry in the <code>results</code> of a <code>CombinedStatus</code> object. See Missing results in a CombinedStatus object.</p>"},{"location":"direct/known-issues/#kind-host-not-configured-for-more-than-two-clusters","title":"Kind host not configured for more than two clusters","text":"<p>This can arise when using <code>kind</code> inside a virtual machine (e.g., when using Docker on a Mac). The symptom is either a complaint from KubeStellar setup that <code>sysctl fs.inotify.max_user_watches is only 155693 but must be at least 524288</code> or setup grinding to a halt. See Kind host not configured for more than two clusters.</p>"},{"location":"direct/known-issues/#insufficient-cpu-for-your-clusters","title":"Insufficient CPU for your clusters","text":"<p>This can happen when you are using a docker-in-docker technique. The symptom is that setup stops making progress at some point. See Insufficient CPU for your clusters</p>"},{"location":"direct/knownissue-collector-miss/","title":"Missing results in a CombinedStatus object","text":""},{"location":"direct/knownissue-collector-miss/#description-of-the-issue","title":"Description of the Issue","text":"<p>A <code>CombinedStatus</code> object, which is specific to one <code>Binding</code> (<code>BindingPolicy</code>) and one workload object, lacks an entry in <code>results</code> for some <code>StatusCollector</code> whose name is associated with the workload object by the <code>Binding</code>.</p>"},{"location":"direct/knownissue-collector-miss/#root-cause","title":"Root Cause","text":"<p>There is no <code>StatusCollector</code> object with the name given in the <code>Binding</code>.</p> <p>This could be because of a typo in the <code>Binding</code> or because something failed to create the intended <code>StatusCollector</code>.</p>"},{"location":"direct/knownissue-cpu-insufficient-for-its1/","title":"Insufficient CPU for your clusters","text":"<p>When following Getting Started, you may find that it hangs at some point --- simply stops making progress. For example: after instantiating the core Helm chart, a <code>kflex ctx</code> command may grind to a halt with the following output and no more.</p> <pre><code>$ kflex ctx --overwrite-existing-context its1\nno kubeconfig context for its1 was found: context its1 not found for control plane its1\n\u2714 Overwriting existing context for control plane\ntrying to load new context its1 from server...\n</code></pre>"},{"location":"direct/knownissue-cpu-insufficient-for-its1/#root-cause","title":"Root cause","text":"<p>You are using <code>kind</code>, <code>k3d</code>, GitHub Codespaces, or any other docker-in-docker based technique and your host does not have enough CPU for all of your clusters.</p>"},{"location":"direct/knownissue-cpu-insufficient-for-its1/#resolution","title":"Resolution","text":""},{"location":"direct/knownissue-cpu-insufficient-for-its1/#general","title":"General","text":"<p>Stop any irrelevant containers.</p>"},{"location":"direct/knownissue-cpu-insufficient-for-its1/#macos","title":"MacOS","text":"<p>On Mac computers, Docker runs all of your containers in a virtual machine. Examples of things that do this include Docker Desktop, Rancher desktop, and colima. You may need to increase the CPU allocated to this virtual machine.</p> <p>For example, the colima default VM is configured to use <code>--cpu 2 --memory 4</code> --- which is insufficient for Kubestellar components on KinD clusters. In fact, KinD inherit colima resources when created.</p> <p>To solve this issue, increase colima resource capacity to increase KinD clusters resource capcity:</p> <ol> <li>Stop colima VM</li> </ol> <pre><code>colima stop\n</code></pre> <ol> <li>Increase colima cpu and memory capacity</li> </ol> <pre><code>colima start --cpu 4 --memory 8\n</code></pre> <ol> <li>Delete kind cluster created by the tutorial, and start over again on a clean state.</li> </ol>"},{"location":"direct/knownissue-helm-ghcr/","title":"Authorization failure while fetching Helm chart from ghcr.io","text":""},{"location":"direct/knownissue-helm-ghcr/#description-of-the-issue","title":"Description of the Issue","text":"<p>When following the Getting Started recipe you might get a failure from the command to instantiate KubeStellar's core Helm chart. The error message is as follows.</p> <p>Error: failed to authorize: failed to fetch oauth token: unexpected status from GET request to https://ghcr.io/token?scope=repository%3Akubestellar%2Fkubestellar%2Fcore-chart%3Apull&amp;service=ghcr.io: 403 Fobidden</p> <p>This is Issue 2544.</p>"},{"location":"direct/knownissue-helm-ghcr/#root-cause","title":"Root Cause","text":"<p>Following is one root cause that is partly understood. There may be others.</p> <p>The cause is the user having a broken configuration for Docker. Even though <code>helm</code> does not itself use containers, <code>helm</code> will consult the user's Docker configuration file (<code>~/.docker/config.json</code>) for registry credentials if that file exists.</p> <p>Fetching a Helm chart from an OCI registry can involve getting a temporary token. For a private Helm chart, registry credentials are required in order to get that temporary token; for a public Helm chart, registry credentials are not needed.  Even though fetching a public Helm chart does not require registry credentials, <code>helm</code> tries to get and use credentials for the <code>ghcr.io</code> registry if that Docker configuration file exists.  When that file exists but specifies something that does not work, that can lead to an error message about an authorization failure in the request to get the temporary token.</p> <p>This pathology is discussed in an Issue in the Helm repository on GitHub.</p> <p>For an example, consider the case of someone using Rancher Desktop on Linux. The installation instructions for Rancher Desktop, in the Linux case, recommend installing and initializing a package named \"pass\". This is explained in more detail in a linked document. If the user does not install and initialize \"pass\" then Docker's handling of registry credentials will be messed up.</p>"},{"location":"direct/knownissue-helm-ghcr/#testing-whether-helm-can-fetch-public-charts","title":"Testing whether Helm can fetch public charts","text":"<p>To test whether the problem is breakage in helm/docker, try the command <code>helm show chart oci://ghcr.io/kubestellar/kubestellar/core-chart</code>. If that fails all by itself, the problem is in Helm or something that it uses.</p>"},{"location":"direct/knownissue-helm-ghcr/#resolution-for-lack-of-pass","title":"Resolution for lack of \"pass\"","text":"<p>Install and initialize the package named \"pass\".</p>"},{"location":"direct/knownissue-helm-ghcr/#workarounds","title":"Workarounds","text":"<p>If the resolution above does not work then you can try doing the KubeStellar setup as a different user --- an ordinary user or root (but remember that unnecessary use of root is a security risk). When the problem is caused by the user's Docker config file, a different user's Docker config file might not have the problem. Also, as noted in the GitHub Issue, <code>helm</code> will succeed at fetching public charts if the user does NOT have a Docker config file.</p> <p>Another way to work around a broken Docker config file is to temporarily remove or rename it while doing the KubeStellar setup. The KubeStellar setup does not require credentials for any registry --- except for pull rate limit considerations. The KubeStellar setup does involve using some images from DockerHub, and DockerHub imposes a strict rate limit on non-logged-in users.</p>"},{"location":"direct/knownissue-kflex-extension/","title":"Confusion due to hidden state in your kubeconfig","text":"<p>The <code>kflex</code> command maintains and works with a bit of state hidden in your kubeconfig file. This is where KubeFlex stashes the name of the kubeconfig context to use for accessing the KubeFlex hosting cluster. Following is an example of examining that state.</p> <pre><code>mspreitz@mjs13 kubestellar % yq .preferences ${KUBECONFIG:-$HOME/.kube/config}\nextensions:\n  - extension:\n      data:\n        kflex-initial-ctx-name: kscore-stage\n      metadata:\n        creationTimestamp: null\n        name: kflex-config-extension-name\n    name: kflex-config-extension-name\n</code></pre> <p>The <code>kflex ctx</code> commands are normally hesitant to replace a bad value in there. Later releases of <code>kflex</code> are better than older ones, and the latest releases have ways on the command line to explicitly remove this hesitancy; the KubeStellar instructions and scripts use those.</p> <p>Although it should no longer be necessary to use this, the following command shows a way to remove that bit of hidden state; after this, a <code>kflex ctx</code> command will succeed if your current kubeconfig context is the one to use for accessing the KubeFlex hosting cluster.</p> <pre><code>yq -i 'del(.preferences)' ${KUBECONFIG:-$HOME/.kube/config}\n</code></pre>"},{"location":"direct/knownissue-kind-config/","title":"Potential Error with Kubestellar Installation related to Issues with Kind backed by Rancher Desktop","text":""},{"location":"direct/knownissue-kind-config/#description-of-the-issue","title":"Description of the Issue","text":"<p>Kubestellar installation may fail for some users during the setup of the second cluster (cluster2) when running Kind with Docker provided by Rancher Desktop. The failure occurs while initializing the cluster, with an error related to <code>kubeadm</code>. Insufficient system parameter settings (<code>sysctl</code>) within the Rancher Desktop virtual machine may be causing this issue.</p>"},{"location":"direct/knownissue-kind-config/#error-message-example","title":"Error Message Example","text":"<pre><code>Error: hub oriented command should not running against non-hub cluster\nCreating cluster \"cluster2\" ...\n...\nERROR: failed to create cluster: failed to init node with kubeadm: command \"docker exec --privileged cluster2-control-plane kubeadm init --skip-phases=preflight --config=/kind/kubeadm.conf --skip-token-print --v=6\" failed with error: exit status 1\nCommand Output: I1008 16:11:20.743111 134 initconfiguration.go:255] loading configuration from \"/kind/kubeadm.conf\"\n...\n[config] WARNING: Ignored YAML document with GroupVersionKind kubeadm.k8s.io/v1beta3, Kind=JoinConfiguration\n...\n</code></pre>"},{"location":"direct/knownissue-kind-config/#root-cause","title":"Root Cause","text":"<p>This is caused by a known issue with kind.</p> <p>When using Rancher Desktop, the Linux machine that needs to be reconfigured is a virtual machine that Rancher Desktop is managing.</p> <p>Kind requires the following minimum settings in the Linux machine:</p> <pre><code>fs.inotify.max_user_watches = 524288\nfs.inotify.max_user_instances = 512\n</code></pre> <p>If these parameters are set lower than the suggested values, the second cluster initialization may fail.</p>"},{"location":"direct/knownissue-kind-config/#steps-to-reproduce-the-issue","title":"Steps to Reproduce the Issue","text":"<ol> <li>Install Rancher Desktop</li> <li>Download and install Rancher Desktop from the official website.</li> <li>Configure it to use Docker as the container runtime (<code>dockerd</code>).</li> <li>Install Kind</li> <li>Follow the installation instructions provided in the Kind documentation.</li> <li>Install Kubestellar Prerequisites</li> <li>Ensure that all required dependencies for Kubestellar are installed on your system. Refer to the Kubestellar documentation for a complete list.</li> <li>Run the Kubestellar Getting Started Guide or Demo Environment Setup Script</li> <li>Follow the steps in the Kubestellar Getting Started guide or run the automated demo environment setup script.</li> <li>Monitor the Installation Process</li> <li>Confirm the successful installation of kubeflex.</li> <li>Ensure that ITS1 (Information Transformation Service 1) and WDS1 (Workload Distribution Service 1) are created.</li> <li>Verify the creation of the first cluster (cluster1).</li> <li>Wait for the Creation of Cluster2</li> <li>Allow the script to attempt the creation of the second remote cluster (cluster2).</li> <li>The error should occur during this step if the issue is present.</li> </ol>"},{"location":"direct/knownissue-kind-config/#expected-behavior","title":"Expected Behavior","text":"<p>Cluster 2 should create successfully, and the installation should complete without errors.</p>"},{"location":"direct/knownissue-kind-config/#steps-to-fix","title":"Steps to Fix","text":"<ol> <li>Check Current <code>sysctl</code> Parameter Values<ul> <li>Use the command <code>rdctl shell</code> to log in to the Rancher Desktop VM.   Run:     <pre><code>sysctl fs.inotify.max_user_watches\nsysctl fs.inotify.max_user_instances\n</code></pre></li> <li>Confirm if these values are below the recommended settings (524288 for max_user_watches and 512 for max_user_instances).</li> </ul> </li> <li>Modify the Parameter Settings<ul> <li>Setting these parameters temporarily with <code>sysctl</code> will revert after restarting Rancher Desktop. To persist the changes, you need to modify the configuration using an overlay file.</li> </ul> </li> <li> <p>Create an Override Configuration File</p> <ul> <li> <p>On a Mac:</p> <ul> <li> <p>Open a terminal and create a new file:</p> <pre><code>vi ~/Library/Application\\ Support/rancher-desktop/lima/_config/override.yaml\n</code></pre> </li> <li> <p>Add the following content:</p> <pre><code>provision:\n- mode: system\n  script: |\n    #!/bin/sh\n    echo \"fs.inotify.max_user_watches=524288\" &gt; /etc/sysctl.d/fs.inotify.conf\n    echo \"fs.inotify.max_user_instances=512\" &gt;&gt; /etc/sysctl.d/fs.inotify.conf\n    sysctl -p /etc/sysctl.d/fs.inotify.conf\n</code></pre> </li> <li> <p>Save the file.</p> </li> <li>Restart Rancher Desktop<ul> <li>Restart Rancher Desktop for the changes to take effect and ensure the new <code>sysctl</code> parameter values persist.</li> </ul> </li> <li>Delete Existing Kind Clusters<ul> <li>Before re-running the Kubestellar Getting Started guide, delete all previously created clusters:</li> </ul> </li> </ul> <pre><code>kind delete cluster --name &lt;cluster-name&gt;\n</code></pre> </li> <li> <p>Repeat for each cluster (e.g., kubeflex, cluster1, cluster2).</p> </li> <li>Re-run the Kubestellar Setup</li> <li>With the updated configuration, run the Kubestellar Getting Started guide or the automated demo environment script again.</li> <li>Verify that both clusters are created successfully without errors.</li> </ul> </li> </ol>"},{"location":"direct/knownissue-kind-config/#additional-note-ensuring-a-clean-environment-for-reinstallation","title":"Additional Note: Ensuring a Clean Environment for Reinstallation","text":"<p>Deleting all existing Kind clusters before re-running the installation ensures no leftover configurations interfere with the new setup.</p>"},{"location":"direct/kubeflex-intro/","title":"KubeFlex","text":"<p>One of the technologies underlying KubeStellar is KubeFlex, a kubernetes-based platform designed to:</p> <ul> <li>Provide lightweight Kube API Server instances and selected controllers as a service.</li> <li>Provide a flexible architecture for the storage backend</li> <li>Offer flexibility in choice of API Server build</li> <li>Present a single binary command line interface for improved user experience</li> </ul> <p>KubeFlex is a flexible framework that supports various kinds of control planes, such as:</p> <ul> <li> <p>k8s: a basic Kubernetes API Server with a subset of kube controllers.  The control plane in this context does not execute workloads, such as pods,  because the controllers associated with these objects are not activated.  This environment is referred to as \u2018denatured\u2019 because it lacks the typical  characteristics and functionalities of a standard Kubernetes cluster It uses about 350 MB of memory per instance with a shared Postgres Database Backend.</p> </li> <li> <p>vcluster: a virtual cluster that runs on the hosting cluster,  based on the  vCluster Project. This type of control  plane can run pods using worker nodes of the hosting cluster.</p> </li> <li> <p>host: the KubeFlex hosting cluster, which is exposed as a control plane.</p> </li> <li> <p>external: an external cluster that is imported as a control plane (this is in the roadmap but not yet implemented)</p> </li> <li> <p>ocm: a control plane that uses the  multicluster-controlplane project  for managing multiple clusters.</p> </li> </ul> <p>When using KubeFlex, users interact with the API server of the hosting cluster to create or delete control planes. KubeFlex defines a ControlPlane CRD that represents a Control Plane.</p>"},{"location":"direct/kubeflex-intro/#learn-more","title":"Learn More","text":"<p>To explore more fully KubeFlex's capabilities visit the repository at https://github.com/kubestellar/kubeflex</p> <p>There is also a introductory video about KubeFlex on the KubeStellar YouTube Channel</p> <p> image info </p>"},{"location":"direct/observability/","title":"Observability in KubeStellar","text":"<p>KubeStellar provides endpoints and integrations for observability and monitoring. This page describes the available observability features, how to access them, and how to use them in a typical deployment.</p>"},{"location":"direct/observability/#metrics-endpoints","title":"Metrics Endpoints","text":"<p>KubeStellar controllers expose Prometheus-compatible metrics endpoints. These endpoints respond to HTTP requests for metrics and can be queried by any monitoring system; KubeStellar does not mandate how metrics are collected or scraped. For an example of collecting both metrics and debug endpoint data using Prometheus, see the monitoring directory. This is just one possible approach; KubeStellar does not require or mandate any specific monitoring tool or method.</p>"},{"location":"direct/observability/#metrics-endpoint-table","title":"Metrics Endpoint Table","text":"Controller Protocol Port Path AuthN/AuthZ Notes kubestellar-controller-manager HTTPS 8443 /metrics Kubernetes client authentication required when using the Service Service: <code>kubestellar-controller-manager-metrics-service</code> (default port, configurable via Helm values). kubestellar-controller-manager HTTPS 8443 /metrics None (direct pod access) Access via Pod port 8443 (default, configurable via Helm values). ks-transport-controller HTTP 8090 /metrics None (in-cluster) Default port, configurable via Helm values. status-addon-controller HTTP 9280 /metrics None (in-cluster) Default port, configurable via Helm values. status-agent-controller HTTP 8080 /metrics None (in-cluster) Default port, configurable via Helm values. <p>Note: The ports listed above are defaults. In the core Helm chart, the ports to use are configured in the chart's values. When using the Service for <code>kubestellar-controller-manager</code>, protocol is HTTPS and Kubernetes client authentication is required. Direct pod access may use HTTP.</p>"},{"location":"direct/observability/#debugprofiling-endpoints","title":"Debug/Profiling Endpoints","text":"<p>Some KubeStellar components expose Go's built-in pprof debug endpoints for profiling and troubleshooting.</p>"},{"location":"direct/observability/#pprof-endpoint-table","title":"pprof Endpoint Table","text":"Controller Protocol Port Path AuthN/AuthZ Notes kubestellar-controller-manager HTTP 8082 /debug/pprof/ None ks-transport-controller HTTP 8092 /debug/pprof/ None status-addon-controller HTTP 9282 /debug/pprof/ None status-agent-controller HTTP 8082 /debug/pprof/ None"},{"location":"direct/observability/#example-accessing-metrics-and-debug-endpoints","title":"Example: Accessing Metrics and Debug Endpoints","text":"<p>Note: The following example assumes you have a running KubeStellar controller-manager pod and access to the appropriate Kubernetes context and namespace. The Deployment name is always <code>kubestellar-controller-manager</code>, but you may need to adjust the context and namespace for your environment.</p> <pre><code>kubectl --context kind-kubeflex port-forward -n wds1-system deployment/kubestellar-controller-manager 8443:8443 8082:8082\n</code></pre> <p>Access metrics: https://localhost:8443/metrics (Kubernetes client authentication required)</p> <p>Access pprof: http://localhost:8082/debug/pprof/</p>"},{"location":"direct/observability/#grafana-dashboards","title":"Grafana Dashboards","text":"<ul> <li>Example Grafana dashboards and configuration can be found in <code>monitoring/grafana/</code>.</li> <li>After deploying Prometheus and Grafana (or your preferred stack), you can import dashboards to visualize KubeStellar metrics.</li> </ul>"},{"location":"direct/observability/#additional-resources","title":"Additional Resources","text":"<ul> <li>KubeStellar Monitoring (one possible way to collect metrics and profiles)</li> <li>Prometheus Operator Documentation</li> <li>Grafana Documentation</li> <li>Go pprof Documentation</li> </ul> <p>If you have suggestions for more observability features or documentation, please open an issue.</p>"},{"location":"direct/packaging/","title":"Packaging and Delivery","text":""},{"location":"direct/packaging/#outline-of-github-repositories","title":"Outline of GitHub repositories","text":"<p>The following is a graph of the GitHub repositories in the <code>kubestellar</code> GitHub organization and the dependencies among them. The repo at the tail of an arrow depends on the repo at the head of the arrow. These are not just build-time dependencies but any reference from one repo to another.</p> <pre><code>flowchart LR\n    kubestellar --&gt; kubeflex\n    kubestellar --&gt; ocm-status-addon\n    ocm-status-addon --&gt; kubestellar</code></pre> <p>The references from ocm-status-addon to kubestellar are only in documentation and are in the process of being removed (no big difficulty is anticipated).</p>"},{"location":"direct/packaging/#kubeflex","title":"KubeFlex","text":"<p>See the GitHub repo.</p>"},{"location":"direct/packaging/#ocm-status-addon","title":"OCM Status Addon","text":"<p>The OCM Status Addon repo is the source of an Open Cluster Management Addon. It builds one image that has two subcommands that tell it which role to play in that framework: the controller (which runs in the OCM hub, the KubeStellar ITS) or the agent.</p>"},{"location":"direct/packaging/#outline-of-ocm-status-addon-publishing","title":"Outline of OCM status addon publishing","text":"<pre><code>flowchart LR\n    subgraph \"ocm-status-addon@GitHub\"\n    osa_code[OSA source code]\n    osa_hc_src[OSA Helm chart source]\n    end\n    osa_ctr_image[OSA container image] --&gt; osa_code\n    osa_hc_repo[published OSA Helm Chart] --&gt; osa_hc_src\n    osa_hc_src -.-&gt; osa_ctr_image\n    osa_hc_repo -.-&gt; osa_ctr_image</code></pre> <p>The dashed dependencies are at run time, not build time.</p> <p>\"OSA\" is OCM Status Addon.</p>"},{"location":"direct/packaging/#ocm-status-addon-container-image","title":"OCM status addon container image","text":"<p>There is a container image at ghcr.io/kubestellar/ocm-status-addon. This image can operate as either controller or agent.</p> <p>In its capacity as controller, the code in this image can emit YAML for a Deployment object that runs the OCM Status Add-On Agent. The compiled code has an embedded copy of <code>pkg/controller/manifests</code>, which includes the YAML source for the agent Deployment.</p> <p>The container image is built and published by that repository's release process, which is documented at its <code>docs/release.md</code> file.</p> <p>By our development practices and not doing any manual hacks, we maintain the association that a container image tagged with <code>$VERSION</code> is built from the Git commit that has the Git tag <code>v$VERSION</code>.</p> <p>To support testing, <code>make ko-local-build</code> will build a single-platform image and not push it, only leave it among your Docker images. The single platform's OS is Linux. The single platform's ISA is defined by the <code>make</code> variable <code>ARCH</code>, which defaults to what <code>go env GOARCH</code> prints.</p>"},{"location":"direct/packaging/#ocm-status-addon-helm-chart","title":"OCM status addon Helm chart","text":"<p>The OCM Status Add-On Controller is delivered by a Helm chart at ghcr.io/kubestellar/ocm-status-addon-chart. The chart references the container image.</p> <p>By our development practices and doing doing any manual hacks, we maintain the association that the OCI image tagged <code>v$VERSION</code> contains a Helm chart that declares its <code>version</code> and its <code>appVersion</code> to be <code>v$VERSION</code> and the templates in that chart include a Deployment for the OCM Status Add-On Agent using the container image <code>ghcr.io/kubestellar/ocm-status-addon:$VERSION</code>.</p>"},{"location":"direct/packaging/#ocm-transport-plugin","title":"OCM Transport Plugin","text":"<p>This repository (github.com/kubestellar/ocm-transport-plugin) is retired. Its contents have been merged into the kubestellar repository.</p> <p>The primary product was the OCM Transport Controller, which is built from generic transport controller code plus code specific to using OCM for transport. This controller now comes from the ks/ks repository. The published artifacts for this controller from ks/OTP, which still linger because older releases of KubeStellar are still in use and because GitHub is all about not forgetting things, are as follows. DO NOT USE THEM with releases of KubeStellar after <code>0.24.0-alpha.2</code>.</p> <ul> <li>OCM Transport Controller container image. Appears at ghcr.io/kubestellar/ocm-transport-plugin/transport-controller.</li> <li>OCM Transport Controller Helm chart. Appears at ghcr.io/kubestellar/ocm-transport-plugin/chart/ocm-transport-plugin.</li> </ul>"},{"location":"direct/packaging/#kubestellar","title":"KubeStellar","text":""},{"location":"direct/packaging/#warning","title":"WARNING","text":"<p>Literal KubeStellar release numbers appear here, and are historical. The version of this document in a given release does not mention that release. See the release process for more details on what self-references are and are not handled.</p>"},{"location":"direct/packaging/#outline-of-publishing","title":"Outline of publishing","text":"<p>The following diagram shows most of it. For simplicity, this omits the clusteradm and the Helm CLI container images.</p> <pre><code>flowchart LR\n    osa_hc_repo[published OSA Helm Chart]\n    subgraph ks_repo[\"kubestellar@GitHub\"]\n    kcm_code[KCM source code]\n    otc_code[OTC source code]\n    ksc_hc_src[KS Core Helm chart source]\n    setup_ksc[\"'Getting Started' setup\"]\n    e2e_local[\"E2E setup&lt;br&gt;local\"]\n    e2e_release[\"E2E setup&lt;br&gt;release\"]\n    end\n    kcm_ctr_image[KCM container image] --&gt; kcm_code\n    otc_ctr_image[OTC container image]\n    otc_ctr_image --&gt; otc_code\n    ksc_hc_repo[published KS Core chart] --&gt; ksc_hc_src\n    ksc_hc_src -.-&gt; osa_hc_repo\n    ksc_hc_src -.-&gt; otc_ctr_image\n    ksc_hc_src -.-&gt; kcm_ctr_image\n    ksc_hc_repo -.-&gt; osa_hc_repo\n    ksc_hc_repo -.-&gt; otc_ctr_image\n    ksc_hc_repo -.-&gt; kcm_ctr_image\n    setup_ksc -.-&gt; ksc_hc_repo\n    setup_ksc -.-&gt; KubeFlex\n    e2e_local -.-&gt; ksc_hc_src\n    e2e_local -.-&gt; KubeFlex\n    e2e_release -.-&gt; ksc_hc_repo\n    e2e_release -.-&gt; KubeFlex</code></pre> <p>The following diagram shows the parts involving the clusteradm and Helm CLI container images.</p> <pre><code>flowchart LR\n    subgraph helm_repo[\"helm/helm@GitHub\"]\n    helm_src[\"helm source\"]\n    end\n    subgraph cladm_repo[\"ocm/clusteradm@GitHub\"]\n    cladm_src[\"clusteradm source\"]\n    end\n    subgraph ks_repo[\"kubestellar@GitHub\"]\n    ksc_hc_src[KS Core Helm chart source]\n    e2e_local[\"E2E setup&lt;br&gt;local\"]\n    e2e_release[\"E2E setup&lt;br&gt;release\"]\n    end\n    helm_image[\"ks/helm image\"] --&gt; helm_src\n    cladm_image[\"ks/clusteradm image\"] --&gt; cladm_src\n    ksc_hc_repo[published KS Core chart] --&gt; ksc_hc_src\n    ksc_hc_src -.-&gt; helm_image\n    ksc_hc_src -.-&gt; cladm_image\n    ksc_hc_repo -.-&gt; cladm_image\n    ksc_hc_repo -.-&gt; helm_image\n    e2e_local -.-&gt; ksc_hc_src\n    e2e_release -.-&gt; ksc_hc_repo</code></pre> <p>The dashed dependencies are at run time, not build time.</p> <p>\"KCM\" is the KubeStellar controller-manager.</p> <p>NOTE: among the references to published artifacts, some have a   version that is maintained in Git while others have a placeholder in   Git that is replaced in the publishing process. See the release   document for more details. This is an on-going matter   of development.</p>"},{"location":"direct/packaging/#local-copy-of-kubestellar-git-repo","title":"Local copy of KubeStellar git repo","text":"<p>NOTE: Because of a restriction in one of the code generators that we use, a contributor needs to have their local copy of the git repo in a directory whose pathname ends with the Go package name --- that is, ends with <code>/github.com/kubestellar/kubestellar</code>.</p>"},{"location":"direct/packaging/#derived-files","title":"Derived files","text":"<p>Some files in the kubestellar repo are derived from other files there. Contributors are responsible for invoking the commands to (re)derive the derived files as necessary.</p> <p>Some of these derived files are derived by standard generators from the Kubernetes milieu. A contributor can use the following command to make all of those, or use the individual <code>make</code> commands described in the following subsubsections to update particular subsets.</p> <pre><code>make all-generated\n</code></pre> <p>The following command, which we aspire to check in CI, checks whether all those derived files have been correctly derived. It must be invoked in a state where the <code>git status</code> is clean, or at least the dirty files are irrelevant; the current commit is what is checked. This command has side-effects on the filesystem like <code>make all-generated</code>.</p> <pre><code>hack/verify-codegen.sh\n</code></pre>"},{"location":"direct/packaging/#files-generated-by-controller-gen","title":"Files generated by controller-gen","text":"<ul> <li> <p><code>make manifests</code> generates the CustomResourceDefinition files,   which exist in two places:   <code>config/crd/bases</code> and   <code>pkg/crd/files</code>.</p> </li> <li> <p><code>make generate</code> generates the deep copy code, which exists in   <code>zz_generated.deepcopy.go</code> next to the API source.</p> </li> </ul>"},{"location":"direct/packaging/#files-generated-by-code-generator","title":"Files generated by code-generator","text":"<p>The files in <code>pkg/generated</code> are generated by k/code-generator. This generation is done at development time by the command <code>make codegenclients</code>.</p>"},{"location":"direct/packaging/#kubestellar-controller-manager-container-image","title":"KubeStellar controller-manager container image","text":"<p>KubeStellar has one container image, for what is called the KubeStellar controller-manager. For each WDS, KubeStellar has a pod running that image. It installs the needed custom resource definition objects if they are not already present, and is a controller-manager hosting the per-WDS controllers (binding controller and status controller) from the kubestellar repo.</p> <p>The image repository is <code>ghcr.io/kubestellar/kubestellar/controller-manager</code>.</p> <p>By our development practices and not doing any manual hacking we maintain the association that the container image tagged <code>$VERSION</code> is built from the Git commit having the Git tag <code>v$VERSION</code>.</p> <p>The release process builds and publishes that container image.</p> <p><code>make ko-build-controller-manager-local</code> will make a local image for just the local platform. This is used in local testing.</p>"},{"location":"direct/packaging/#ocm-transport-controller-container-image","title":"OCM Transport Controller container image","text":"<p>The release process builds and publishes this image at ghcr.io/kubestellar/kubestellar/ocm-transport-controller.</p> <p>By our development practices and not doing any manual hacking we maintain the association that the container image tagged <code>$VERSION</code> is built from the Git commit having the Git tag <code>v$VERSION</code>.</p>"},{"location":"direct/packaging/#clusteradm-container-image","title":"clusteradm container image","text":"<p>The kubestellar GitHub repository has a script, <code>hack/build-clusteradm-image.sh</code>, that creates and publishes a container image holding the <code>clusteradm</code> command from OCM. The source of the container image is read from the latest release of github.com/open-cluster-management-io/clusteradm, unless a command line flag says to use a specific version. This script also pushes the built container image to quay.io/kubestellar/clusteradm using a tag that equals the ocm/clusteradm version that the image was built from.</p> <p>This image is used by the core Helm chart to initialize an ITS as an Open Cluster Management hub.</p>"},{"location":"direct/packaging/#helm-cli-container-image","title":"Helm CLI container image","text":"<p>The container image at <code>quay.io/kubestellar/helm:3.14.0</code> was built by <code>hack/build-helm-image.sh</code>.</p>"},{"location":"direct/packaging/#kubestellar-core-helm-chart","title":"KubeStellar core Helm chart","text":"<p>This Helm chart is instantiated in a pre-existing Kubernetes cluster and (1) makes it into a KubeFlex hosting cluster and (2) sets up a requested collection of WDSes and ITSes. See the core chart doc. This chart is defined in the <code>core-chart</code> directory and published to <code>ghcr.io/kubestellar/kubestellar/core-chart</code>.</p> <p>The chart's <code>templates/</code> generate KubeFlex <code>ControlPlane</code> objects for the ITSes and WDSes specified in the chart's \"values\". These use the PostCreateHooks discussed below, which are also sensitive to a variety of settings in the chart's values. A PostCreateHook is cluster-scoped.</p> <p>This Helm chart defines and uses two KubeFlex PostCreateHooks in the KubeFlex hosting cluster, as follows.</p> <ul> <li> <p><code>its</code> defines a Job with two containers. One container uses the clusteradm container image to initialize the target cluster as an OCM \"hub\". The other container uses the Helm CLI container image to instantiate the OCM Status Addon Helm chart. The version to use is defined in the <code>values.yaml</code> of the core chart. This PostCreateHook is used for every requested ITS.</p> </li> <li> <p><code>wds</code> defines two <code>Deployment</code> objects and supporting RBAC   objects. One <code>Deployment</code> runs the KubeStellar   controller-manager. The other runs the OCM transport   controller. Each uses a container image repo in   <code>ghcr.io/kubestellar/kubestellar</code>, with an image tag specified in   the chart's values. The default values identify the images built for   the chart's release. When setting up for local testing: a transitory   tag value is set, with the image being built locally and loaded into   the KubeFlex hosting <code>kind</code> cluster named as if it were in   <code>ghcr.io/kubestellar/kubestellar</code>.</p> </li> </ul> <p>By our development practices and not doing any manual hacking, we maintain the association that the OCI image tagged <code>$VERSION</code> contains a Helm chart that declares its <code>version</code> and its <code>appVersion</code> to be <code>$VERSION</code> and instantiates version <code>$VERSION</code> of the KubeStellar controller-manager container image and the OCM Transport Controller container image.</p>"},{"location":"direct/packaging/#kubestellar-controller-manager-helm-chart","title":"KubeStellar controller-manager Helm Chart","text":"<p>NOTE: This is not used for anything anymore, but the published OCI images still exist at ghcr.io/kubestellar/kubestellar/controller-manager-chart.</p>"},{"location":"direct/packaging/#ocm-transport-controller-helm-chart","title":"OCM Transport Controller Helm chart","text":"<p>NOTE: This is not used for anything anymore, but the published OCI images still exist at ghcr.io/kubestellar/kubestellar/ocm-transport-controller-chart.</p>"},{"location":"direct/packaging/#scripts-and-instructions","title":"Scripts and instructions","text":"<p>There are instructions for using a release (Getting Started document) and a setup script for end-to-end testing(<code>test/e2e/common/setup-kubestellar.sh</code>). The end-to-end testing can either test the local copy/version of the kubestellar repo or test a release. So there are three cases to consider.</p>"},{"location":"direct/packaging/#getting-started-setup-instructions","title":"'Getting Started' setup instructions","text":"<p>Although we maintained variants in the past, we now maintain just one \"getting started\" setup recipe. It uses the core Helm chart.</p> <p>The instructions are a Markdown file that displays commands for a user to execute. These start with commands that define environment variables that hold the release of ks/kubestellar to use.</p> <p>The instructions display a command to instantiate the core Helm chart, at the version in the relevant environment variable, requesting the creation of one ITS and one WDS.</p> <p>The instructions display commands to update the user's kubeconfig file to have contexts for the ITS and the WDS created by the chart instance. These commands use the KubeFlex CLI (<code>kflex</code>). There is also a script under development that will do the job using <code>kubectl</code> instead of <code>kflex</code>; when it appears, the instructions will display a curl-to-bash command that fetches the script from GitHub using a version that appears as a literal in the instructions and gets manually updated as part of making a new release.</p>"},{"location":"direct/packaging/#e2e-setup-for-testing-a-release","title":"E2E setup for testing a release","text":"<p>When setting up to test a release, the setup script uses the published core Helm chart of the release being tested. That is the latest release as of the script's version.</p>"},{"location":"direct/packaging/#e2e-setup-for-testing-local-copyversion","title":"E2E setup for testing local copy/version","text":"<p>When setting up to test the local copy/version, the setup script uses the local version of the core Helm chart.</p> <p>The script builds a local kubestellar controller-manager container image from local sources. Then the script loads that image into the KubeFlex hosting cluster (e.g., using <code>kind load</code>). The script does the same for the OCM transport controller. The core chart is instantiated with settings to use the images just built.</p>"},{"location":"direct/packaging/#amalgamated-graph","title":"Amalgamated graph","text":"<p>Currently only showing kubestellar and ocm-status-addon.</p> <p>Again, omitting clusteradm and Helm CLI container images for simplicity.</p> <p>TODO: finish this</p> <pre><code>flowchart LR\n    subgraph osa_repo[\"ocm-status-addon@GitHub\"]\n    osa_code[OSA source code]\n    osa_hc_src[OSA Helm chart source]\n    end\n    osa_ctr_image[OSA container image] --&gt; osa_code\n    osa_hc_repo[published OSA Helm Chart] --&gt; osa_hc_src\n    osa_hc_src -.-&gt; osa_ctr_image\n    osa_hc_repo -.-&gt; osa_ctr_image\n    subgraph ks_repo[\"kubestellar@GitHub\"]\n    kcm_code[KCM source code]\n    gtc_code[\"generic transport&lt;br&gt;controller code\"]\n    otp_code[OTP source code]\n    ksc_hc_src[KS Core Helm chart source]\n    setup_ksc[\"'Getting Started' setup\"]\n    e2e_local[\"E2E setup&lt;br&gt;local\"]\n    e2e_release[\"E2E setup&lt;br&gt;release\"]\n    end\n    osa_repo -.-&gt; ks_repo\n    kcm_ctr_image[KCM container image] --&gt; kcm_code\n    otc_ctr_image[OTC container image]\n    otc_ctr_image --&gt; gtc_code\n    otc_ctr_image --&gt; otp_code\n    ksc_hc_repo[published KS Core chart] --&gt; ksc_hc_src\n    ksc_hc_src -.-&gt; osa_hc_repo\n    ksc_hc_src -.-&gt; kcm_ctr_image\n    ksc_hc_src -.-&gt; otc_ctr_image\n    ksc_hc_repo -.-&gt; osa_hc_repo\n    ksc_hc_repo -.-&gt; kcm_ctr_image\n    ksc_hc_repo -.-&gt; otc_ctr_image\n    setup_ksc -.-&gt; ksc_hc_repo\n    setup_ksc -.-&gt; KubeFlex\n    e2e_local -.-&gt; ksc_hc_src\n    e2e_local -.-&gt; KubeFlex\n    e2e_release -.-&gt; ksc_hc_repo\n    e2e_release -.-&gt; KubeFlex</code></pre> <p>Every dotted line is a reference that must be versioned. How do we keep all those versions right?</p> <p>Normally a git tag is an immutable reference to an immutable git commit. Let's not violate that.</p> <p>Can/should we say that an OCI image (or whatever) tag equals the tag of the commit that said image (or whatever) was built from? While keeping <code>main</code> always a working system?</p>"},{"location":"direct/pr-signoff/","title":"Git Commit Signoff and Signing","text":"<p>NOTE: \"sign-off\" is different from \"signing\" a commit.  The former indicates your assent to the repository's terms for contributors, the latter adds a cryptographic signature that is rarely displayed.  See the git book about signing. For commit signoff, do a web search on <code>git signoff</code>. GitHub has a concept of a commit being \"verified\" that extends the Git concept of signing.</p> <p>In order to get a pull request approved, you must first complete a DCO sign-off for each commit that the request is asking to add to the repository. This process is defined by the CNCF, and there are two cases: individual contributors and contributors that work for a corporate CNCF member. Both mean consent with the terms stated in the <code>DCO</code> file at the root of this Git repository. In the case of an individual, DCO sign-off is accomplished by doing a Git \"sign-off\" on the commit.</p> <p>We prefer that commits contributed to this repository be signed and GitHub verified, but this is not strictly necessary or enforced.</p>"},{"location":"direct/pr-signoff/#commit-sign-off","title":"Commit Sign-off","text":"<p>Your submitted PR must pass the automated checks in order to be merged. One of these checks that each commit that you propose to contribute is signed-off. If you use the <code>git</code> shell command, this involves passing the <code>-s</code> flag on the command line. For example, the following command will create a signed-off commit but not sign it.</p> <pre><code>git commit -s\n</code></pre> <p>Alternatively, the following command will create a commit that is both signed-off and signed.</p> <pre><code>git commit -s -S\n</code></pre> <p>For other tools, consult their documentation.</p>"},{"location":"direct/pr-signoff/#signing-commits","title":"Signing Commits","text":"<p>Before signing any commits, you must have a GPG and SSH key. Basic setup instructions can be found below (For more detailed instructions, refer to the Github GPG and SSH setup pages.)</p> <p>To sign a particular commit, you must either include <code>-S</code> on the <code>git commit</code> command line (see the command exhibited above for an example) or have configured automatic signing (see \"Everyone Must Sign\" in the Git Book for a hint about that).</p> <p>Before starting, make sure that your user email is verified on Github. To check for this:</p> <ol> <li>Login to Github and navigate to your Github Settings page</li> <li>In the sidebar, open the Emails tab</li> <li>Emails associated with Github should be listed at the top of the page under the \"Emails\" label</li> <li>An unverified email would have an \"Unverified\" label under it in orange text</li> <li>To verify, click Resend verification email and follow its prompts</li> <li>Navigate back to your Emails page, if the \"Unverified\" label is no longer there, then you're good to go!</li> </ol> <p></p> <p>For Windows users, Git Bash is also highly recommended.</p> <p></p>"},{"location":"direct/pr-signoff/#setting-up-the-gpg-key","title":"Setting up the GPG Key","text":"<ol> <li> <p>Install GnuPG (the GPG command line tool).</p> <ul> <li>Binary releases for your specific OS can be found here after scrolling down to the Binary Releases section (i.e. Gpg4win on Windows, Mac GPG for MacOS, etc).</li> <li>After downloading the installer, follow the prompts to set up GnuPG.</li> </ul> </li> <li> <p>Open Git Bash (or your CLI of choice) and use the following command to generate your GPG key pair:</p> </li> </ol> <pre><code>gpg --full-generate-key\n</code></pre> <ol> <li>If prompted to specify the size, type, and duration of the key that you want, press <code>Enter</code> to select the default option.</li> <li>Once prompted, enter your user info and a passphrase:<ul> <li>Make sure to list your email as the same one that's verified by Github</li> </ul> </li> <li>Use the following command to list the long form of your generated GPG keys:</li> </ol> <pre><code>gpg --list-secret-keys --keyid-format=long\n</code></pre> <pre><code>- Your GPG key ID should be the characters on the output line starting with `sec`, beginning directly after the `/` and ending before the listed date.\n- For example, in the output below (from the Github [GPG](https://docs.github.com/en/authentication/managing-commit-signature-verification/generating-a-new-gpg-key) setup page), the GPG key ID would be `3AA5C34371567BD2`\n\n    ```shell\n    $ gpg --list-secret-keys --keyid-format=long\n     /Users/hubot/.gnupg/secring.gpg\n     ------------------------------------\n     sec   4096R/3AA5C34371567BD2 2016-03-10 [expires: 2017-03-10]\n     uid                          Hubot &lt;hubot@example.com&gt;\n     ssb   4096R/4BB6D45482678BE3 2016-03-10\n    ```\n</code></pre> <ol> <li> <p>Copy your GPG key ID and run the command below, replacing <code>[your_GPG_key_ID]</code> with the key ID you just copied:</p> <pre><code>gpg --armor --export [your_GPG_key_ID]\n</code></pre> </li> <li> <p>This should generate an output with your GPG key. Copy the characters starting from <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code> and ending at <code>--END PGP PUBLIC KEY BLOCK-----</code> (inclusive) to your clipboard.</p> </li> <li>After copying or saving your GPG key, navigate to Settings in your Github</li> <li>Navigate to the SSH and GPG keys page under the Access section in the sidebar</li> <li>Under GPG keys, select New GPG key<ul> <li>Enter a suitable name for your key under \"Title\" and paste your GPG key that you copied/saved in Step 7 under \"Key\".</li> <li>Once done, click Add GPG key</li> </ul> </li> <li>Your new GPG key should now be displayed under GPG keys.</li> </ol> <p></p>"},{"location":"direct/pr-signoff/#setting-up-the-ssh-key","title":"Setting up the SSH Key","text":"<ol> <li> <p>Open Git Bash (or your CLI of choice) and use the following command to generate your new SSH key (make sure to replace <code>your_email</code> with your Github-verified email address):</p> <pre><code>ssh-keygen -t ed25519 -C \"your_email\"\n</code></pre> </li> <li> <p>Press <code>Enter</code> to select the default option if prompted to set a save-file or passphrase for the key (you may choose to enter a passphrase if desired; this will prompt you to enter the passphrase every time you perform a DCO sign-off).</p> </li> <li>The following output should generate a <code>randomart</code> image </li> <li> <p>Use the following command to copy the new SSH key to your clipboard:</p> <pre><code>clip &lt; ~/.ssh/id_ed25519.pub\n</code></pre> </li> <li> <p>After copying or saving your SSH key, navigate to Settings in your Github.</p> </li> <li>Navigate to the SSH and GPG keys page under the Access section in the sidebar.</li> <li>Under SSH keys, select New SSH key.<ul> <li>Enter a suitable name for your key under \"Title\"</li> <li>Open the dropdown menu under \"Key type\" and select Signing Key</li> <li>Paste your SSH key that you copied/saved in Step 3 under \"Key\"</li> </ul> </li> <li>Your new SSH key should now be displayed under SSH keys.</li> <li> <p>Optional: To test if your SSH key is connecting properly or not, run the following command in your CLI (more specific instructions can be found in the Github documentation):</p> <pre><code>ssh -T git@github.com\n</code></pre> <ul> <li>If given a warning saying something like <code>The authenticity of the host '[host IP]' can't be established</code> along with a key fingerprint and a prompt to continue, verify if the provided key fingerprint matches any of those listed here</li> <li>Once you've verified the match, type <code>yes</code></li> <li>If the resulting message says something along the lines of <code>Hi [User]! You've successfully authenticated, but GitHub does not provide shell access.</code>, then it means your SSH key is up and ready.</li> </ul> </li> </ol> <p></p>"},{"location":"direct/pr-signoff/#creating-pull-requests-using-the-github-website","title":"Creating Pull Requests Using the GitHub Website","text":"<p>This is not recommended for individual contributors, because the commits that it produces are not \"signed-off\" (as defined by Git) and thus do not carry assent to the DCO; see Repairing commits below for a way to recover if you have inadvertently made such a PR. For corporate contributors the DCO assent is indicated differently.</p> <p>Whether it's editing files from Kubestellar.io or directly from the Kubestellar Github, there are a couple steps to follow that streamlines the workflow of your PR:</p> <ol> <li> <p>Changes made to any file are automatically committed to a new branch in your fork.</p> <ul> <li>After clicking Commit changes..., write your commit message summary line and any extended desription that you want. Then click Propose changes, review your changes, and then create the PR.</li> <li>When making the PR, make sure to specify the type of PR at the beginning of the PR's title (i.e. :bug: if it addresses a bug-type issue)</li> </ul> </li> <li> <p>If the PR addresses a specific issue that has already been opened in GitHub, make sure to include the open issue number in Related Issue(s) (i.e. <code>Fixes #NNNN</code>); this will cause GitHub to automatically close the Issue once the PR is merged. If you have finished addressing an open issue without getting it automatically closed then explicitly close it.</p> </li> </ol>"},{"location":"direct/pr-signoff/#repairing-commits","title":"Repairing commits","text":"<p>If you have already created a PR that proposes to merge a branch that adds commits that are not signed-off then you can repair this (and lack of signing, if you choose) by adding the signoff to each using <code>git commit -s --amend</code> on each of them. If you also want those commits signed then you would use <code>git commit -s -S --amend</code> or configure automatic signing. Following is an outline of how to do it for a branch that adds exactly one commit. If your branch adds more than one commit then you can extrapolate using <code>git cherry-pick -s -S</code> to build up a revised series of commits one-by-one.</p> <p>The following instructions provide a basic walk-through if you have already created your own fork of the repository but yet not made a clone on your workstation.</p> <ol> <li> <p>Navigate to the Code page of the Kubestellar github.</p> </li> <li> <p>Click the Fork dropdown in the top right corner of the page.</p> <ul> <li>Under \"Existing Forks\" click your fork (should look something like \"your_username/kubestellar\")</li> </ul> </li> <li>Once in your fork, click the Code dropdown.<ul> <li>Under the \"Local\" tab at the top of the dropdown, select the SSH tab</li> <li>Copy the SSH repo URL to your clipboard</li> </ul> </li> <li>Open Git Bash (or your CLI of choice), create or change to a different directory if desired.</li> <li>Clone the repository using <code>git clone</code> followed by pasting the URL you just copied.</li> <li>Change your directory to the Kubestellar repo using <code>cd kubestellar</code>.</li> <li><code>git checkout</code> to the branch in your fork where the changes were committed.<ul> <li>The branch name should be written at the top of your submitted PR page and looks something like \"patch-X\" (where \"X\" should be the number of PRs made on your fork to date)</li> </ul> </li> <li>Once in your branch, type <code>git commit -s --amend</code> to sign off your PR.<ul> <li>The commit will also be signed if either you have set up automatic signing or both include the <code>-S</code> flag on that command and have set up your GPG key.</li> <li>You may extend that command with <code>-m</code> followed by a quoted commit message if you desire. Otherwise <code>git</code> will pop up an editor for you to use in making any desired adjustment to the commit message. After making any desired changes, save and exit the editor. FYI: in <code>vi</code> (which GitBash uses), when it is in Command mode (which is the normal mode, and contrasts with Insert mode) the keystrokes <code>:wq!</code> will attempt to save and then will exit no matter what.</li> </ul> </li> <li>Type <code>git push -f origin [branch_name]</code>, replacing <code>[branch_name]</code> with the actual name of your branch.</li> <li>Navigate back to your PR github page.<ul> <li>A green <code>dco-signoff: yes</code> label indicates that your PR is successfully signed</li> </ul> </li> </ol>"},{"location":"direct/pre-reqs/","title":"KubeStellar Prerequisites","text":"<p>The following prerequisites are required. You can use the check-pre-req script, to validate if all needed prerequisites are installed.</p>"},{"location":"direct/pre-reqs/#infrastructure-clusters","title":"Infrastructure (clusters)","text":"<p>Because of its multicluster architecture, KubeStellar requires that you have the necessary privileges and infrastructure access to create and/or configure the necessary Kubernetes clusters. These are the following; see the architecture document for more details.</p> <ul> <li>One cluster to serve as the KubeFlex hosting cluster.</li> <li>Any additional Kubernetes clusters that are not created by KubeFlex but you will use as a WDS or ITS.</li> <li>Your WECs.</li> </ul> <p>Our documentation has remarks about using the following sorts of clusters:</p> <ul> <li>kind</li> <li>k3s</li> <li>openshift </li> </ul>"},{"location":"direct/pre-reqs/#software-prerequisites-for-using-kubestellar","title":"Software Prerequisites: for Using KubeStellar","text":"<ul> <li> <p>kubeflex version 0.8.0 or higher.     To install kubeflex go to https://github.com/kubestellar/kubeflex/blob/main/docs/users.md#installation. To upgrade from an existing installation, follow these instructions. At the end of the install make sure that the kubeflex CLI, kflex, is in your <code>$PATH</code>.</p> </li> <li> <p>OCM CLI (clusteradm) 0.10 &lt;= version &lt; 0.11.     To install the latest acceptable version of the OCM CLI use:</p> <pre><code>bash &lt;(curl -L https://raw.githubusercontent.com/open-cluster-management-io/clusteradm/main/install.sh) 0.10.1\n</code></pre> <p>Note that the default installation of clusteradm will install in /usr/local/bin which will require root access. If you prefer to avoid root, you can specify an alternative installation location using the INSTALL_DIR environment variable, as follows:</p> <pre><code>mkdir -p ocm\nexport INSTALL_DIR=\"$PWD/ocm\"\nbash &lt;(curl -L https://raw.githubusercontent.com/open-cluster-management-io/clusteradm/main/install.sh) 0.10.1\nexport PATH=$PWD/ocm:$PATH\n</code></pre> <p>At the end of the install make sure that the OCM CLI, clusteradm, is in your <code>$PATH</code>.</p> </li> <li> <p>helm version &gt;= 3. To deploy the Kubestellar and kubeflex charts. Your <code>helm</code> command must not be broken; see the known issue.</p> </li> <li>kubectl version &gt;= 1.29 - to access the kubernetes clusters</li> </ul>"},{"location":"direct/pre-reqs/#additional-software-for-the-getting-started-setup","title":"Additional Software for the Getting Started setup","text":"<ul> <li>kind version &gt;= 0.20 and configured to be able to run at least 3 clusters (see the <code>kind</code> \"known issue\" named Pod errors due to \"too many open files\" and note that it is NOT about <code>ulimit -n</code>)</li> <li>docker (or compatible docker engine that works with kind) (client version &gt;= 20)</li> </ul>"},{"location":"direct/pre-reqs/#additional-software-for-monitoring","title":"Additional Software for monitoring","text":"<p>The setup in <code>montoring/</code> additional uses the following.</p> <ul> <li><code>yq</code> (also available from Homebrew) version &gt;= 1.5</li> </ul>"},{"location":"direct/pre-reqs/#additional-software-for-running-the-examples","title":"Additional Software For Running the Examples","text":"<ul> <li>argocd version &gt;= 2 - for the examples that use it</li> </ul>"},{"location":"direct/pre-reqs/#additional-software-for-building-kubestellar-from-source-and-testing","title":"Additional Software For Building KubeStellar from Source and Testing","text":"<ul> <li>go version 1.23 or higher - to build Kubestellar</li> <li>GNU make version &gt;= 3.5 - to build Kubestellar and create the Kubestellar container images</li> <li>ko version &gt;= 0.15 - to create some of the Kubestellar container images</li> <li>docker (or equivalent that implements <code>docker buildx</code>) (client version &gt;= 20) - to create other KubeStellar container images</li> </ul> <p>To build and test KubeStellar properly, you will also need</p> <ul> <li>kind version &gt;= 0.20 and, if you want the demo setup or any other with three or more clusters, configured to be able to run at least 3 clusters (see the <code>kind</code> \"known issue\" named Pod errors due to \"too many open files\" and note that it is NOT about <code>ulimit -n</code>)</li> <li>OCP, if you are testing a scenario involving OCP</li> <li>ginkgo, if you will run the ginkgo-based end-to-end test</li> <li><code>yq</code> (also available from Homebrew) version &gt;= 4 - for running tests</li> </ul>"},{"location":"direct/pre-reqs/#automated-check-of-prerequisites-for-kubestellar","title":"Automated Check of Prerequisites for KubeStellar","text":"<p>The check_pre_req script offers a convenient way to check for the prerequisites needed for KubeStellar deployment and use.</p> <p>This script is self-contained, so it is suitable for \"curl-to-bash\" style usage. The latest development version is at https://raw.githubusercontent.com/kubestellar/kubestellar/refs/heads/main/scripts/check_pre_req.sh. To check the prerequisites for using a particular release of KubeStellar, you will want to use the script from that release.</p> <p>The script checks for a prerequisite presence in the <code>$PATH</code>, by using the <code>which</code> command, and it can optionally provide version and path information for prerequisites that are present, or installation information for missing prerequisites.</p> <p>We envision that this script could be useful for user-side debugging as well as for asserting the presence of prerequisites in higher-level automation scripts.</p> <p>The script accepts a list of optional flags and arguments.</p>"},{"location":"direct/pre-reqs/#supported-flags","title":"Supported flags:","text":"<ul> <li><code>-A|--assert</code>: exits with error code 2 upon finding the first missing prerequisite</li> <li><code>-L|--list</code>: prints a list of supported prerequisites</li> <li><code>-V|--verbose</code>: displays version and path information for installed prerequisites or installation information for missing prerequisites</li> <li><code>-X</code>: enable <code>set -x</code> for debugging the script</li> </ul>"},{"location":"direct/pre-reqs/#supported-arguments","title":"Supported arguments:","text":"<p>The script accepts a list of specific prerequisites to check, among the list of available ones:</p> <pre><code>$ check_pre_req.sh --list\nargo brew docker go helm jq kflex kind ko kubectl make ocm yq\n</code></pre>"},{"location":"direct/pre-reqs/#examples","title":"Examples","text":"<p>For example, list of prerequisites required by KubeStellar can be checked with the command below (add the <code>-V</code> flag to get the version of each program and a suggestions on how to install missing prerequisites):</p> <pre><code>$ scripts/check_pre_req.sh\nChecking pre-requisites for using KubeStellar:\n\u2714 Docker (Docker version 27.2.1-rd, build cc0ee3e)\n\u2714 kubectl (v1.29.2)\n\u2714 KubeFlex (Kubeflex version: v0.6.3.672cc8a 2024-09-23T16:15:47Z)\n\u2714 OCM CLI (:v0.9.0-0-g56e1fc8)\n\u2714 Helm (v3.16.1)\n\u2714 helm can fetch public charts\nChecking additional pre-requisites for running the examples:\n\u2714 Kind (kind v0.22.0 go1.22.0 darwin/arm64)\n\u2714 fs.inotify.max_user_watches is 524288\n\u2714 fs.inotify.max_user_instances is 512\n\u2714 ArgoCD CLI (v2.10.1+a79e0ea)\nChecking pre-requisites for building KubeStellar:\n\u2714 GNU Make (GNU Make 3.81)\n\u2714 Go (go version go1.23.2 darwin/arm64)\n\u2714 KO (0.16.0)\n</code></pre> <p>In another example, a specific list of prerequisites could be asserted by a higher-level script, while providing some installation information, with the command below (note that the script will terminate upon finding a missing prerequisite):</p> <pre><code>$ check_pre_req.sh --assert --verbose helm argo docker kind\nChecking KubeStellar pre-requisites:\n\u2714 Helm\n  version (unstructured): version.BuildInfo{Version:\"v3.14.0\", GitCommit:\"3fc9f4b2638e76f26739cd77c7017139be81d0ea\", GitTreeState:\"clean\", GoVersion:\"go1.21.5\"}\n     path: /usr/sbin/helm\nX ArgoCD CLI\n  how to install: https://argo-cd.readthedocs.io/en/stable/cli_installation/; get at least version v2\n</code></pre>"},{"location":"direct/release-notes/","title":"Release notes","text":"<p>The following sections list the known issues for each release. The issue list is not differential (i.e., compared to previous releases) but a full list representing the overall state of the specific release. </p>"},{"location":"direct/release-notes/#0280","title":"0.28.0","text":"<p>Helm chart, the name of the subobject for ArgoCD has changed from <code>argo-cd</code> to <code>argocd</code>.</p> <p>There have been minor fixups, including to the website.</p> <p>We have advanced the version of the kube-rbac-proxy image used, from 0.18.0 (which is based on Kubernetes 1.30) to 0.19.1 (which is based on Kubernetes 1.31). Depending on a later minor release of Kubernetes is generally risky, but expected to work OK in this case.</p>"},{"location":"direct/release-notes/#0280-rc1","title":"0.28.0-rc.1","text":"<p>There is one breaking change for users: in the \"values\" for the core Helm chart, the name of the subobject for ArgoCD has changed from <code>argo-cd</code> to <code>argocd</code>.</p> <p>There have been minor fixups, including to the website.</p>"},{"location":"direct/release-notes/#0280-alpha2","title":"0.28.0-alpha.2","text":"<p>The main change is advancing the version of the kube-rbac-proxy image used, from 0.18.0 (which is based on Kubernetes 1.30) to 0.19.1 (which is based on Kubernetes 1.31). Depending on a later minor release of Kubernetes is generally risky, but expected to work OK in this case.</p>"},{"location":"direct/release-notes/#0280-alpha1","title":"0.28.0-alpha.1","text":"<p>The main changes are moving from Kubernetes 1.29 to 1.30, and picking up advances in other dependencies (but staying limited to Kubernetes 1.30).</p>"},{"location":"direct/release-notes/#remaining-limitations-in-0280","title":"Remaining limitations in 0.28.0","text":"<ul> <li>Although the create-only feature can be used with Job objects to avoid trouble with <code>.spec.selector</code>, requesting singleton reported state return will still lead to a controller fight over <code>.status.replicas</code> while the Job is in progress.</li> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> <li>Creation, deletion, and modification of <code>CustomTransform</code> objects does not cause corresponding updates to the workload objects in the WECs; the current state of the <code>CustomTransform</code> objects is simply read at any moment when the objects in the WECs are being updated for other reasons.</li> <li>It is not known what actually happens when two different <code>Binding</code> objects list the same workload object and either or both say \"create only\".</li> <li>If (a) the workload object count or volume vs the configured limits on content of a <code>ManifestWork</code> causes multiple <code>ManifestWork</code> to be created for one <code>Binding</code> (<code>BindingPolicy</code>) AND (b) the limit on number of workload objects in one <code>ManifestWork</code> is greater then 1, then there may be transients where workload objects are deleted and re-created in a WEC --- which, in addition to possibly being troubling on its own, will certainly thwart the \"create-only\" functionality. The default limit on the number of workload objects in one <code>ManifestWork</code> is 1, so this issue will only arise when you use a non-default value. In this case you will avoid this issue if you set that limit to be at least the highest number of workload objects that will appear in a <code>Binding</code> (do check your <code>Binding</code> objects, lest you be surprised) AND your workload is not so large that multiple <code>ManifestWork</code> are created due to the limit on their size.</li> </ul>"},{"location":"direct/release-notes/#0272","title":"0.27.2","text":"<p>Fixes <code>scripts/check_pre_req.sh</code> so that when it objects to the version of <code>clusteradm</code>, this is more obvious (restores the RED X that was inadvertently removed in the previous patch release).</p> <p>Also some doc improvements, and bumps to some build-time dependencies.</p>"},{"location":"direct/release-notes/#0271","title":"0.27.1","text":"<p>Bumps version of ingrex-nginx used to 0.12.1, to avoid recently disclosed vulnerabilities in older versions.</p> <p>Avoids use of release 0.11 of clusteradm, which introduced an incompatible change in the name of a ServiceAccount.</p>"},{"location":"direct/release-notes/#0270-and-its-rcs","title":"0.27.0 and its RCs","text":"<p>The major changes since 0.26.0 are as follows.</p> <ul> <li>Adding the ability to use a pre-existing cluster as an ITS.</li> <li>Reliability improvement: The core Helm chart now uses KubeFlex release 0.8.1, which avoids pulling from DockerHub (which is rate-limited).</li> </ul>"},{"location":"direct/release-notes/#0260-and-its-rcs","title":"0.26.0 and its RCs","text":"<p>The major changes since 0.25.1 are as follows.</p> <ul> <li>Increase the Kubernetes release that kubestellar depends on, from 1.28 to 1.29.</li> <li>The demo environment creation script is much more reliable, mainly due to no longer attempting concurrent operations. Still, external network/server hiccups can cause the script to fail.</li> <li>This release removes the thrashing of workload objects in the WEC in the case where the transport controller's <code>max-num-wrapped</code> is 1.</li> <li>This release adds reporting, in <code>BindingPolicy</code> and <code>Binding</code> status, of whether any of the referenced <code>StatusCollector</code> objects do not exist.</li> <li>This release changes the schema for a <code>BindingPolicy</code> so that the request for sigleton status return is made/not-made independently in each <code>DownsyncPolicyClause</code> rather than once on the whole <code>BindingPolicySpec</code>. The schema for <code>Binding</code> objects is changed correspondingly. This is a breaking change in the YAML schema for Binding[Policy] objects that request singleton status return.</li> </ul>"},{"location":"direct/release-notes/#remaining-limitations-in-0260","title":"Remaining limitations in 0.26.0","text":"<ul> <li>Although the create-only feature can be used with Job objects to avoid trouble with <code>.spec.selector</code>, requesting singleton reported state return will still lead to a controller fight over <code>.status.replicas</code> while the Job is in progress.</li> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> <li>Creation, deletion, and modification of <code>CustomTransform</code> objects does not cause corresponding updates to the workload objects in the WECs; the current state of the <code>CustomTransform</code> objects is simply read at any moment when the objects in the WECs are being updated for other reasons.</li> <li>It is not known what actually happens when two different <code>Binding</code> objects list the same workload object and either or both say \"create only\".</li> <li>If (a) the workload object count or volume vs the configured limits on content of a <code>ManifestWork</code> causes multiple <code>ManifestWork</code> to be created for one <code>Binding</code> (<code>BindingPolicy</code>) AND (b) the limit on number of workload objects in one <code>ManifestWork</code> is greater then 1, then there may be transients where workload objects are deleted and re-created in a WEC --- which, in addition to possibly being troubling on its own, will certainly thwart the \"create-only\" functionality. The default limit on the number of workload objects in one <code>ManifestWork</code> is 1, so this issue will only arise when you use a non-default value. In this case you will avoid this issue if you set that limit to be at least the highest number of workload objects that will appear in a <code>Binding</code> (do check your <code>Binding</code> objects, lest you be surprised) AND your workload is not so large that multiple <code>ManifestWork</code> are created due to the limit on their size.</li> </ul>"},{"location":"direct/release-notes/#0260-alpha5","title":"0.26.0-alpha.5","text":"<p>This release is intended to have the same functionality as 0.26.0-alpha.3 and 0.26.0-alpha.4 but test a change to the GitHub Actions workflow that makes a release; the change adds SBOM generation (PR 2718).</p>"},{"location":"direct/release-notes/#0260-alpha4","title":"0.26.0-alpha.4","text":"<p>This release is intended to have the same functionality as 0.26.0-alpha.3 but test a change to the GitHub Actions workflow that makes a release; the change suppresses attachment of useless binary archives (PR 2704).</p>"},{"location":"direct/release-notes/#0260-alpha3","title":"0.26.0-alpha.3","text":"<p>This release adds the option for the core Helm chart to not take responsibility for running <code>clusteradm init</code> on an ITS. Somebody has to, but not necessarily this chart.</p>"},{"location":"direct/release-notes/#remaining-limitations-in-0260-alpha3","title":"Remaining limitations in 0.26.0-alpha.3","text":"<ul> <li>Although the create-only feature can be used with Job objects to avoid trouble with <code>.spec.selector</code>, requesting singleton reported state return will still lead to a controller fight over <code>.status.replicas</code> while the Job is in progress.</li> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> <li>Creation, deletion, and modification of <code>CustomTransform</code> objects does not cause corresponding updates to the workload objects in the WECs; the current state of the <code>CustomTransform</code> objects is simply read at any moment when the objects in the WECs are being updated for other reasons.</li> <li>It is not known what actually happens when two different <code>Binding</code> objects list the same workload object and either or both say \"create only\".</li> <li>If (a) the workload object count or volume vs the configured limits on content of a <code>ManifestWork</code> causes multiple <code>ManifestWork</code> to be created for one <code>Binding</code> (<code>BindingPolicy</code>) AND (b) the limit on number of workload objects in one <code>ManifestWork</code> is greater then 1, then there may be transients where workload objects are deleted and re-created in a WEC --- which, in addition to possibly being troubling on its own, will certainly thwart the \"create-only\" functionality. The default limit on the number of workload objects in one <code>ManifestWork</code> is 1, so this issue will only arise when you use a non-default value. In this case you will avoid this issue if you set that limit to be at least the highest number of workload objects that will appear in a <code>Binding</code> (do check your <code>Binding</code> objects, lest you be surprised) AND your workload is not so large that multiple <code>ManifestWork</code> are created due to the limit on their size.</li> </ul>"},{"location":"direct/release-notes/#0260-alpha1-0260-alpha2","title":"0.26.0-alpha.1, 0.26.0-alpha.2","text":"<p>This release removes the thrashing of workload objects in the WEC in the case where the transport controller's <code>max-num-wrapped</code> is 1.</p> <p>This release changes the schema for a <code>BindingPolicy</code> so that the request for sigleton status return is made/not-made independently in each <code>DownsyncPolicyClause</code> rather than once on the whole <code>BindingPolicySpec</code>. The schema for <code>Binding</code> objects is changed correspondingly.</p>"},{"location":"direct/release-notes/#remaining-limitations-in-0260-alpha1-and-0260-alpha2","title":"Remaining limitations in 0.26.0-alpha.1 and 0.26.0-alpha.2","text":"<ul> <li>Although the create-only feature can be used with Job objects to avoid trouble with <code>.spec.selector</code>, requesting singleton reported state return will still lead to a controller fight over <code>.status.replicas</code> while the Job is in progress.</li> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> <li>Creation, deletion, and modification of <code>CustomTransform</code> objects does not cause corresponding updates to the workload objects in the WECs; the current state of the <code>CustomTransform</code> objects is simply read at any moment when the objects in the WECs are being updated for other reasons.</li> <li>It is not known what actually happens when two different <code>Binding</code> objects list the same workload object and either or both say \"create only\".</li> <li>If (a) the workload object count or volume vs the configured limits on content of a <code>ManifestWork</code> causes multiple <code>ManifestWork</code> to be created for one <code>Binding</code> (<code>BindingPolicy</code>) AND (b) the limit on number of workload objects in one <code>ManifestWork</code> is greater then 1, then there may be transients where workload objects are deleted and re-created in a WEC --- which, in addition to possibly being troubling on its own, will certainly thwart the \"create-only\" functionality. The default limit on the number of workload objects in one <code>ManifestWork</code> is 1, so this issue will only arise when you use a non-default value. In this case you will avoid this issue if you set that limit to be at least the highest number of workload objects that will appear in a <code>Binding</code> (do check your <code>Binding</code> objects, lest you be surprised) AND your workload is not so large that multiple <code>ManifestWork</code> are created due to the limit on their size.</li> </ul>"},{"location":"direct/release-notes/#0251","title":"0.25.1","text":"<p>This patch release fixes some bugs and some documentation oversights. Following are the most notable ones.</p> <ul> <li>The transport controller bugs that strike when there is more than one <code>ManifestWork</code> for a given <code>Binding</code> (<code>BindingPolicy</code>) have been fixed (we hope).</li> <li>The Getting Started document has been updated to include documentation of how to use the script that does the steps listed in that document.</li> </ul>"},{"location":"direct/release-notes/#remaining-limitations-in-0251","title":"Remaining limitations in 0.25.1","text":"<ul> <li>Although the create-only feature can be used with Job objects to avoid trouble with <code>.spec.selector</code>, requesting singleton reported state return will still lead to a controller fight over <code>.status.replicas</code> while the Job is in progress.</li> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> <li>Creation, deletion, and modification of <code>CustomTransform</code> objects does not cause corresponding updates to the workload objects in the WECs; the current state of the <code>CustomTransform</code> objects is simply read at any moment when the objects in the WECs are being updated for other reasons.</li> <li>It is not known what actually happens when two different <code>Binding</code> objects list the same workload object and either or both say \"create only\".</li> <li>If the workload object count or volume vs the configured limits on content of a <code>ManifestWork</code> causes multiple <code>ManifestWork</code> to be created for one <code>Binding</code> (<code>BindingPolicy</code>) then there may be transients where workload objects are deleted and re-created in a WEC --- which, in addition to possibly being troubling on its own, will certainly thwart the \"create-only\" functionality. Unless you workload is very large, you can avoid this situation by setting the <code>transport_controller.max_num_wrapped</code> \"value\" of the core Helm chart to a number that is larger than the number of your workload objects (double check your count in your <code>Binding</code> object).</li> </ul>"},{"location":"direct/release-notes/#0250-and-its-candidates","title":"0.25.0 and its candidates","text":"<ul> <li>The main advance in this release is finishing the implementation of the create-only feature. It is now available for use.</li> <li>The default value of transport controller's <code>max-num-wrapped</code> flag is changed to 1, in the core Helm chart.</li> </ul>"},{"location":"direct/release-notes/#remaining-limitations-in-0250-and-its-candidates","title":"Remaining limitations in 0.25.0 and its candidates","text":"<ul> <li>Although the create-only feature can be used with Job objects to avoid trouble with <code>.spec.selector</code>, requesting singleton reported state return will still lead to a controller fight over <code>.status.replicas</code> while the Job is in progress.</li> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> <li>Creation, deletion, and modification of <code>CustomTransform</code> objects does not cause corresponding updates to the workload objects in the WECs; the current state of the <code>CustomTransform</code> objects is simply read at any moment when the objects in the WECs are being updated for other reasons.</li> <li>If the workload object count or volume vs the configured limits on content of a <code>ManifestWork</code> causes multiple <code>ManifestWork</code> to be created for one <code>Binding</code> (<code>BindingPolicy</code>) then there are bugs in the updating of workload objects in the WECs.</li> <li>It is not known what actually happens when two different <code>Binding</code> objects list the same workload object and either or both say \"create only\".</li> <li>If the workload object count or volume vs the configured limits on content of a <code>ManifestWork</code> causes multiple <code>ManifestWork</code> to be created for one <code>Binding</code> (<code>BindingPolicy</code>) then there may be transients where workload objects are deleted and re-created in a WEC --- which, in addition to possibly being troubling on its own, will certainly thwart the \"create-only\" functionality. Unless you workload is very large, you can avoid this situation by setting the <code>transport_controller.max_num_wrapped</code> \"value\" of the core Helm chart to a number that is larger than the number of your workload objects (double check your count in your <code>Binding</code> object).</li> </ul>"},{"location":"direct/release-notes/#0250-alpha1-test-releases","title":"0.25.0-alpha.1 test releases","text":"<p>These test the release-building functionality, which has been revised in the course of merge the controller-manager and transport-controller Helm charts into the core Helm chart.</p>"},{"location":"direct/release-notes/#0240-and-its-candidates-and-their-precursors","title":"0.24.0 and its candidates and their precursors","text":"<p>The main functional change from 0.23.X is the completion of the status combination and the partial introduction of the create-only feature (its API is there but its implementation is not --- DO NOT TRY TO USE THIS FEATURE). There is also further work on the organization of the website. There is also a major change in the GitHub repository structure: the kubestellar/ocm-transport-plugin repository's contents have been merged into the kubestellar/kubestellar repo (after <code>0.24.0-alpha.2</code>).</p>"},{"location":"direct/release-notes/#remaining-limitations-in-0240","title":"Remaining limitations in 0.24.0","text":"<ul> <li>Job objects are not properly supported.</li> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> <li>Creation, deletion, and modification of <code>CustomTransform</code> objects does not cause corresponding updates to the workload objects in the WECs; the current state of the <code>CustomTransform</code> objects is simply read at any moment when the objects in the WECs are being updated for other reasons.</li> <li>If the workload object count or volume vs the configured limits on content of a <code>ManifestWork</code> causes multiple <code>ManifestWork</code> to be created for one <code>Binding</code> (<code>BindingPolicy</code>) then there are bugs in the updating of workload objects in the WECs.</li> <li>It is not known what actually happens when two different <code>Binding</code> objects list the same workload object and either or both say \"create only\".</li> <li>If the workload object count or volume vs the configured limits on content of a <code>ManifestWork</code> causes multiple <code>ManifestWork</code> to be created for one <code>Binding</code> (<code>BindingPolicy</code>) then there may be transients where workload objects are deleted and re-created in a WEC --- which, in addition to possibly being troubling on its own, will certainly thwart the \"create-only\" functionality.</li> </ul>"},{"location":"direct/release-notes/#0231","title":"0.23.1","text":"<p>The main change from 0.23.0 is a re-organization of the website, which is still a work in progress, and archival of all website content that is outdated.</p>"},{"location":"direct/release-notes/#remaining-limitations-in-0231","title":"Remaining limitations in 0.23.1","text":"<ul> <li>Job objects are not properly supported.</li> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> </ul>"},{"location":"direct/release-notes/#0230-and-its-release-candidates","title":"0.23.0 and its release candidates","text":"<p>The main change is introduction of the an all-in-one chart, called the core chart, for installing KubeStellar in a given hosting cluster and creating an initial set of WDSes and ITSes.</p> <p>This release also introduces a preliminary API for combining workload object reported state from the WECs --- BUT THE IMPLEMENTATION IS NOT DONE*. The control objects can be created but the designed response is not there. The design of the control objects is likely to change in the future too (without change in the Kubernetes API group's version string). In short, stay away from this feature in this release.</p> <p>This release also features better observability (<code>/metrics</code> and <code>/debug/pprof</code>) and control over client-side self-restraint (request QPS and burst).</p>"},{"location":"direct/release-notes/#remaining-limitations-in-0230-and-its-release-candidates","title":"Remaining limitations in 0.23.0 and its release candidates","text":"<ul> <li>Job objects are not properly supported.</li> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> </ul>"},{"location":"direct/release-notes/#0220-and-its-release-candidates","title":"0.22.0 and its release candidates","text":"<p>The changes include adding the following features.</p> <ul> <li>Custom WEC-independent transformations of workload objects on their way from WDS to WEC.</li> <li>WEC-dependent Go template expansion in the strings of a workload object on its way from WDS to WEC.</li> <li><code>PriorityClass</code> objects (from API group <code>scheduling.k8s.io</code>) propagate now.</li> <li>Support multiple WDSes.</li> <li>Allow multiple ITSes.</li> <li>Use the new Helm chart from kubestellar/ocm-transport-plugin for deploying the transport controller.</li> </ul> <p>Prominent bug fixes include more discerning cleaning of workload objects on their way from WDS to WEC. This includes keeping a \"headless\" <code>Service</code> headless and removing the <code>spec.suspend</code> field from a <code>Job</code>.</p> <p>See the changelogs on GitHub for full details.</p>"},{"location":"direct/release-notes/#remaining-limitations-in-0220-and-its-release-candidates","title":"Remaining limitations in 0.22.0 and its release candidates","text":"<ul> <li>Job objects are not properly supported.</li> <li>Removing of WorkStatus objects (in the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> </ul>"},{"location":"direct/release-notes/#0212-and-its-release-candidates","title":"0.21.2 and its release candidates","text":"<p>The changes since 0.21.1 include efficiency improvements, reducing costs of running the kubestellar-controller-manager for a WDS that is an OpenShift cluster. There are also bug fixes and documentation improvements.</p>"},{"location":"direct/release-notes/#0211","title":"0.21.1","text":"<p>This release mainly updates the documentation exposed under kubestellar.io.</p>"},{"location":"direct/release-notes/#0210-and-its-release-candidates","title":"0.21.0 and its release candidates","text":""},{"location":"direct/release-notes/#major-changes-for-0210-and-its-release-candidates","title":"Major changes for 0.21.0 and its release candidates","text":"<ul> <li>This release introduces pluggable transport. Currently the only plugin is the OCM transport plugin.</li> </ul>"},{"location":"direct/release-notes/#bug-fixes-in-0210-and-its-release-candidates","title":"Bug fixes in 0.21.0 and its release candidates","text":"<ul> <li>dynamic changes to WECs are supported. Existing Bindings and ManifestWorks will be updated when new WECs are added/updated/delete or when labels are added/updated/deleted on existing WECs</li> <li>An update to a workload object that removes some BindingPolicies from the matching set is handled correctly.</li> <li>These changes that happen while a controller is down are handled correctly:</li> <li>If a workload object is deleted, or changed to remove some BindingPolicies from the matching set;</li> <li>A BindingPolicy update that removes workload objects or clusters from their respective matching sets.</li> </ul>"},{"location":"direct/release-notes/#remaining-limitations-in-0210-and-its-release-candidates","title":"Remaining limitations in 0.21.0 and its release candidates","text":"<ul> <li>Job objects are not properly supported.</li> <li>Removing of WorkStatus objects (on the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> </ul>"},{"location":"direct/release-notes/#0200-and-its-release-candidates","title":"0.20.0 and its release candidates","text":"<ul> <li>Job objects are not properly supported.</li> <li>Dynamic changes to WECs are not supported. Existing ManifestWorks will not be updated when new WECs are added or when labels are added/deleted on existing WECs</li> <li>Removing of WorkStatus objects (on the transport namespace) is not supported and may not result in recreation of that object</li> <li>Singleton status return: It is the user responsibility to make sure that if a BindingPolicy requesting singleton status return matches a given workload object then no other BindingPolicy matches the same object. Currently there is no enforcement of that.</li> <li>Objects on two different WDSes shouldn't have the exact same identifier (same group, version, kind, name and namespace). Such a conflict is currently not identified.</li> <li>An update to a workload object that removes some BindingPolicies from the matching set is not handled correctly.</li> <li>Some operations are not handled correctly while the controller is down:</li> <li>If a workload object is deleted, or changed to remove some BindingPolicies from the matching set, it will not be handled correctly.</li> <li>A BindingPolicy update that removes workload objects or clusters from their respective matching sets is not handled correctly.</li> </ul>"},{"location":"direct/release-testing/","title":"Testing a new KubeStellar Release","text":"<p>The following testing process should be applied to every new KubeStellar release in order to validate it, this include both regular releases and release candidates. All the tests should be done while the KubeStellar code is still under code-freeze and new code shouldn't be merged into the main branch until all tests are passed and the release is officially declared as ready. In case the release tests fail (even one of them), the release should be declared as unstable and a fix through a new release candidate should be worked on ASAP. The KubeStellar code-freeze should be lifted only after all tests are passed and a the release was completed. To reduce the exposure of unstable releases the update of the KubeStellar site kubestellar.io should be done only once all release tests passed successfully. </p>"},{"location":"direct/release-testing/#release-tests","title":"Release tests","text":"<p>The following section describe the tests that must be executed for each release.</p> <p>Our release tests consists of:    * Automatic tests running on Ubuntu X86 (see below)    * Manually initiated tests running on OCP (TODO: add specific version and machine details)</p> <p>Due to the lack of OCP based automatic testing, these tests will be performed only once a release candidate passed all other tests and is a candidate to become a regular release. </p> <p>Note:  We plan to automate all release tests in the future</p>"},{"location":"direct/release-testing/#automatic-github-based-release-tests","title":"Automatic (github based) release tests","text":"<p>KubeStellar CICD automatically runs a set of e2e tests on each new release. Currently these tests include 2 main test types bash based e2e tests and ginkgo based e2e tests. The bash test basically tests the scenario of  multi-cluster workload deployment with kubectl. The ginkgo test cover the Singleton status test, and several other tests that are listed in the test README. Note, however, that the content of the releases tests may be changed in the future. We will refer to those tests as the e2e release tests.  The automatic tests are running on github hosted runners of type Ubuntu latest (currently 22.04) X86 64 bit  Note: When a new release is created please verify that the automatic tests indeed executed and passed. </p>"},{"location":"direct/release-testing/#e2e-release-tests-on-ocp","title":"e2e release tests on OCP","text":"<p>As many of the KubeStellar customers are using OCP, the release tests should be executed on an OCP cluster as well. Currently these tests should be initiated manually on a dedicated OCP cluster that is reserved for the release testing process. </p> <p>TODO: The details on how to setup and run the test </p>"},{"location":"direct/release-testing/#other-platforms","title":"Other platforms","text":"<p>KubeStellar is also used on other platforms such as ARM64, MacOS, etc.. Currently these platforms are not part of the routine release testing, however the KubeStellar team will try its best to help and solve issues detected on other platforms as well. Users should go through the regular procedure of opening issues against the KubeStellar project .</p>"},{"location":"direct/release/","title":"Making KubeStellar Releases","text":"<p>This document defines how releases of the KubeStellar repository are made. This document is a work-in-progress.</p> <p>This document starts with step-by-step instructions for the current procedure, then proceeds with the thinking behind them.</p> <p>See the associated packaging and delivery doc for some clues about the problem.</p> <p>Every release should pass all release tests before it can be officially declare as a new stable release. Please see the details in release-testing.</p>"},{"location":"direct/release/#step-by-step","title":"Step-by-Step","text":""},{"location":"direct/release/#reacting-to-a-new-kubeflex-release","title":"Reacting to a new KubeFlex release","text":"<ul> <li>Update the KubeFlex release in <code>go.mod</code></li> <li><code>go mod tidy</code></li> <li>Update the KubeFlex release in <code>core-chart/Chart.yaml</code></li> <li>Update the KubeFlex release everywhere it occurs in any of the <code>.github/workflows</code>:<ul> <li><code>.github/workflows/ocp-self-runner.yml</code></li> <li><code>.github/workflows/pr-test-e2e.yml</code></li> <li><code>.github/workflows/pr-test-integration.yml</code></li> <li><code>.github/workflows/test-latest-release.yml</code></li> </ul> </li> </ul> <p>Or you could search for appearances of the old release string yourself using a command like the following. And maybe also search for the release before that, in case it was overlooked earlier.</p> <pre><code>find * .github/workflows \\( -name \"*.svg\" -prune \\) -or \\( -path \"*venv\" -prune \\) -or \\( -path hack/tools -prune \\) -or \\( -type f -exec fgrep 0.6.2 \\{\\} \\; -print -exec echo \\; \\)\n</code></pre>"},{"location":"direct/release/#to-increase-the-lower-bound-on-kubeflex-release","title":"To increase the lower bound on KubeFlex release","text":"<ul> <li>Update the KubeFlex release in <code>docs/content/direct/pre-reqs.md</code></li> <li>Update the \"kflex\" release in <code>scripts/check_pre_req.sh</code></li> </ul>"},{"location":"direct/release/#reacting-to-a-new-ocm-status-addon-release","title":"Reacting to a new ocm-status-addon release","text":"<p>Between each release of ks/OSA and the next release of ks/ks, update the references to the ocm-status-addon release in the following files.</p> <ul> <li><code>core-chart/values.yaml</code></li> <li><code>monitoring/README.md</code></li> </ul>"},{"location":"direct/release/#making-a-new-kubestellar-release","title":"Making a new kubestellar release","text":"<p>Making a new kubestellar release requires a contributor to do the following things. Here <code>$version</code> is the semver identifier for the release (e.g., <code>1.2.3-rc2</code>).</p> <ul> <li> <p>If not already in effect, declare a code freeze. There should be nothing but bug fixes and doc improvements while working towards a regular release.</p> </li> <li> <p>Edit <code>docs/mkdocs.yml</code> and update the definition of <code>ks_latest_release</code> to <code>$version</code> (e.g., <code>'0.23.0-rc42'</code>). If this is a regular release then also update the definition of <code>ks_latest_regular_release</code>.</p> </li> <li> <p>Update the version in <code>scripts/check_pre_req.sh</code>.</p> </li> <li> <p>Update the version in the core chart defaults, <code>core-chart/values.yaml</code>.</p> </li> <li> <p>Update the version in <code>scripts/create-kubestellar-demo-env.sh</code>. Note: merging this change will cause the script to be broken until the release is made.</p> </li> <li> <p>Until we have our first stable release, edit the old docs README(<code>oldocs/README.md</code>, section \"latest-stable-release\") where it wishes it could cite a stable release but instead cites the latest release, to refer to the coming release.</p> </li> <li> <p>Edit the release notes in <code>docs/content/direct/release-notes.md</code>.</p> </li> <li> <p>Make a new Git commit with those changes and get it into the right branch in the shared repo (through the regular PR process if not authorized to cheat).</p> </li> <li> <p>Wait for successful completion of the testing after that merge.</p> </li> <li> <p>Apply the Git tag <code>v$version</code> to that new commit in the shared repo.</p> </li> <li> <p>After that, the \"goreleaser\" GitHub workflow then creates and publishes the artifacts for that release (as discussed above) and then the \"Test latest release\" workflow will run the E2E tests using those artifacts. </p> </li> <li> <p>Verify that the automatic tests indeed executed and passed (see more details in CICD release testing)</p> </li> <li> <p>After the release artifacts have been published, create and push to the shared repo a branch named <code>release-$version</code>. This will also trigger the workflow that tests the latest release. Every push to a branch with such a name triggers that workflow, in case there has been a change in an E2E test for that release.</p> </li> <li> <p>Follow the procedure in OCP testing, to verify that the release is functional on OCP.</p> </li> <li> <p>If the test results are good and the release is regular (not an RC) then declare the code freeze over.</p> </li> </ul>"},{"location":"direct/release/#goals-and-limitations","title":"Goals and limitations","text":"<p>The release process has the following goals.</p> <ul> <li>A release is identified using semantic versioning. This means that the associated semantics are followed, in terms of what sort of changes to the repo require what sort of changes to the release identifier.</li> <li>A user can pick up and use a given existing release without being perturbed by on-going contributor work. A release is an immutable thing.</li> <li>A release with a given semver identifier is built from a commit of this Git repository tagged with a tag whose name is \"v\" followed by the release identifier.</li> <li>The contents of <code>main</code> always work. This includes passing CI tests. This includes documentation being accurate. We allow point-in-time specific documentation, such as a document that says \"Here is how to use release 1.2.3\" --- which would refer to a release made in the past. We do not require the documentation in <code>main</code> to document all releases.</li> <li>A git tag is immutable. Once associated with a given Git commit, that association is not changed later.</li> <li>We do not put self-references into Git. For example, making release <code>1.2.3</code> does not require changing any file in Git to have the string <code>1.2.3</code> in it.</li> </ul> <p>We have the following limitations.</p> <ul> <li>The only way to publish artifacts (broadly construed, not (necessarily) GitHub \"release artifacts\") is to make a release.</li> <li>The only way to test published artifacts is to make a release and test it.</li> <li>Thus, it is necessary to keep users clearly appraised of the quality (or status of evaluating the quality) of each release.</li> <li>Because of the lack of self references, most user instructions (e.g., examples) and tests do not have concrete release identifiers in them; instead, the user has to chose and supply the release identifier. There can also be documentation of a specific past release (e.g., the latest stable release) that uses the literal identifier for that past release.</li> <li>PAY ATTENTION TO THIS ONE: Because of the prohibition of self references, Git will not contain the exact bytes of our Helm chart definitions. Where a Helm chart states its own version or has a container image reference to an image built from the same release, the bytes in Git have a placeholder for that image's tag and the process of creating the published release artifacts fills in that placeholder. Think of this as being analogous to the linking done when building a binary executable file.</li> <li>The design below falls short of the goal of not putting self-references in files under Git control. One place is in the core Helm chart's <code>values.yaml</code> file. Another is in the Getting Started setup instructions.</li> </ul>"},{"location":"direct/release/#dependency-cycle-with-ksotp","title":"Dependency cycle with ks/OTP","text":"<p>This is a thing of the past. The kubestellar/ocm-transport-plugin repository is retired now, its contents have been moved into the kubestellar/kubestellar repository.</p>"},{"location":"direct/release/#technology","title":"Technology","text":"<p>There is a GitHub workflow that creates the published artifacts for each Git tag whose name starts with \"v\". The rest of the tag name is required to be a semver release identifier. Note that this document does not (yet, anyway) specify how that GitHub workflow gets its job done. This workflow is confusingly named \"goreleaser\" and in a file named \"goreleaser.yml\" and has a job named \"goreleaser\" despite the fact that it does more than use goreleaser.</p> <p>For each tag <code>v$version</code> the following published artifacts will be created.</p> <ul> <li>The container image for the kubestellar-controller-manager (KCM), at <code>ghcr.io/kubestellar/kubestellar/controller-manager</code>. Image tag will be <code>$version</code>. This GitHub \"package\" will be connected to the ks/ks repo (this connection is something that an admin will do once, it will stick for all versions).</li> <li>The container image for the OCM transport-controller (OTC), at <code>ghcr.io/kubestellar/kubestellar/ocm-transport-controller</code>. Image tag will be <code>$version</code>. This GitHub \"package\" will be connected to the ks/ks repo (this connection is something that an admin will do once, it will stick for all versions).</li> <li>The core Helm chart, at <code>ghcr.io/kubestellar/kubestellar/core-chart</code> with version <code>$version</code> and Helm \"appVersion\" <code>$version</code>. This GitHub \"package\" will also be connected to the ks/ks repo. The chart has a reference to container image for the KCM and that reference is <code>ghcr.io/kubestellar/kubestellar/controller-manager:$version</code>. The chart also has a reference to container image for the OTC and that reference is <code>ghcr.io/kubestellar/kubestellar/ocm-transport-controller:$version</code>. In Git the chart has only placeholders in these places, not <code>$version</code>; the <code>$version</code> is inserted into a distinct copy by the GitHub workflow, which then publishes this specialized copy.</li> </ul>"},{"location":"direct/release/#website","title":"Website","text":"<p>We use <code>mike</code> and <code>MkDocs</code> to derive and publish GitHub pages. See <code>docs/README.md</code> for details.</p> <p>The published GitHub pages are organized into \"releases\".  Each release in the GitHub pages corresponds to a git branch whose name begins with \"release-\" or is \"main\".</p> <p>Our documentation is, mostly, viewable in either of two ways. The source documents can be viewed directly through GitHub's web UI for files. The other way is through the website.</p>"},{"location":"direct/release/#testing-and-examples","title":"Testing and Examples","text":"<p>The unit tests (of which we have almost none right now), integration tests (of which we also have very few), and end-to-end (E2E) tests in this repository are run in the context of a local copy of this repository and test that version of this repository --- not using any published release artifacts. Additionally, some E2E tests have the option to test published artifacts instead of the local copy of this repo.</p> <p>The end-to-end tests include ones written in <code>bash</code>, and these are the only documentation telling a user how to use the present version of this repository. Again, these tests do not use any published artifacts from a release of this repo.</p> <p>We have another category of tests, release tests. These test a given release, using the published artifacts of that release. These differ from the non-release tests only in the setup script, where it uses the published core Helm chart instead of the local version and uses published image tags rather than ephemeral local ones.</p> <p>We have GitHub workflows that exercise the E2E tests, normally on the copy of the repo that the workflow applies to. However, these workflows are parameterized and can be told to test the released artifacts instead.</p> <p>We also have a GitHub workflow, named \"Test latest release\" in <code>.github/workflows/test-latest-release.yml</code>, that invokes those E2E tests on the latest release. This workflow can be triggered manually, and is also configured to run after completion of the workflow (\"goreleaser\") that publishes release artifacts.</p> <p>We will maintain a document that lists releases that pass our quality bar. The latest of those is thus the latest stable release. This document is updated in <code>main</code> as quality evaluations come in.</p> <p>We used to maintain a statement of what is the latest stable release in <code>docs/content/direct/README.md</code>.</p> <p>We maintain a Getting Started document that tells users how to exercise the release that the document appears in. This requires a self-reference that is updated as part of the release process.</p>"},{"location":"direct/release/#policy","title":"Policy","text":"<p>We aim for all regular releases to be working. In order to do that, we have to make test releases and test them. The widely recognized pattern for doing that is to make \"release candidates\" (i.e., releases for testing purposes) <code>1.2.3-rc0</code>, <code>1.2.3-rc1</code>, <code>1.2.3-rc2</code>, and so on, while trying to get to a quality release <code>1.2.3</code>. Once one of them is judged to be of passing quality, we make a release without the <code>-rc&lt;N&gt;</code> suffix. Due to the self-references in the repo, this will involve making a new commit.</p> <p>Right after making a release we test it thoroughly.</p>"},{"location":"direct/release/#deliberately-feature-incomplete-releases","title":"Deliberately feature-incomplete releases","text":"<p>We plan a few deliberately feature-incomplete releases. They will be regular releases as far as the technology here is concerned. They will be announced only to selected users who acknowledge that they are getting something that is incomplete. In GitHub, these will be marked as \"pre-releases\". The status of these releases will be made clear in their documentation (which currently appears in the release notes.</p>"},{"location":"direct/release/#website_1","title":"Website","text":"<p>We aim to keep the documents viewable both through the website and GitHub's web UI for viewing files. We aim for all of the documentation to be reachable on the website and in the GitHub file UI starting from the repository's README.md.</p> <p>We create a release in the GitHub pages for every release. A patch release is a release. A test release is a release. Creating that GitHub pages release is done by creating a git branch named <code>release-$version</code>.</p>"},{"location":"direct/release/#future-process-development","title":"Future Process Development","text":"<p>We intend to get rid of the self-reference in the KCM PCH, as follows. Define a Helm chart for installing the PCH. Update the release workflow to specialize that Helm chart, similarly to the specialization done for the KCM Helm chart.</p>"},{"location":"direct/release/#open-questions","title":"Open questions","text":"<p>Exactly when does a new release branch diverge from <code>main</code>? What about cherry-picking between <code>main</code> and the latest (or also earlier?) release branch?</p> <p>What about the clusteradm container image?</p>"},{"location":"direct/roadmap/","title":"KubeStellar Roadmap","text":"<p>This document defines the KubeStellar feature roadmap. This document is a work-in-progress.</p>"},{"location":"direct/setup-limitations/","title":"Setup Limitations","text":"<p>Note:  This section is under construction and includes partial information.</p>"},{"location":"direct/setup-limitations/#size-considerations","title":"Size considerations","text":"<p>As KubeStellar is built on top of Kubernetes all Kubernetes limitations and recommendations apply to KubeStellar as well. These recommendations can be found in Kubernetes Considerations for large clusters.</p>"},{"location":"direct/setup-overview/","title":"Setting up KubeStellar","text":"<p>\"Setup\" is a porous grouping of some of the steps in the full outline, and comprises the following. Also, bear in mind the Setup limitations.</p> <ul> <li>Install software prerequisites. See prerequisites.</li> <li>KubeFlex Hosting cluster<ul> <li>Acquire the ability to use a Kubernetes cluster to serve as the KubeFlex hosting cluster. See Acquire cluster for KubeFlex hosting.</li> <li>Initialize that cluster as a KubeFlex hosting cluster.</li> </ul> </li> <li>Core Spaces<ul> <li>Create an Inventory and Transport Space (ITS).</li> <li>Create a Workload Description Space (WDS).</li> </ul> </li> <li>Core Helm Chart (covering three of the above topics).</li> <li>Workload Execution Clusters<ul> <li>Create a Workload Execution Cluster (WEC).</li> <li>Register the WEC in the ITS.</li> </ul> </li> </ul>"},{"location":"direct/start-from-ocm/","title":"Adding KubeStellar to OCM","text":"<p>In general, the key idea is to use the OCM hub cluster as the KubeStellar \"Inventory and Transport Space\" (ITS) and the KubeFlex hosting cluster.</p> <p>This page shows one concrete example of adding KubeStellar to an existing OCM system. In particular, a hub plus two managed clusters almost exactly as created by the OCM Quick Start instructions. In terms of the full Installation and Usage outline of KubeStellar, the modified OCM Quick Start has already: established some, but not all, of the software prerequisites; acquired the ability to use a Kube cluster as KubeFlex hosting cluster; created an Inventory and Transport Space; created two Workload Execution Clusters (WECs) and registered them. These are the boxes outlined in red in the following flowchart.</p> <p> this copy of the general installation and usage flowchart .</p> <ol> <li>Setup<ol> <li>Install remaining software prerequisites</li> <li>Cleanup from previous runs</li> <li>OCM Quick Start with Ingress</li> <li>Label WECs for selection by examples</li> <li>Install Kubestellar core components</li> </ol> </li> <li>Exercise KubeStellar</li> <li>Troubleshooting</li> </ol>"},{"location":"direct/start-from-ocm/#setup","title":"Setup","text":"<p>Continuing with the spirit of the OCM Quick Start, this is one way to produce a very simple system --- suitable for study but not production usage. For general setup information, see the full story.</p>"},{"location":"direct/start-from-ocm/#install-software-prerequisites","title":"Install software prerequisites","text":"<p>The following command will check for the prerequisites that KubeStellar will need for the later steps. See the prerequisites doc for more details.</p> <pre><code>bash &lt;(curl https://raw.githubusercontent.com/kubestellar/kubestellar/v0.28.0/scripts/check_pre_req.sh) kflex ocm helm kubectl docker kind\n</code></pre>"},{"location":"direct/start-from-ocm/#cleanup-from-previous-runs","title":"Cleanup from previous runs","text":"<p>If you have run this recipe or any related recipe previously then you will first want to remove any related debris. The following commands tear down the state established by this recipe.</p> <pre><code>kind delete cluster --name hub\nkind delete cluster --name cluster1\nkind delete cluster --name cluster2\nkubectl config delete-context cluster1\nkubectl config delete-context cluster2\n</code></pre> <p>After that cleanup, you may want to <code>set -e</code> so that failures do not go unnoticed (the various cleanup commands may legitimately \"fail\" if there is nothing to clean up).</p>"},{"location":"direct/start-from-ocm/#set-the-version-appropriately-as-an-environment-variable","title":"Set the Version appropriately as an environment variable","text":"<pre><code>kubestellar_version=0.28.0\n</code></pre>"},{"location":"direct/start-from-ocm/#ocm-quick-start-with-ingress","title":"OCM Quick Start with Ingress","text":"<p>This recipe uses a modified version of the OCM Quick Start script. The modification is necessary because KubeStellar requires the hosting cluster to have an Ingress controller with SSL passthrough enabled. The modified Quick Start script has the following modifications compared to the baseline.</p> <ol> <li>The <code>kind</code> cluster created for the hub has an additional port mapping, where the Ingress controller listens.</li> <li>The script installs the NGINX Ingress Controller into the hub cluster, then patches the controller to enable SSL passthrough, and later waits for it to be in service.</li> </ol> <p>You can invoke the modified OCM Quick Start as follows.</p> <pre><code>curl -L https://raw.githubusercontent.com/kubestellar/kubestellar/refs/tags/v0.28.0/scripts/ocm-local-up-for-ingress.sh | bash\n</code></pre> <p>Like the baseline, this script creates a <code>kind</code> cluster named \"hub\" to serve as hub cluster (known in KubeStellar as an Inventory and Transport Space, ITS) and two <code>kind</code> clusters named \"cluster1\" and \"cluster2\" to serve as managed clusters (known in KubeStellar as Workload Execution Clusters, WECs), and registers them in the hub.</p>"},{"location":"direct/start-from-ocm/#label-wecs-for-selection-by-examples","title":"Label WECs for selection by examples","text":"<p>The examples will use label selectors to direct workload to WECs (in terms of their ManagedCluster representations). The following commands apply labels that will be used in the examples.</p> <pre><code>kubectl --context kind-hub label managedcluster cluster1 location-group=edge name=cluster1\nkubectl --context kind-hub label managedcluster cluster2 location-group=edge name=cluster2\n</code></pre>"},{"location":"direct/start-from-ocm/#use-core-helm-chart-to-initialize-kubeflex-recognize-its-and-create-wds","title":"Use Core Helm chart to initialize KubeFlex, recognize ITS, and create WDS","text":"<p>This chart instance will do the following.</p> <ul> <li>Install KubeFlex in the hosting cluster.</li> <li>Assign the hosting cluster the role of an ITS, named \"its1\".</li> <li>Create a KubeFlex ControlPlane named \"wds1\" to play the role of a WDS.</li> <li>Install the KubeStellar core stuff in the hosting cluster.</li> </ul> <pre><code>helm --kube-context kind-hub upgrade --install ks-core oci://ghcr.io/kubestellar/kubestellar/core-chart \\\n    --version $kubestellar_version \\\n    --set-json='ITSes=[{\"name\":\"its1\", \"type\":\"host\"}]' \\\n    --set-json='WDSes=[{\"name\":\"wds1\"}]' \\\n    --set-json='verbosity.default=5' # so we can debug your problem reports\n</code></pre> <p>That command will print some notes about how to get a kubeconfig \"context\" named \"wds1\" defined. Do that, because this context is used in the steps that follow. The notes assume that your current kubeconfig context is the one where the Helm chart was installed, which is not necessarily true --- so take care for that too.</p> <pre><code>kubectl config use-context kind-hub\nkflex ctx --set-current-for-hosting # make sure the KubeFlex CLI's hidden state is right for what the Helm chart just did\nkflex ctx --overwrite-existing-context wds1\n</code></pre> <p>For more information about this Helm chart, see its documentation.</p>"},{"location":"direct/start-from-ocm/#exercise-kubestellar","title":"Exercise KubeStellar","text":"<p>Use the following commands to wait for the KubeStellar core Helm chart to finish setting up the WDS, because the examples assume that this has completed.</p> <pre><code>while [ -z \"$(kubectl --context wds1 get crd bindingpolicies.control.kubestellar.io --no-headers -o name 2&gt; /dev/null)\" ] ;  do\n    sleep 5\ndone\nkubectl --context wds1 wait --for condition=Established crd bindingpolicies.control.kubestellar.io\n</code></pre> <p>Proceed to Scenario 1 (multi-cluster workload deployment with kubectl) in the example scenarios after defining the shell variables that characterize the setup done above. Following are the settings for those variables, whose meanings are defined at the start of the example scenarios document.</p> <pre><code>host_context=kind-hub\nits_cp=its1\nits_context=${host_context}\nwds_cp=wds1\nwds_context=wds1\nwec1_name=cluster1\nwec2_name=cluster2\nwec1_context=kind-$wec1_name\nwec2_context=kind-$wec2_name\nlabel_query_both=location-group=edge\nlabel_query_one=name=cluster1\n</code></pre>"},{"location":"direct/start-from-ocm/#troubleshooting","title":"Troubleshooting","text":"<p>In the event something goes wrong, check out the troubleshooting page to see if someone else has experienced the same thing</p>"},{"location":"direct/teardown/","title":"Teardown","text":"<p>This document describes a way to tear down a KubeStellar system, back to the point where the WECs and KubeFlex hosting cluster exist but have nothing from OCM, KubeFlex, and KubeStellar installed in them. You may find it useful to refer to the setup and usage flowchart.</p> <p>There is a documented procedure for detaching an OCM \"managed cluster\" from an OCM \"hub cluster\" on the OCM website. In my experience this hangs if invoked while the managed cluster has any workload objects being managed by the klusterlet. So teardown has to proceed in mincing steps, first removing workload from the WECs before detaching them from their ITS.</p>"},{"location":"direct/teardown/#preparation","title":"Preparation","text":"<p>You need to be clear on what is the KubeFlex hosting cluster and have a kubeconfig context that refers to that cluster. Put the name of that context in the shell variable <code>host_context</code>.</p> <p>Next, get a list of the KubeFlex ControlPlanes; a command like the following will do that.</p> <pre><code>kubectl --context $host_context get controlplanes\n</code></pre> <p>Following is an example of the output from such a command. Note that the output that you should expect to see depends on the ITSes and WDSes that you have defined.</p> <pre><code>NAME   SYNCED   READY   TYPE       AGE\nits1   True     True    vcluster   196d\nwds0   True     True    host       186d\n</code></pre>"},{"location":"direct/teardown/#teardown-procedure-depends-on-helm-charts-used","title":"Teardown procedure depends on Helm charts used","text":"<p>If the KubeStellar core Helm chart was used then you will be deleting the instances of that, and this relieves you of having to do some deletions directly.</p>"},{"location":"direct/teardown/#deleting-the-wdses","title":"Deleting the WDSes","text":"<p>In the KubeFlex hosting cluster delete each <code>ControlPlane</code> object for a WDS that was not created by an instance of the KubeStellar core Helm chart.</p>"},{"location":"direct/teardown/#deleting-the-ocm-workload-and-addon","title":"Deleting the OCM workload and addon","text":"<p>For each ITS (remember, a KubeStellar ITS is an OCM hub cluster) you need to make sure that all the OCM workload and addons are gone.</p> <p>Get a kubeconfig context for accessing the ITS, either using the KubeFlex CLI <code>kflex</code> or by directly reading the relevant <code>ControlPlane</code> and <code>Secret</code> objects. See the KubeFlex user guide for details.</p> <p>Get a listing of <code>ManifestWork</code> objects in the ITS. The following command will do that, supposing that the current kubeconfig file has a context for accessing the ITS and the context's name is in the shell variable <code>its_context</code>.</p> <pre><code>kubectl --context $its_context get manifestworks -A\n</code></pre> <p>Following is an example of the output to expect; it shows that KubeStellar's OCM Status Addon is still installed.</p> <pre><code>NAMESPACE                NAME                          AGE\nedgeplatform-test-wec1   addon-addon-status-deploy-0   196d\nedgeplatform-test-wec2   addon-addon-status-deploy-0   196d\n</code></pre> <p>Delete any <code>ManifestWork</code> objects that are NOT from KubeStellar's OCM Status Addon. The ones from that addon are maintained by that addon, so it is not effective for you to simply delete them.</p> <p>KubeStellar's OCM Status Addon was installed by a Helm chart from ks/OSA. When using KubeStellar's core Helm chart, the OSA chart is instantiated by a command run inside a container of a <code>Job</code>, so merely deleting the core chart instance does not remove the OSA chart instance; you must do it yourself.</p> <p>Following is an example of a command that lists the Helm chart instances (\"releases\", in Helm jargon) in an ITS.</p> <pre><code>helm --kube-context $its_context list -A\n</code></pre> <p>Following is an example of output from that command.</p> <pre><code>NAME            NAMESPACE               REVISION    UPDATED                                 STATUS      CHART                               APP VERSION\nstatus-addon    open-cluster-management 1           2024-06-04 02:37:37.505803572 +0000 UTC deployed    ocm-status-addon-chart-v0.2.0-rc9   v0.2.0-rc9\n</code></pre> <p>Following is an example of a command to delete such a chart instance.</p> <pre><code>helm --kube-context $its_context delete status-addon -n open-cluster-management\n</code></pre> <p>Check whether that got the <code>ManifestWork</code> objects deleted, using a command like the following.</p> <pre><code>kubectl --context $its_context get manifestworks -A\n</code></pre> <p>You will probably find that they are still there. So delete them explicitly, with a command like the following.</p> <pre><code>kubectl --context $its_context delete manifestworks -A --all\n</code></pre>"},{"location":"direct/teardown/#removing-ocm-from-the-wecs","title":"Removing OCM from the WECs","text":"<p>The OCM website has instructions for disconnecting a managed cluster from its hub. Do this.</p> <p>Check whether all traces of OCM are gone; you can use a command like the following, supposing that your current kubeconfig file has a context for accessing the WEC and the context's name is in the shell variable <code>wec_context</code>.</p> <pre><code>kubectl --context $wec_context get ns  | grep open-cluster\n</code></pre> <p>You will probably find that OCM is not all gone; following is example output.</p> <pre><code>open-cluster-management                            Active   196d\n</code></pre> <p>If you find that Namespace remaining, delete it. You can use a command like the following; it may take a few tens of seconds to complete.</p> <pre><code>kubectl --context $wec_context delete ns open-cluster-management\n</code></pre>"},{"location":"direct/teardown/#deleting-crds-and-their-instances-in-the-wecs","title":"Deleting CRDs and their instances in the WECs","text":"<p>The steps above still leave some <code>CustomResourceDefinition</code> (CRD) objects from OCM in the WECs. You should remove those, and their instances.  See Deleting CRDs and their instances in the KubeFlex hosting cluster, but do it for the WECs instead of the KubeFlex hosting cluster and for CRDs from OCM instead of from KubeStellar (grep for \"open-cluster-management\" instead of \"kubestellar\").</p>"},{"location":"direct/teardown/#deleting-the-itses","title":"Deleting the ITSes","text":"<p>Now that there is no trace of OCM left in the WECs, it is safe to delete the ITSes. In the KubeFlex hosting cluster, delete each ITS <code>ControlPlane</code> object that was not created due to an instance of the KubeStellar core Helm chart.</p>"},{"location":"direct/teardown/#deleting-remaining-stuff-in-the-kubeflex-hosting-cluster","title":"Deleting Remaining Stuff in the KubeFlex hosting cluster","text":""},{"location":"direct/teardown/#delete-helm-chart-instances-in-the-kubeflex-hosting-cluter","title":"Delete Helm chart instances in the KubeFlex hosting cluter","text":"<p>Look at the Helm chart instances in the KubeFlex hosting cluster. You might use a command like the following.</p> <pre><code>helm --kube-context $host_context list -A\n</code></pre> <p>The output may look something like the following.</p> <pre><code>NAME                            NAMESPACE               REVISION    UPDATED                                 STATUS      CHART                                       APP VERSION  \n...\nkubeflex-operator               kubeflex-system         1           2024-06-03 22:35:24.360137 -0400 EDT    deployed    kubeflex-operator-v0.6.2                    v0.6.2       \npostgres                        kubeflex-system         1           2024-06-03 22:33:33.210041 -0400 EDT    deployed    postgresql-13.1.5                           16.0.0       \n...\n</code></pre> <p>If you used the KubeStellar core chart then one or more instances of it will be in that list. In this case you can simply delete those Helm chart instances. If the core chart was not used to install KubeFlex in its hosting cluster then you will need to delete the <code>kubeflex-system</code> Helm chart instance. That will probably not delete the postgress chart instance. If that remains, delete it too.</p> <p>Following is an example of a command that deletes a Helm chart instance.</p> <pre><code>helm --kube-context $host_context delete kubeflex-operator -n kubeflex-system\n</code></pre> <p>Next, delete the <code>kubeflex-system</code> Namespace. That should be the only one that you can find related to KubeFlex or KubeStellar.</p>"},{"location":"direct/teardown/#deleting-crds-and-their-instances-in-the-kubeflex-hosting-cluster","title":"Deleting CRDs and their instances in the KubeFlex hosting cluster","text":"<p>Deleting those Helm chart instances does not remove the <code>CustomResourceDefinition</code> (CRD) objects created by containers in those charts. You have to delete those by hand. If I recall correctly: before deleting the definition of a custom resource, you should (for the sake of not leaving junk in the underlying object storage, which is bad enough in itself and could be a problem if you later introduce a different definition for a resource of the same name) also delete instances (objects) of that resource. The first step is to get a listing of all the KubeStellar CRDs. You could do that with a command like the following.</p> <pre><code>kubectl --context $host_context get crds | grep kubestellar\n</code></pre> <p>Following is an example of output from such a command.</p> <pre><code>bindingpolicies.control.kubestellar.io           2024-03-01T22:03:47Z\nbindings.control.kubestellar.io                  2024-03-01T22:03:48Z\ncampaigns.stacker.kubestellar.io                 2024-02-26T21:10:25Z\nclustermetrics.galaxy.kubestellar.io             2024-06-05T15:07:08Z\ncontrolplanes.tenancy.kflex.kubestellar.org      2024-06-03T13:58:14Z\ncustomtransforms.control.kubestellar.io          2024-06-04T01:29:16Z\ngalaxies.stacker.kubestellar.io                  2024-02-26T21:10:25Z\nmissions.stacker.kubestellar.io                  2024-02-26T21:10:26Z\nplacements.control.kubestellar.io                2024-02-14T16:26:00Z\nplacements.edge.kubestellar.io                   2024-02-14T13:23:44Z\npostcreatehooks.tenancy.kflex.kubestellar.org    2024-06-03T13:58:14Z\nstars.stacker.kubestellar.io                     2024-02-26T21:10:26Z\nuniverses.stacker.kubestellar.io                 2024-02-26T21:10:27Z\n</code></pre> <p>Next, for each one of those CRD objects, find and delete the instances of the resource that it defines. Following is an example of a command that gets a list for a given custom resource; this is not strictly necessary (see the delete command below), but you may want to do it for your own information.</p> <pre><code>kubectl --context $host_context get -A postcreatehooks.tenancy.kflex.kubestellar.org\n</code></pre> <p>Following is an example of output from such a command.</p> <pre><code>NAME             SYNCED   READY   TYPE   AGE\nkubestellar                              196d\nocm                                      196d\nopenshift-crds                           196d\n</code></pre> <p>You can delete all of the instances of a given resource with a command like the following (which works for both cluster-scoped and namespaced resources).</p> <pre><code>kubectl --context $host_context delete -A --all postcreatehooks.tenancy.kflex.kubestellar.org\n</code></pre> <p>Following is an example of output from such a command.</p> <pre><code>postcreatehook.tenancy.kflex.kubestellar.org \"kubestellar\" deleted\npostcreatehook.tenancy.kflex.kubestellar.org \"ocm\" deleted\npostcreatehook.tenancy.kflex.kubestellar.org \"openshift-crds\" deleted\n</code></pre>"},{"location":"direct/testing/","title":"Testing","text":"<p>Make sure all pre-requisites are installed as described in pre-reqs.</p>"},{"location":"direct/testing/#unit-testing","title":"Unit testing","text":"<p>The Makefile has a target for running all the unit tests.</p> <pre><code>make test\n</code></pre>"},{"location":"direct/testing/#integration-testing","title":"Integration testing","text":"<p>There are currently three integration tests. Contributors can run them. There is also a GitHub Actions workflow (in <code>.github/workflows/pr-test-integration.yml</code>) that runs these tests.</p> <p>These tests require you to already have <code>etcd</code> on your <code>$PATH</code>. See https://github.com/kubernetes/kubernetes/blob/v1.29.10/hack/install-etcd.sh for an example of how to do that.</p> <p>To run the tests sequentially, issue a command like the following.</p> <pre><code>CONTROLLER_TEST_NUM_OBJECTS=24 go test -v ./test/integration/controller-manager &amp;&gt; /tmp/test.log\n</code></pre> <p>If <code>CONTROLLER_TEST_NUM_OBJECTS</code> is not set then the number of objects will be 18. This parameterization by an environment variable is only a point-in-time hack, it is expected to go away once we have a test that runs reliably on a large number of objects.</p> <p>To run one of the individual tests, issue a command like the following example.</p> <pre><code>go test -v -timeout 60s -run ^TestCRDHandling$ ./test/integration/controller-manager\n</code></pre>"},{"location":"direct/testing/#end-to-end-testing","title":"End-to-end testing","text":"<p>See <code>test/e2e/</code> in the GitHub repository. It has a README.</p>"},{"location":"direct/testing/#testing-releases","title":"Testing releases","text":"<p>See the release testing doc.</p>"},{"location":"direct/transforming/","title":"Transforming Desired State","text":"<p>This document is for users of a release. Examples of using the latest release are in the example scenarios document. This document adds information not conveyed in the examples.</p> <p>KubeStellar has two kinds of transformations of desired workload state on its way from WDS to WEC: one kind is independent of the WEC, and the other supports variation from WEC to WEC.</p>"},{"location":"direct/transforming/#wec-independent-workload-object-transformation","title":"WEC-independent workload object transformation","text":"<p>KubeStellar does some transformation of workload objects on their way from WDS to WEC. First, there are transformations that are independent of the destination; these are described in this section. Second, there is customization to the WEC, described later.</p> <p>The WEC-independent transformations are removal of certain content.</p> <p>There are three categories of these transformations, as follows. They are applied in this order.</p> <ol> <li>Transformations that are built into KubeStellar and apply to all workload objects.</li> <li>Transformations that are built into KubeStellar and apply to specific kinds of workload objects.</li> <li>Transformations that are configured by control objects and apply to specific kinds of workload objects.</li> </ol>"},{"location":"direct/transforming/#transformations-for-all-workload-objects","title":"Transformations for all workload objects","text":"<p>The following are applied to every workload object.</p> <ol> <li>Remove the following fields from <code>metadata</code>: <code>managedFields</code>, <code>finalizers</code>, <code>generation</code>, <code>ownerReferences</code>, <code>selfLink</code>, <code>resourceVersion</code>, <code>UID</code>, <code>generateName</code>.</li> <li>Remove the annotation named <code>kubectl.kubernetes.io/last-applied-configuration</code>.</li> <li>Remove the <code>status</code>.</li> </ol>"},{"location":"direct/transforming/#built-in-transformations-of-specific-kinds-of-workload-object","title":"Built-in transformations of specific kinds of workload object","text":"<p>In a <code>Service</code> (core API group) object:</p> <ol> <li> <p>remove the following fields from <code>spec</code>: <code>ipFamilies</code>, <code>externalTrafficPolicy</code>, <code>internalTrafficPolicy</code>, <code>ipFamilyPolicy</code>, <code>sessionAffinity</code>. Also remove the <code>nodePort</code> field from every port unless the annotation <code>control.kubestellar.io/preserve=nodeport</code> is present.</p> </li> <li> <p>in the <code>spec</code> remove the field <code>clusterIP</code> unless it is present with value \"None\".</p> </li> <li> <p>in the <code>spec</code>: if the field <code>clusterIPs</code> (which holds an array of strings) is present and those strings include \"None\" then keep it present holding only \"None\", otherwise remove that field if it is present.</p> </li> </ol> <p>In a <code>Job</code> (API group <code>batch</code>) object, remove the following things.</p> <ol> <li> <p><code>spec.selector</code></p> </li> <li> <p><code>spec.suspended</code></p> </li> <li> <p>In <code>metadata</code>, the annotation named <code>batch.kubernetes.io/job-tracking</code></p> </li> <li> <p>In <code>metadata</code> and in <code>spec.template.metadata</code>, the labels named <code>controller-uid</code> or <code>batch.kubernetes.io/controller-uid</code>.</p> </li> </ol>"},{"location":"direct/transforming/#configured-transformation-of-workload-objects","title":"Configured transformation of workload objects","text":"<p>The user can configure additional transformations of workload objects by putting <code>CustomTransform</code> (in the <code>control.kubestellar.io</code> API group) objects in the WDS. Each <code>CustomTransform</code> object binds to certain workload objects and specifies certain transformations.</p> <p>Currently the binding is simply by naming the workload object's API group and \"resource\" name in the <code>CustomTransform</code>'s <code>spec</code>. The transformations from all of the bound <code>CustomTransform</code> objects are applied to the workload object. There should be at most one <code>CustomTransform</code> object that specifies a given API group and resource.</p> <p>Currently the only available transformations are removals of specified content. The content to be removed is identified by a small subset of JSONPath (which was originally and somewhat loosely defined in an article by Stefan Goessner and later defined more carefully in RFC 9535). In the subset accepted here: the root node identifier (<code>$</code>) must be followed by a positive number of segments, where each segment is either (a) <code>.</code> and a name (a <code>member-name-shorthand</code>, in the grammar of the RFC) or (b) <code>[</code>, a string literal, and <code>]</code>; no more of the grammar is allowed, not even whitespace. The allowed names and string literals are as specified in RFC 9535, except that only double-quoted strings are allowed.</p> <p>For example, the following <code>CustomTransform</code> object says to remove the <code>spec</code> field named <code>suspend</code> from <code>Job</code> objects (in the API group <code>batch</code>).</p> <pre><code>apiVersion: control.kubestellar.io/v1alpha1\nkind: CustomTransform\nmetadata:\n  name: example\nspec:\n  apiGroup: batch\n  resource: jobs\n  remove:\n  - \"$.spec.suspend\"\n</code></pre>"},{"location":"direct/transforming/#rule-based-customization","title":"Rule-based customization","text":"<p>KubeStellar can distribute one workload object to multiple WECs, and it is common for users to need some customization to each WEC. By rule based we mean that the customization is not expressed via one or more literal expressions but rather can refer to properties of each WEC by property name. As KubeStellar distributes or transports a workload object from WDS to a WEC, the object can be transformed in a way that depends on those properties.</p> <p>At its current level of development, KubeStellar has a simple but limited way to specify rule-based customization, called \"template expansion\".</p>"},{"location":"direct/transforming/#template-expansion","title":"Template Expansion","text":"<p>Template expansion is an optional feature that a user can request on an object-by-object basis. The way to request this feature on an object is to put the following annotation on the object.</p> <pre><code>    control.kubestellar.io/expand-templates: \"true\"\n</code></pre> <p>The customization that template expansion does when distributing an object from a WDS to a WEC is applied independently to each leaf string of the object and is based on the \"text/template\" standard package of Go. The string is parsed as a template and then replaced with the result of expanding the template. Errors from this process are reported in the status field of the Binding object involved. Errors during template expansion usually produce broken YAML, in which case no corresponding object will be created in the WEC.</p> <p>The data used when expanding the template are properties of the WEC. These properties are collected from the following four sources, which are listed in decreasing order of precedence.</p> <ol> <li>The ConfigMap object, if any, that is in the namespace named \"customization-properties\" in the ITS and has the same name as the inventory object for the WEC. In particular, the ConfigMap string and binary data items whose name is valid as a Go language identifier supply properties.</li> <li>The annotations of the inventory item for the WEC supply properties if the annotation's name (AKA key) is valid as a Go language identifier.</li> <li>The labels of the inventory item for the WEC supply properties if the label's name (AKA key) is valid as a Go language identifier.</li> <li>There is a pre-defined property whose name is \"clusterName\" and whose value is the name of the inventory item (i.e., the <code>ManagedCluster</code> object) for the WEC.</li> </ol> <p>A Binding object's <code>status</code> section has a field holding a slice of error message strings reporting user errors that arose the last time the transport controller processed that Binding, along with the <code>observedGeneration</code> reporting the <code>metadata.generation</code> that was processed. For each workload object that the Binding references: if template expansion reports errors for any destinations, the errors reported for the first such destination are included in the Binding object's status.</p> <p>Any failure in any template expansion for a given Binding suppresses propagation of desired state from that Binding; the previously propagated desired state from that Binding, if any, remains in place in the WEC.</p> <p>Template expansion can only be applied when and where the unexpanded leaf strings pass the validation that the WDS applies, and can only express substring replacements.</p> <p>For example, consider the following example workload object.</p> <pre><code>apiVersion: logging.openshift.io/v1\nkind: ClusterLogForwarder\nmetadata:\n  name: instance\n  namespace: openshift-logging\n  annotations:\n    control.kubestellar.io/expand-templates: \"true\"\nspec:\n  outputs:\n    - name: remote-loki\n      type: loki\n      url: \"https://my.loki.server.com/{\\u007B .clusterName }}-{\\u007B.clusterHash}}\"\n...\n</code></pre> <p>(Note: \"{\\u007B\" is JSON for a string consisting of two consecutive left curly brackets --- which mkdocs does not have a way to quote inside a fenced code block.)</p> <p>The following ConfigMap in the ITS provides a value for the <code>clusterHash</code> property.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: customization-properties\n  name: virgo\ndata:\n  clusterHash: 1001-dead-beef\n...\n</code></pre> <p>When distributed to the virgo WEC, that ClusterLogForwarder would say the following.</p> <pre><code>...\n      url: \"https://my.loki.server.com/virgo-1001-dead-beef\"\n...\n</code></pre>"},{"location":"direct/troubleshooting/","title":"Troubleshooting","text":"<p>This guide is a work in progress.</p>"},{"location":"direct/troubleshooting/#debug-log-levels","title":"Debug log levels","text":"<p>The KubeStellar controllers take an optional command line flag that sets the level of debug logging to emit. Each debug log message is associated with a log level, which is a non-negative integer. Higher numbers correspond to messages that appear more frequently and/or give more details. The flag's name is <code>-v</code> and its value sets the highest log level that gets emitted; higher level messages are suppressed.</p> <p>The KubeStellar debug log messages are assigned to log levels roughly according to the following rules. Note that the various Kubernetes libraries used in these controllers also emit leveled debug log messages, according to their own numbering conventions. The KubeStellar rules are designed to be mostly consistent with the Kubernetes practice.</p> <ul> <li>0: messages that appear O(1) times per run.</li> <li>1: more detailed messages that appear O(1) times per run.</li> <li>2: messages that appear O(1) times per lifecycle event of an API object or important conjunction of them (e.g., when a Binding associates a workload object with a WEC).</li> <li>3: more detailed messages that appear O(1) times per lifecycle event of an API object or important conjunction of them.</li> <li>4: messages that appear O(1) times per sync. A sync is when a controller reads the current state of one API object and reacts to that.</li> <li>5: more detailed messages that appear O(1) times per sync.</li> </ul> <p>The core Helm chart has \"values\" that set the verbosity (<code>-v</code>) of various controllers.</p>"},{"location":"direct/troubleshooting/#things-to-look-at","title":"Things to look at","text":"<ul> <li>Existence and version of dependencies. There is a document and a checking script (<code>scripts/check_pre_req.sh</code>).</li> <li>While double-checking your input is never bad, using <code>kubectl get -o yaml --show-managed-fields</code> to examine the live API objects adds some good stuff: confirmation that your input was received and parsed as expected, display of any error messages in your API objects, timestamps in the metadata (helpful for comparing with log messages), indication of what last wrote to each part of your API objects and when.</li> <li>When basic stuff is not working, survey the Pod objects in the KubeFlex hosting cluster to look for ones that are damaged in some way. For example: you can get a summary with the command <code>kubectl --context kind-kubeflex get pods -A</code> --- adjust as necessary for the name of your kubeconfig context to use for the KubeFlex hosting cluster.</li> <li>Remember that for each of your BindingPolicy objects, there is a corresponding Binding object that reports what is matching the policy object.</li> <li>Although not part of the interface, when debugging you can look at the ManifestWork and WorkStatus objects in the ITS.</li> <li>More broadly, remember that KubeStellar uses OCM.</li> <li>Look at logs of controllers. If they have had container restarts that look relevant, look also at the previous logs. Do not forget OCM controllers. Do not forget that some Pods have more than one interesting container.<ul> <li>Remember that the amount of log retained is typically a configured option in the relevant container runtime. If your logs are too short, look into increasing that log retention.</li> </ul> </li> <li>If a controller's <code>-v</code> is not at least 5, increase it.</li> <li>Remember that Kubernetes controllers tend to report transient problems as errors without making it clear that the problem is transient and tend to not make it clear if/when the problem has been resolved (sigh).</li> </ul>"},{"location":"direct/troubleshooting/#some-known-problems","title":"Some known problems","text":"<p>We have the start of a list.</p>"},{"location":"direct/troubleshooting/#making-a-good-trouble-report","title":"Making a good trouble report","text":"<p>Basic configuration information.</p> <ul> <li>Include the versions of all the relevant software; do not forget the OCM pieces.</li> <li>Report on each Kubernetes/OCP cluster involved. What sort of cluster is it (kind, k3d, OCP, ...)? What version of that?</li> <li>For each WDS and ITS involved, report on what sort of thing is playing that role (remember that a Space is a role) --- a new KubeFlex control plane (report type) or an existing cluster (report which one).</li> </ul> <p>Do a simple clean demonstration of the problem, if possible.</p> <p>Show the particulars of something going wrong.</p> <ul> <li>Show a shell session, starting from scratch.</li> <li>Show a run of <code>scripts/check_pre_req.sh</code>.</li> <li>Report timestamps of when salient changes happened. Make it clear which timezone is involved in each one. Particularly interesting times are when KubeStellar did the wrong thing or failed to do anything at all in response to something.</li> <li>Show the relevant API objects. When the problem is behavior over time, show the objects contents from before and after the misbehavior.<ul> <li>In the WDS: the workload objects involved; any <code>BidingPolicy</code> involved, and the corresponding <code>Binding</code> for each; any <code>CustomTransform</code>, <code>StatusCollector</code>, or <code>CombinedStatus</code> involved.</li> <li>Any involved objects in the WEC(s).</li> <li>Implementation objects in the ITS: <code>ManifestWork</code>, <code>WorkStatus</code>.</li> <li>Here is one way to show the evolution of a relevant set of objects over time. The following command displays the <code>ManifestWork</code> objects, after creation and after each update (modulo the gaps allowed by eventual consistency), in an ITS as addressed by the kubeconfig context named <code>its1</code> --- after first listing the existing objects. Each line is prefixed with the hour:minute:second at which it appears.     <pre><code>kubectl --context its1 get manifestworks -A --show-managed-fields -o yaml --watch | while IFS=\"\" read line; do echo \"$(date +%T)| $line\"; done\n</code></pre></li> </ul> </li> <li>When reporting kube API object contents, include the <code>meta.managedFields</code>. For example, when using <code>kubectl get</code>, include <code>--show-managed-fields</code>.</li> <li>Show the logs from relevant controllers. The most active and directly relevant ones are the following.<ul> <li>The KubeStellar controller-manager (running in the KubeFlex hosting cluster) for the WDS</li> <li>KubeStellar's OCM-based transport-controller (running in the KubeFlex hosting cluster) for the WDS+ITS</li> <li>The OCM Status Add-On Agent in the WEC.</li> <li>OCM's klusterlet-agent in the WEC.</li> </ul> </li> </ul>"},{"location":"direct/troubleshooting/#use-the-snapshot-script","title":"Use the snapshot script","text":"<p>There is a script that is intended to capture a lot of relevant state; using it can help make a good trouble report (but remember all of the ideas above).</p> <p>You can use a command like the following to invoke the script.</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/kubestellar/kubestellar/refs/heads/main/scripts/kubestellar-snapshot.sh) -V -Y -L\n</code></pre> <p>Report the log of running the script.</p> <p>If the script is successful then it will create an archive file and tell you about it; include that file in your trouble report.</p>"},{"location":"direct/ui-intro/","title":"KubeStellar UI (User Interface)","text":"<p> KubeStellarUI Splash Page  The KubeStellar UI is an add-on developed to make managing workloads via KubeStellar even simpler and more intuitive. With its web-based interface, you can view and manage your Workload Definition Space, Inventory and Transport Space, and Binding Policies all interactively, with both drag-and-drop and text-based interface modes available for use.</p>"},{"location":"direct/ui-intro/#learn-more","title":"Learn More","text":"<p>The Kubestellar UI has its own section in our User Guide</p> <p>To explore more fully under the covers, visit the UI repository at https://github.com/kubestellar/ui</p>"},{"location":"direct/usage-limitations/","title":"Usage Limitations","text":"<p>Note:  This section is under construction and includes partial information.</p>"},{"location":"direct/usage-limitations/#size-considerations","title":"Size considerations","text":"<p>The KubeStellar Transport Plugin is built on top of OCM, so KubeStellar also comply to some of OCM's limitations. Users should take into account the following restrictions:</p> <ul> <li>The ManifestWork shouldn't exceed 500KB</li> </ul>"},{"location":"direct/user-guide-intro/","title":"KubeStellar User Guide","text":"<p>This document is an overview of the User Guide. See the KubeStellar overview for architecture and other information.</p> <p>This user guide is an ongoing project. If you find errors, please point them out in our Slack channel or open an issue in our github repository!</p>"},{"location":"direct/user-guide-intro/#simple-examples","title":"Simple Examples","text":"<p>If you want to try a simple installation process and example then you can try out Getting Started, which uses kind and a helm chart. The helm chart supports many options; the instructions on the Getting Started page show only the chart's usage in that recipe.</p> <p>Another simple example, which starts with (a slightly modified version of) the OCM Quick Start is here.</p>"},{"location":"direct/user-guide-intro/#in-brief","title":"In Brief","text":"<p>If you want a simple rough grouping, you can divide the concepts here into:</p> <ul> <li>\"setup\" (steps 1--7 below), exemplified in the Setup section of Getting Started, and</li> <li>\"usage\" (the remaining steps), illustrated by the example scenarios document.</li> </ul> <p>However, you do not need to follow that dichotomy. As noted below, the relevant components can be organized more flexibly.</p>"},{"location":"direct/user-guide-intro/#the-full-story","title":"The Full Story","text":"<p>Installing and using KubeStellar progresses through the following steps.</p> <ol> <li>Install software prerequisites. See prerequisites.</li> <li>Acquire the ability to use a Kubernetes cluster to serve as the KubeFlex hosting cluster. See Acquire cluster for KubeFlex hosting.</li> <li>Initialize that cluster as a KubeFlex hosting cluster.</li> <li>Inventory and Transport Space (ITS).<ol> <li>Create something to serve as ITS.</li> <li>Register the ITS as a KubeFlex ControlPlane.</li> </ol> </li> <li>Workload Description Space (WDS).<ol> <li>Create something to serve as WDS.</li> <li>Register the WDS as a KubeFlex ControlPlane and initialize it for KubeStellar usage.</li> </ol> </li> <li>Create a Workload Execution Cluster (WEC).</li> <li>Register the WEC in the ITS.</li> <li>Maintain workload desired state in the WDS.</li> <li>Maintain control objects in the WDS to bind workload with WEC and modulate the state propagation back and forth. The API reference documents all of them. There are control objects for the following topics.<ol> <li>Binding workload with WEC(s).</li> <li>Transforming desired state as it travels from WDS to WEC.</li> <li>Summarizing reported state from WECs into WDS.</li> </ol> </li> <li>Enjoy the effects of workloads being propagated to the WEC.</li> <li>Consume reported state from WDS.</li> </ol> <p>By \"maintain\" we mean create, read, update, delete, list, and watch as you like, over time. KubeStellar is eventually consistent: you can change your inputs as you like over time, and KubeStellar continually strives to achieve what you are currently asking it to do.</p> <p>There is some flexibility in the ordering of those steps. The following flowchart shows the key ordering constraints. </p> <p> Ordering among installation and usage actions </p> <p>You can have multiple ITSes, WDSes, and WECs, created and deleted over time as you like.</p> <p>Besides \"Start\", the other green items in that graph are entry points for extending usage at any later time. You could also see them as distinct user roles or authorities, or as additional layers of setup/install.</p> <p>KubeStellar's Core Helm chart combines (a) initializing the KubeFlex hosting cluster, (b) optionally creating and certainly registering some ITSes, and (c) optionally creating and certainly registering and initializing some WDSes.</p> <p>You can find an example run through of steps 2--7 in Getting Started. This dovetails with the example scenarios document, which shows examples of the later steps.</p> <p>There is also an example run through of steps 2--7 that starts with (a slightly modified version of) the OCM Quick Start and also dovetails with the example scenarios. See here.</p>"},{"location":"direct/user-guide-intro/#observability-and-monitoring","title":"Observability and Monitoring","text":"<p>KubeStellar provides several endpoints and integrations for observability, including Prometheus metrics and debug endpoints. See the Observability page for details on available metrics, endpoints, and how to access them.</p>"},{"location":"direct/user-guide-intro/#troubleshooting","title":"Troubleshooting","text":"<p>See the Troubleshooting guide.</p>"},{"location":"direct/user-guide-intro/#teardown","title":"Teardown","text":"<p>See Teardown for how to tear everything down to unadorned Kubernetes clusters.</p>"},{"location":"direct/wds/","title":"Workload Description Spaces","text":"<ul> <li>What is a WDS?</li> <li>Creating a WDS</li> <li>Using the KubeFlex CLI</li> <li>KubeFlex Hosting Cluster as WDS</li> <li>WDS vs. ControlPlane Registration</li> <li>Controllers Running in a WDS</li> <li>Working with a WDS</li> <li>Accessing the WDS</li> </ul> <p>A Workload Description Space (WDS) is a core component of the KubeStellar architecture that serves as the primary interface for users to define and manage workloads for multi-cluster deployment.</p>"},{"location":"direct/wds/#what-is-a-wds","title":"What is a WDS?","text":"<p>A WDS is a space (a Kubernetes-like API server with storage) that:</p> <ul> <li>Stores the definitions of workloads in their native Kubernetes format</li> <li>Hosts the control objects (<code>BindingPolicy</code> and <code>Binding</code>) that define how workloads are distributed</li> <li>Maintains status information about deployed workloads</li> <li>Acts as the main user interface to the KubeStellar system</li> </ul>"},{"location":"direct/wds/#creating-a-wds","title":"Creating a WDS","text":"<p>A WDS can be created in several ways:</p>"},{"location":"direct/wds/#using-the-kubestellar-core-helm-chart","title":"Using the KubeStellar Core Helm Chart","text":"<p>The recommended approach is to use the KubeStellar Core Chart:</p> <pre><code>helm upgrade --install ks-core oci://ghcr.io/kubestellar/kubestellar/core-chart \\\n  --set-json='WDSes=[{\"name\":\"wds1\", \"type\":\"k8s\"}]'\n</code></pre> <p>You can customize your WDS by specifying: - <code>name</code>: A unique name for the WDS - <code>type</code>:    - <code>k8s</code> (default): Creates a basic Kubernetes API Server with a subset of kube controllers   - <code>host</code>: Uses the KubeFlex hosting cluster itself - <code>APIGroups</code>: A comma-separated list of API Groups to include - <code>ITSName</code>: The name of the ITS to be used by this WDS (required if multiple ITSes exist)</p>"},{"location":"direct/wds/#using-the-kubeflex-cli","title":"Using the KubeFlex CLI","text":"<p>You can also create a WDS using the KubeFlex CLI:</p> <pre><code>kflex create wds1 -p kubestellar\n</code></pre> <p>This command creates a WDS and runs a post-create hook that deploys the KubeStellar controller manager and transport controller.</p>"},{"location":"direct/wds/#kubeflex-hosting-cluster-as-wds","title":"KubeFlex Hosting Cluster as WDS","text":"<p>The KubeFlex hosting cluster can be configured to act as a WDS by specifying <code>type: host</code> when creating the WDS:</p> <pre><code>helm upgrade --install ks-core oci://ghcr.io/kubestellar/kubestellar/core-chart \\\n  --set-json='WDSes=[{\"name\":\"wds1\", \"type\":\"host\"}]'\n</code></pre> <p>This approach: - Avoids creating a separate control plane - Simplifies the architecture by reusing the hosting cluster - Makes the WDS directly accessible through the hosting cluster's API server</p>"},{"location":"direct/wds/#wds-vs-controlplane-registration","title":"WDS vs. ControlPlane Registration","text":"<p>It's important to distinguish between:</p> <ol> <li>Creating a space that can serve as a WDS: This involves setting up a Kubernetes-like API server.</li> <li>Registering it with KubeFlex as a ControlPlane and deploying KubeStellar components: This is the step that makes the space function as a WDS in the KubeStellar ecosystem.</li> </ol> <p>When using the Core Helm Chart or KubeFlex CLI with appropriate parameters, both steps happen automatically.</p>"},{"location":"direct/wds/#controllers-running-in-a-wds","title":"Controllers Running in a WDS","text":"<p>When a space is configured as a WDS, the following controllers are deployed:</p> <ol> <li> <p>KubeStellar Controller Manager: Watches <code>BindingPolicy</code> objects and creates corresponding <code>Binding</code> objects that contain references to concrete workload objects and destination clusters.</p> </li> <li> <p>Transport Controller: Projects KubeStellar workload and control objects from the WDS into the Inventory and Transport Space (ITS).</p> </li> </ol> <p>These controllers are managed as Deployment objects in the KubeFlex hosting cluster.</p>"},{"location":"direct/wds/#working-with-a-wds","title":"Working with a WDS","text":"<p>Once your WDS is created, you can:</p> <ol> <li>Create workload objects in their native Kubernetes format</li> <li>Define BindingPolicy objects to specify which workloads should be deployed to which WECs</li> <li>Monitor the status of your deployed workloads</li> </ol>"},{"location":"direct/wds/#accessing-the-wds","title":"Accessing the WDS","text":"<p>You can access your WDS using the kubeconfig context provided by KubeFlex:</p> <pre><code># Set up the WDS context\nkflex ctx --overwrite-existing-context wds1\n\n# Switch to the WDS context\nkubectl config use-context wds1\n</code></pre>"},{"location":"direct/wec-registration/","title":"Registering a Workload Execution Cluster","text":"<p>This document will tell users what how to register a WEC in an ITS.</p> <p>TODO: write this.</p>"},{"location":"direct/wec/","title":"Workload Execution Clusters","text":"<ul> <li>What is a WEC?</li> <li>Creating a WEC</li> <li>Using Kind (for development/testing)</li> <li>Using K3d (for development/testing)</li> <li>Using MicroShift (for edge deployments)</li> <li>Using Production Kubernetes Distributions</li> <li>Registering a WEC</li> <li>Labeling WECs</li> <li>WEC Customization Properties</li> <li>WEC Status and Monitoring</li> <li>Workload Transformation Workload Execution Clusters (WECs) are the Kubernetes clusters where KubeStellar deploys and runs the workloads defined in the Workload Description Spaces (WDSes).</li> </ul>"},{"location":"direct/wec/#what-is-a-wec","title":"What is a WEC?","text":"<p>A WEC is a standard Kubernetes cluster that:</p> <ul> <li>Runs the actual workloads distributed by KubeStellar</li> <li>Has the OCM Agent (klusterlet) installed for communication with the ITS</li> <li>Is registered with an Inventory and Transport Space (ITS)</li> <li>May have specific characteristics (location, resources, capabilities) that make it suitable for particular workloads</li> </ul>"},{"location":"direct/wec/#requirements-for-a-wec","title":"Requirements for a WEC","text":"<p>For a Kubernetes cluster to function as a WEC in the KubeStellar ecosystem, it must:</p> <ol> <li>Have network connectivity to the Inventory and Transport Space (ITS)</li> <li>Be a valid Kubernetes cluster with a working control plane</li> <li>Have the OCM Agent installed for integration with KubeStellar</li> <li>Be registered with an ITS to receive workloads</li> </ol>"},{"location":"direct/wec/#creating-a-wec","title":"Creating a WEC","text":"<p>You can use any existing Kubernetes cluster as a WEC, or create a new one using your preferred method:</p>"},{"location":"direct/wec/#using-kind-for-developmenttesting","title":"Using Kind (for development/testing)","text":"<pre><code>kind create cluster --name cluster1\nkubectl config rename-context kind-cluster1 cluster1\n</code></pre>"},{"location":"direct/wec/#using-k3d-for-developmenttesting","title":"Using K3d (for development/testing)","text":"<pre><code>k3d cluster create -p \"9443:443@loadbalancer\" cluster1\nkubectl config rename-context k3d-cluster1 cluster1\n</code></pre>"},{"location":"direct/wec/#using-microshift-for-edge-deployments","title":"Using MicroShift (for edge deployments)","text":"<p>For resource-constrained environments like edge devices, you can use MicroShift:</p> <pre><code># Instructions for setting up MicroShift can be found at\n# https://community.ibm.com/community/user/cloud/blogs/alexei-karve/2021/11/28/microshift-4\n</code></pre>"},{"location":"direct/wec/#using-production-kubernetes-distributions","title":"Using Production Kubernetes Distributions","text":"<p>For production environments, consider using:</p> <ul> <li>Red Hat OpenShift</li> <li>Amazon EKS</li> <li>Google GKE</li> <li>Microsoft AKS</li> <li>Any conformant Kubernetes distribution</li> </ul>"},{"location":"direct/wec/#registering-a-wec","title":"Registering a WEC","text":"<p>After creating your cluster, you need to register it with an ITS. This process installs the OCM Agent and establishes the communication channel.</p> <pre><code># Get the join command from the ITS\nclusteradm --context its1 get token\n\n# Execute the join command with your WEC name\nclusteradm join --hub-token &lt;token&gt; --hub-apiserver &lt;api-server-url&gt; --cluster-name cluster1 --context cluster1\n\n# Accept the registration on the ITS side\nclusteradm --context its1 accept --clusters cluster1\n</code></pre> <p>For detailed registration instructions, see WEC Registration.</p>"},{"location":"direct/wec/#labeling-wecs","title":"Labeling WECs","text":"<p>After registration, you should label your WEC to make it selectable by BindingPolicies:</p> <pre><code>kubectl --context its1 label managedcluster cluster1 location-group=edge name=cluster1\n</code></pre> <p>These labels can represent any characteristics relevant to your workload placement decisions:</p> <ul> <li>Geographic location (<code>region=us-east</code>, <code>location=edge</code>)</li> <li>Hardware capabilities (<code>gpu=true</code>, <code>cpu-architecture=arm64</code>)</li> <li>Environment type (<code>environment=production</code>, <code>environment=development</code>)</li> <li>Compliance requirements (<code>pci-dss=compliant</code>, <code>hipaa=compliant</code>)</li> <li>Custom organizational labels (<code>team=retail</code>, <code>business-unit=finance</code>)</li> </ul>"},{"location":"direct/wec/#wec-customization-properties","title":"WEC Customization Properties","text":"<p>For each WEC, you can define additional properties in a ConfigMap stored in the \"customization-properties\" namespace of the ITS. These properties can be used for rule-based transformations of workloads.</p>"},{"location":"direct/wec/#wec-status-and-monitoring","title":"WEC Status and Monitoring","text":"<p>You can check the status of your registered WECs:</p> <pre><code>kubectl --context its1 get managedclusters\n</code></pre>"},{"location":"direct/wec/#workload-transformation","title":"Workload Transformation","text":"<p>KubeStellar performs transformations on workloads before they are deployed to WECs:</p> <ol> <li>Generic transformations that apply to all workloads</li> <li>Rule-based customizations that adapt workloads to specific WEC characteristics</li> </ol> <p>For more information, see Transforming Desired State.</p>"},{"location":"direct/images/image-files-readme/","title":"How to edit these pictures","text":"<p>The pictures have been created with draw.io, and have been saved in an editable format. You can use draw.io to modify and save back these pictures using an editable format.</p>"},{"location":"ui-docs/","title":"KubestellarUI Setup Guide","text":"<p>Welcome to KubestellarUI! This guide will help you set up the KubestellarUI application on your local machine after cloning the repository for development. The application consists of two main parts:</p> <ol> <li>Frontend: Built with React and TypeScript</li> <li>Backend: Built with Golang using the Gin framework.</li> </ol>"},{"location":"ui-docs/#contents","title":"Contents","text":"<ul> <li>Prerequisites</li> <li>Installation Steps</li> <li>Local Setup</li> <li>Local Setup with Docker Compose</li> <li>Docker Image Versioning and Pulling</li> <li>Accessing the Application</li> </ul>"},{"location":"ui-docs/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that your system meets the following requirements:</p>"},{"location":"ui-docs/#1-golang","title":"1. Golang","text":"<ul> <li>Version: 1.23.4</li> <li>Download Link: Golang Downloads</li> </ul>"},{"location":"ui-docs/#2-nodejs-and-npm","title":"2. Node.js and npm","text":"<ul> <li>Node.js Version: \u2265 16.x.x</li> <li>npm Version: Comes bundled with Node.js</li> <li>Download Link: Node.js Downloads</li> </ul> <p>[!NOTE] You can use nvm to manage multiple Node.js versions.</p>"},{"location":"ui-docs/#3-git","title":"3. Git","text":"<ul> <li>Ensure Git is installed to clone the repository</li> <li>Download Link: Git Downloads</li> </ul>"},{"location":"ui-docs/#4-kubernetes-clusters","title":"4. Kubernetes Clusters","text":"<ul> <li> <p>Ensure you have access to a Kubernetes clusters setup with Kubestellar Getting Started Guide &amp; Kubestellar prerequisites installed</p> </li> <li> <p>Kubestellar guide: Guide</p> </li> </ul>"},{"location":"ui-docs/#installation-steps","title":"Installation Steps","text":"<p>Clone the Repository</p> <p><pre><code>git clone https://github.com/your-github-username/ui.git\n\ncd ui\n</code></pre> Then go through one of the setup options below: - Local Setup - Local Setup with Docker Compose</p>"},{"location":"ui-docs/#local-setup","title":"Local Setup","text":""},{"location":"ui-docs/#step-1-create-env-file-for-frontend-configuration","title":"Step 1: Create <code>.env</code> File for Frontend Configuration","text":"<p>To configure the frontend, copy the <code>example.env</code> file to a <code>.env</code> file in the project root directory (where <code>package.json</code> is located).</p> <pre><code>cp example.env .env\n</code></pre> <p>Example <code>.env</code> file: </p> <pre><code>VITE_BASE_URL=http://localhost:4000\nVITE_APP_VERSION=0.1.0\nVITE_GIT_COMMIT_HASH=$GIT_COMMIT_HASH\n</code></pre> <p>[!NOTE]  This is because <code>.env</code> files are intended to be a personal environment configuration file. The included <code>example.env</code> in the repo is a standard that most other node projects include for the same purpose. You rename the file to <code>.env</code> and then change its contents to align with your system and personal needs.</p>"},{"location":"ui-docs/#tracking-application-version-and-git-commit-hash","title":"Tracking Application Version and Git Commit Hash","text":"<p>KubestellarUI uses environment variables to track the app version and the current Git commit hash.  </p> <p>Environment Variables </p> Variable Purpose Example <code>VITE_BASE_URL</code> Defines the base URL for API calls <code>http://localhost:4000</code> <code>VITE_APP_VERSION</code> Defines the current application version <code>0.1.0</code> <code>VITE_GIT_COMMIT_HASH</code> Captures the current Git commit hash (Set during build)"},{"location":"ui-docs/#step-2-run-redis-container-optional","title":"Step 2: Run Redis Container (Optional)","text":"<p>KubestellarUI uses Redis for caching real-time WebSocket updates to prevent excessive Kubernetes API calls.  </p> <p>Run Redis using Docker:  </p> <pre><code>docker run --name redis -d -p 6379:6379 redis\n</code></pre> <p>Verify Redis is running:  </p> <pre><code>docker ps | grep redis\n</code></pre>"},{"location":"ui-docs/#step-3-install-and-run-the-backend","title":"Step 3: Install and Run the Backend","text":"<p>Make sure you are in the root directory of the project</p> <pre><code>cd backend\n\ngo mod download\n\ngo run main.go\n</code></pre> <p>You should see output indicating the server is running on port <code>4000</code>.</p>"},{"location":"ui-docs/#step-4-install-and-run-frontend","title":"Step 4: Install and Run Frontend","text":"<p>Open another terminal and make sure you are in the root directory of the project.</p> <pre><code>npm install\n\nnpm run dev\n</code></pre> <p>You should see output indicating the server is running on port <code>5173</code>.</p>"},{"location":"ui-docs/#local-setup-with-docker-compose","title":"Local Setup with Docker Compose","text":"<p>If you prefer to run the application using Docker Compose, follow these steps:</p>"},{"location":"ui-docs/#step-1-ensure-docker-is-installed","title":"Step 1: Ensure Docker is Installed","text":"<ul> <li>Download Link: Docker Downloads</li> </ul> <p>[!NOTE]  If you are using Compose V1, change the <code>docker compose</code> command to <code>docker-compose</code> in the following steps. Checkout Migrating to Compose V2 for more info.</p>"},{"location":"ui-docs/#step-2-run-services","title":"Step 2: Run Services","text":"<p>From the project root directory</p> <pre><code>docker compose up --build\n</code></pre> <p>You should see output indicating the services are running.</p> <p>To stop the application</p> <pre><code>docker compose down\n</code></pre>"},{"location":"ui-docs/#use-docker-compose-in-development-cycle","title":"Use Docker Compose in Development Cycle","text":"<p>For ongoing development, use the following steps:</p> <ul> <li> <p>Step 1: Stop the running Application:   <pre><code>docker compose down\n</code></pre></p> </li> <li> <p>Step 2: Pull the Latest Source Code Changes:   <pre><code>git pull origin main\n</code></pre></p> </li> <li> <p>Step 3: Rebuild and Restart the Application:   <pre><code>docker compose up --build\n</code></pre> This will:</p> </li> <li> <p>Stop the running containers.</p> </li> <li>Pull the latest source code changes.</li> <li>Rebuild and restart the application.</li> </ul>"},{"location":"ui-docs/#install-golangci-lint","title":"\ud83d\ude80 Install GolangCI-Lint","text":"<p>To install GolangCI-Lint, follow these steps:</p>"},{"location":"ui-docs/#linux-macos","title":"\ud83d\udd39 Linux &amp; macOS","text":"<p>Run the following command: <pre><code>curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin v1.54.2\n</code></pre> Ensure <code>$(go env GOPATH)/bin</code> is in your <code>PATH</code>: <pre><code>export PATH=$(go env GOPATH)/bin:$PATH\n</code></pre></p>"},{"location":"ui-docs/#windows","title":"\ud83d\udd39 Windows","text":"<p>Use scoop (recommended): <pre><code>scoop install golangci-lint\n</code></pre> Or Go install: <pre><code>go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest\n</code></pre></p>"},{"location":"ui-docs/#verify-installation","title":"\ud83d\udd39 Verify Installation","text":"<p>Run: <pre><code>golangci-lint --version\n</code></pre></p>"},{"location":"ui-docs/#linting-fixing-code","title":"\ud83d\udee0 Linting &amp; Fixing Code","text":""},{"location":"ui-docs/#check-for-issues","title":"\ud83d\udd39 Check for Issues","text":"<pre><code>make check-lint\n</code></pre>"},{"location":"ui-docs/#auto-fix-issues","title":"\ud83d\udd39 Auto-Fix Issues","text":"<pre><code>make fix-lint\n</code></pre>"},{"location":"ui-docs/#run-both","title":"\ud83d\udd39 Run Both","text":"<pre><code>make lint\n</code></pre>"},{"location":"ui-docs/#docker-image-versioning-and-pulling","title":"Docker Image Versioning and Pulling","text":"<p>If you'd like to work with the Docker images for the KubestellarUI project, here's how you can use the <code>latest</code> and versioned tags:</p> <ol> <li>Frontend Image:</li> <li>Tag: <code>quay.io/kubestellar/ui:frontend</code></li> <li>Latest Version: <code>latest</code></li> <li> <p>Specific Version (Commit Hash): <code>frontend-&lt;commit-hash&gt;</code></p> </li> <li> <p>Backend Image:</p> </li> <li>Tag: <code>quay.io/kubestellar/ui:backend</code></li> <li>Latest Version: <code>latest</code></li> <li>Specific Version (Commit Hash): <code>backend-&lt;commit-hash&gt;</code></li> </ol>"},{"location":"ui-docs/#how-to-pull-the-latest-images","title":"How to Pull the Latest Images:","text":"<ul> <li> <p>Frontend Image:   <pre><code>docker pull quay.io/kubestellar/ui:frontend\n</code></pre></p> </li> <li> <p>Backend Image:   <pre><code>docker pull quay.io/kubestellar/ui:backend\n</code></pre></p> </li> </ul>"},{"location":"ui-docs/#how-to-pull-specific-version-commit-hash","title":"How to Pull Specific Version (Commit Hash):","text":"<p>If you want to pull an image for a specific version (e.g., commit hash), use:</p> <ul> <li> <p>Frontend Image with Version:   <pre><code>docker pull quay.io/kubestellar/ui:frontend-abcd1234\n</code></pre></p> </li> <li> <p>Backend Image with Version:   <pre><code>docker pull quay.io/kubestellar/ui:backend-abcd1234\n</code></pre></p> </li> </ul>"},{"location":"ui-docs/#accessing-the-application","title":"Accessing the Application","text":"<ol> <li>Backend API: http://localhost:4000</li> <li>Frontend UI: http://localhost:5173</li> </ol>  Contributors"},{"location":"ui-docs/ui-overview/","title":"KubeStellar UI (User Interface)","text":"<p> UI splash page </p> <p>The KubeStellar UI is an add-on developed to make managing workloads via KubeStellar even simpler and more intuitive. With its web-based interface, you can view and manage your Workload Definition Space, Inventory and Transport Space, and Binding Policies all interactively, with both drag-and-drop and text-based interface modes available for use.</p>"},{"location":"ui-docs/ui-overview/#learn-more","title":"Learn More","text":""},{"location":"ui-docs/ui-overview/#ui-repository","title":"UI Repository","text":"<p>To explore more fully under the covers, visit the UI repository at https://github.com/kubestellar/ui</p>"},{"location":"ui-docs/ui-overview/#readme-copied-from-the-ui-repository","title":"README (copied from the UI Repository)","text":"<p>(The content that follows is imported from a copy of the README.md file for the KubeStellarUI repository)</p>"},{"location":"ui-docs/ui-overview/#kubestellarui-setup-guide","title":"KubestellarUI Setup Guide","text":"<p>Welcome to KubestellarUI! This guide will help you set up the KubestellarUI application on your local machine after cloning the repository for development. The application consists of two main parts:</p> <ol> <li>Frontend: Built with React and TypeScript</li> <li>Backend: Built with Golang using the Gin framework.</li> </ol>"},{"location":"ui-docs/ui-overview/#contents","title":"Contents","text":"<ul> <li>Prerequisites</li> <li>Installation Steps</li> <li>Local Setup</li> <li>Local Setup with Docker Compose</li> <li>Docker Image Versioning and Pulling</li> <li>Accessing the Application</li> </ul>"},{"location":"ui-docs/ui-overview/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that your system meets the following requirements:</p>"},{"location":"ui-docs/ui-overview/#1-golang","title":"1. Golang","text":"<ul> <li>Version: 1.23.4</li> <li>Download Link: Golang Downloads</li> </ul>"},{"location":"ui-docs/ui-overview/#2-nodejs-and-npm","title":"2. Node.js and npm","text":"<ul> <li>Node.js Version: \u2265 16.x.x</li> <li>npm Version: Comes bundled with Node.js</li> <li>Download Link: Node.js Downloads</li> </ul> <p>[!NOTE] You can use nvm to manage multiple Node.js versions.</p>"},{"location":"ui-docs/ui-overview/#3-git","title":"3. Git","text":"<ul> <li>Ensure Git is installed to clone the repository</li> <li>Download Link: Git Downloads</li> </ul>"},{"location":"ui-docs/ui-overview/#4-kubernetes-clusters","title":"4. Kubernetes Clusters","text":"<ul> <li> <p>Ensure you have access to a Kubernetes clusters setup with Kubestellar Getting Started Guide &amp; Kubestellar prerequisites installed</p> </li> <li> <p>Kubestellar guide: Guide</p> </li> </ul>"},{"location":"ui-docs/ui-overview/#installation-steps","title":"Installation Steps","text":"<p>Clone the Repository</p> <p><pre><code>git clone https://github.com/your-github-username/ui.git\n\ncd ui\n</code></pre> Then go through one of the setup options below: - Local Setup - Local Setup with Docker Compose</p>"},{"location":"ui-docs/ui-overview/#local-setup","title":"Local Setup","text":""},{"location":"ui-docs/ui-overview/#step-1-create-env-file-for-frontend-configuration","title":"Step 1: Create <code>.env</code> File for Frontend Configuration","text":"<p>To configure the frontend, copy the <code>example.env</code> file to a <code>.env</code> file in the project root directory (where <code>package.json</code> is located).</p> <pre><code>cp example.env .env\n</code></pre> <p>Example <code>.env</code> file: </p> <pre><code>VITE_BASE_URL=http://localhost:4000\nVITE_APP_VERSION=0.1.0\nVITE_GIT_COMMIT_HASH=$GIT_COMMIT_HASH\n</code></pre> <p>[!NOTE]  This is because <code>.env</code> files are intended to be a personal environment configuration file. The included <code>example.env</code> in the repo is a standard that most other node projects include for the same purpose. You rename the file to <code>.env</code> and then change its contents to align with your system and personal needs.</p>"},{"location":"ui-docs/ui-overview/#tracking-application-version-and-git-commit-hash","title":"Tracking Application Version and Git Commit Hash","text":"<p>KubestellarUI uses environment variables to track the app version and the current Git commit hash.  </p> <p>Environment Variables </p> Variable Purpose Example <code>VITE_BASE_URL</code> Defines the base URL for API calls <code>http://localhost:4000</code> <code>VITE_APP_VERSION</code> Defines the current application version <code>0.1.0</code> <code>VITE_GIT_COMMIT_HASH</code> Captures the current Git commit hash (Set during build)"},{"location":"ui-docs/ui-overview/#step-2-run-redis-container-optional","title":"Step 2: Run Redis Container (Optional)","text":"<p>KubestellarUI uses Redis for caching real-time WebSocket updates to prevent excessive Kubernetes API calls.  </p> <p>Run Redis using Docker:  </p> <pre><code>docker run --name redis -d -p 6379:6379 redis\n</code></pre> <p>Verify Redis is running:  </p> <pre><code>docker ps | grep redis\n</code></pre>"},{"location":"ui-docs/ui-overview/#step-3-install-and-run-the-backend","title":"Step 3: Install and Run the Backend","text":"<p>Make sure you are in the root directory of the project</p> <pre><code>cd backend\n\ngo mod download\n\ngo run main.go\n</code></pre> <p>You should see output indicating the server is running on port <code>4000</code>.</p>"},{"location":"ui-docs/ui-overview/#step-4-install-and-run-frontend","title":"Step 4: Install and Run Frontend","text":"<p>Open another terminal and make sure you are in the root directory of the project.</p> <pre><code>npm install\n\nnpm run dev\n</code></pre> <p>You should see output indicating the server is running on port <code>5173</code>.</p>"},{"location":"ui-docs/ui-overview/#local-setup-with-docker-compose","title":"Local Setup with Docker Compose","text":"<p>If you prefer to run the application using Docker Compose, follow these steps:</p>"},{"location":"ui-docs/ui-overview/#step-1-ensure-docker-is-installed","title":"Step 1: Ensure Docker is Installed","text":"<ul> <li>Download Link: Docker Downloads</li> </ul> <p>[!NOTE]  If you are using Compose V1, change the <code>docker compose</code> command to <code>docker-compose</code> in the following steps. Checkout Migrating to Compose V2 for more info.</p>"},{"location":"ui-docs/ui-overview/#step-2-run-services","title":"Step 2: Run Services","text":"<p>From the project root directory</p> <pre><code>docker compose up --build\n</code></pre> <p>You should see output indicating the services are running.</p> <p>To stop the application</p> <pre><code>docker compose down\n</code></pre>"},{"location":"ui-docs/ui-overview/#use-docker-compose-in-development-cycle","title":"Use Docker Compose in Development Cycle","text":"<p>For ongoing development, use the following steps:</p> <ul> <li> <p>Step 1: Stop the running Application:   <pre><code>docker compose down\n</code></pre></p> </li> <li> <p>Step 2: Pull the Latest Source Code Changes:   <pre><code>git pull origin main\n</code></pre></p> </li> <li> <p>Step 3: Rebuild and Restart the Application:   <pre><code>docker compose up --build\n</code></pre> This will:</p> </li> <li> <p>Stop the running containers.</p> </li> <li>Pull the latest source code changes.</li> <li>Rebuild and restart the application.</li> </ul>"},{"location":"ui-docs/ui-overview/#install-golangci-lint","title":"\ud83d\ude80 Install GolangCI-Lint","text":"<p>To install GolangCI-Lint, follow these steps:</p>"},{"location":"ui-docs/ui-overview/#linux-macos","title":"\ud83d\udd39 Linux &amp; macOS","text":"<p>Run the following command: <pre><code>curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin v1.54.2\n</code></pre> Ensure <code>$(go env GOPATH)/bin</code> is in your <code>PATH</code>: <pre><code>export PATH=$(go env GOPATH)/bin:$PATH\n</code></pre></p>"},{"location":"ui-docs/ui-overview/#windows","title":"\ud83d\udd39 Windows","text":"<p>Use scoop (recommended): <pre><code>scoop install golangci-lint\n</code></pre> Or Go install: <pre><code>go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest\n</code></pre></p>"},{"location":"ui-docs/ui-overview/#verify-installation","title":"\ud83d\udd39 Verify Installation","text":"<p>Run: <pre><code>golangci-lint --version\n</code></pre></p>"},{"location":"ui-docs/ui-overview/#linting-fixing-code","title":"\ud83d\udee0 Linting &amp; Fixing Code","text":""},{"location":"ui-docs/ui-overview/#check-for-issues","title":"\ud83d\udd39 Check for Issues","text":"<pre><code>make check-lint\n</code></pre>"},{"location":"ui-docs/ui-overview/#auto-fix-issues","title":"\ud83d\udd39 Auto-Fix Issues","text":"<pre><code>make fix-lint\n</code></pre>"},{"location":"ui-docs/ui-overview/#run-both","title":"\ud83d\udd39 Run Both","text":"<pre><code>make lint\n</code></pre>"},{"location":"ui-docs/ui-overview/#docker-image-versioning-and-pulling","title":"Docker Image Versioning and Pulling","text":"<p>If you'd like to work with the Docker images for the KubestellarUI project, here's how you can use the <code>latest</code> and versioned tags:</p> <ol> <li>Frontend Image:</li> <li>Tag: <code>quay.io/kubestellar/ui:frontend</code></li> <li>Latest Version: <code>latest</code></li> <li> <p>Specific Version (Commit Hash): <code>frontend-&lt;commit-hash&gt;</code></p> </li> <li> <p>Backend Image:</p> </li> <li>Tag: <code>quay.io/kubestellar/ui:backend</code></li> <li>Latest Version: <code>latest</code></li> <li>Specific Version (Commit Hash): <code>backend-&lt;commit-hash&gt;</code></li> </ol>"},{"location":"ui-docs/ui-overview/#how-to-pull-the-latest-images","title":"How to Pull the Latest Images:","text":"<ul> <li> <p>Frontend Image:   <pre><code>docker pull quay.io/kubestellar/ui:frontend\n</code></pre></p> </li> <li> <p>Backend Image:   <pre><code>docker pull quay.io/kubestellar/ui:backend\n</code></pre></p> </li> </ul>"},{"location":"ui-docs/ui-overview/#how-to-pull-specific-version-commit-hash","title":"How to Pull Specific Version (Commit Hash):","text":"<p>If you want to pull an image for a specific version (e.g., commit hash), use:</p> <ul> <li> <p>Frontend Image with Version:   <pre><code>docker pull quay.io/kubestellar/ui:frontend-abcd1234\n</code></pre></p> </li> <li> <p>Backend Image with Version:   <pre><code>docker pull quay.io/kubestellar/ui:backend-abcd1234\n</code></pre></p> </li> </ul>"},{"location":"ui-docs/ui-overview/#accessing-the-application","title":"Accessing the Application","text":"<ol> <li>Backend API: http://localhost:4000</li> <li>Frontend UI: http://localhost:5173</li> </ol>  Contributors"}]}